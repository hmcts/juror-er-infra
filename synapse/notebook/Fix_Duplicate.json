{
	"name": "Fix_Duplicate",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkprod",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "bbde27b1-c290-4ede-8431-7aa39ada6535"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5ca62022-6aa2-4cee-aaa7-e7536c8d566c/resourceGroups/baubais-data-factory-rg-prod/providers/Microsoft.Synapse/workspaces/baubais-synapse-prod/bigDataPools/sparkprod",
				"name": "sparkprod",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkprod",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col\n",
					"from notebookutils import mssparkutils\n",
					"import time\n",
					"\n",
					"# Initialize Spark\n",
					"spark = SparkSession.builder.appName(\"Delete Duplicate Record\").getOrCreate()\n",
					"\n",
					"# Define paths\n",
					"storage_account = \"baubaisadfsaprod\"\n",
					"base_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net\"\n",
					"postgresql_path = f\"{base_path}/voters_postgresql\"\n",
					"\n",
					"# Function to safely process each partition\n",
					"def process_partition(partition_path, target_hash_id):\n",
					"    try:\n",
					"        print(f\"Processing partition: {partition_path}\")\n",
					"        \n",
					"        # Read partition\n",
					"        df = spark.read.parquet(partition_path)\n",
					"        \n",
					"        # Check if hash_id exists in this partition\n",
					"        hash_count = df.filter(col(\"hash_id\") == target_hash_id).count()\n",
					"        \n",
					"        if hash_count > 0:\n",
					"            print(f\"Found {hash_count} occurrences of target hash_id in this partition\")\n",
					"            \n",
					"            # Filter out the target hash_id\n",
					"            filtered_df = df.filter(col(\"hash_id\") != target_hash_id)\n",
					"            \n",
					"            # Create temporary output path with timestamp to avoid conflicts\n",
					"            timestamp = int(time.time())\n",
					"            temp_path = f\"{base_path}/temp_partition_{timestamp}\"\n",
					"            \n",
					"            # Write filtered data to temp location\n",
					"            filtered_df.write.mode(\"overwrite\").parquet(temp_path)\n",
					"            \n",
					"            # Move files back to original location\n",
					"            print(f\"Moving filtered data back to {partition_path}\")\n",
					"            \n",
					"            # Delete original partition\n",
					"            mssparkutils.fs.rm(partition_path, True)\n",
					"            \n",
					"            # Copy from temp to original\n",
					"            mssparkutils.fs.cp(temp_path, partition_path, True)\n",
					"            \n",
					"            # Clean up temp\n",
					"            mssparkutils.fs.rm(temp_path, True)\n",
					"            \n",
					"            print(f\"Successfully removed {hash_count} records from {partition_path}\")\n",
					"            return hash_count\n",
					"        else:\n",
					"            print(f\"Target hash_id not found in this partition\")\n",
					"            return 0\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"Error processing partition {partition_path}: {str(e)}\")\n",
					"        return 0\n",
					"\n",
					"# Target hash_id to remove\n",
					"target_hash_id = \"9211754078771705743\"\n",
					"\n",
					"# List all partitions\n",
					"try:\n",
					"    # Get all items in the directory\n",
					"    all_items = mssparkutils.fs.ls(postgresql_path)\n",
					"    partition_paths = []\n",
					"    \n",
					"    # First check if it's partitioned or not\n",
					"    partitioned = False\n",
					"    \n",
					"    for item in all_items:\n",
					"        if item.isDir and \"creation_date_partition=\" in item.path:\n",
					"            partitioned = True\n",
					"            partition_paths.append(item.path)\n",
					"            print(f\"Found partition: {item.path}\")\n",
					"    \n",
					"    # If not partitioned, just process the main directory\n",
					"    if not partitioned:\n",
					"        print(\"Data is not partitioned. Processing entire directory.\")\n",
					"        partition_paths = [postgresql_path]\n",
					"    \n",
					"    # Process each partition\n",
					"    total_removed = 0\n",
					"    for path in partition_paths:\n",
					"        removed = process_partition(path, target_hash_id)\n",
					"        total_removed += removed\n",
					"    \n",
					"    print(f\"Total records removed: {total_removed}\")\n",
					"    \n",
					"except Exception as e:\n",
					"    print(f\"Error listing partitions: {str(e)}\")"
				],
				"execution_count": 1
			}
		]
	}
}