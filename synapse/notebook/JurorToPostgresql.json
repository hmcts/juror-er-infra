{
	"name": "JurorToPostgresql",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkprod",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "90f6d32c-c41d-4d73-8232-d32e15b45adc"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5ca62022-6aa2-4cee-aaa7-e7536c8d566c/resourceGroups/baubais-data-factory-rg-prod/providers/Microsoft.Synapse/workspaces/baubais-synapse-prod/bigDataPools/sparkprod",
				"name": "sparkprod",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkprod",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"\"\"\"Synapse PySpark Data Sanitization Pipeline with Sensitive Data Hashing/Masking\n",
					"Reads from deduplicated delta lake table, hashes/masks sensitive data, and saves to blob storage\n",
					"Uses production data but protects sensitive columns for test environment\n",
					"\"\"\"\n",
					"\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql.window import Window\n",
					"from notebookutils import mssparkutils\n",
					"import logging\n",
					"from datetime import datetime\n",
					"import time\n",
					"from delta.tables import DeltaTable\n",
					"import logging\n",
					"\n",
					"# Configure logging\n",
					"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"logger = logging.getLogger()\n",
					"\n",
					"# Initialize Spark session with Delta Lake support\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"Voters Data Pipeline\") \\\n",
					"    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
					"    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
					"    .config(\"spark.executor.memory\", \"4g\") \\\n",
					"    .config(\"spark.driver.memory\", \"4g\") \\\n",
					"    .getOrCreate()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Configuration\n",
					"STORAGE_ACCOUNT = \"baubaisadfsaprod\"  # Update with your storage account\n",
					"INPUT_DELTA_PATH = f\"abfss://dl-juror-eric-voters-temp@{STORAGE_ACCOUNT}.dfs.core.windows.net/voters_deduplicated_delta\"\n",
					"\n",
					"# Define final column order for PostgreSQL (same schema)\n",
					"FINAL_COLUMNS = [\n",
					"    \"part_no\", \"register_lett\", \"poll_number\", \"new_marker\", \"title\",\n",
					"    \"lname\", \"fname\", \"dob\", \"flags\", \"address\", \"address2\", \"address3\",\n",
					"    \"address4\", \"address5\", \"address6\", \"zip\", \"date_selected1\",\n",
					"    \"date_selected2\", \"date_selected3\", \"rec_num\", \"perm_disqual\",\n",
					"    \"source_id\", \"postcode_start\", \"creation_date\", \"hash_id\"\n",
					"]\n",
					"\n",
					"# Configuration for sensitive data protection\n",
					"# Set PROTECTION_MODE to control how sensitive data is handled:\n",
					"# \"HASH\" - Hash sensitive data (irreversible but maintains uniqueness)\n",
					"# \"MASK\" - Mask sensitive data (replaces with pattern-preserving values)\n",
					"# \"DISABLE\" - Set sensitive fields to NULL (removes data completely)\n",
					"PROTECTION_MODE = \"HASH\"  # Change to \"MASK\" or \"DISABLE\" as needed\n",
					"\n",
					"# Define sensitive columns that need protection\n",
					"SENSITIVE_COLUMNS = {\n",
					"    \"lname\": {\"protect\": True, \"max_length\": 20, \"type\": \"name\"},\n",
					"    \"fname\": {\"protect\": True, \"max_length\": 20, \"type\": \"name\"},\n",
					"    \"dob\": {\"protect\": True, \"type\": \"date\"},\n",
					"    \"address\": {\"protect\": True, \"max_length\": 35, \"type\": \"address\"},\n",
					"    \"address2\": {\"protect\": True, \"max_length\": 35, \"type\": \"address\"},\n",
					"    \"address3\": {\"protect\": True, \"max_length\": 35, \"type\": \"address\"},\n",
					"    \"address5\": {\"protect\": True, \"max_length\": 35, \"type\": \"address\"},\n",
					"    \"zip\": {\"protect\": True, \"max_length\": 10, \"type\": \"postcode\"},\n",
					"    # Non-sensitive columns (keep as-is)\n",
					"    \"address4\": {\"protect\": False, \"max_length\": 35, \"type\": \"address\"},  # Keep for data integrity\n",
					"    \"register_lett\": {\"protect\": False, \"max_length\": 5, \"type\": \"identifier\"},\n",
					"    \"poll_number\": {\"protect\": False, \"max_length\": 5, \"type\": \"identifier\"},\n",
					"    \"rec_num\": {\"protect\": False, \"type\": \"identifier\"},\n",
					"    \"hash_id\": {\"protect\": False, \"type\": \"identifier\"}\n",
					"}\n",
					"\n",
					"def hash_sensitive_field(df, column_name, max_length=None, field_type=\"string\"):\n",
					"    \"\"\"\n",
					"    Hash a sensitive field using xxhash64 for consistent results\n",
					"    \"\"\"\n",
					"    if column_name not in df.columns:\n",
					"        return df\n",
					"    \n",
					"    logger.info(f\"Hashing sensitive field: {column_name}\")\n",
					"    \n",
					"    # Create hashed version with deterministic salt based on column name\n",
					"    salt = f\"TEST_SALT_{column_name.upper()}\"\n",
					"    \n",
					"    if field_type == \"date\":\n",
					"        # For dates, hash but maintain date format\n",
					"        df = df.withColumn(f\"{column_name}_hashed\",\n",
					"            when(col(column_name).isNotNull(),\n",
					"                 # Create a deterministic date based on hash\n",
					"                 date_add(\n",
					"                     to_date(lit(\"1950-01-01\")),\n",
					"                     (abs(hash(col(\"dob\"))) % 18250).cast(\"int\")  # Key fix: .cast(\"int\")\n",
					"                 )\n",
					"            ).otherwise(col(column_name))\n",
					"        )\n",
					"    else:\n",
					"        # For text fields, create hash-based values\n",
					"        df = df.withColumn(f\"{column_name}_hashed\",\n",
					"            when(col(column_name).isNotNull(),\n",
					"                 concat(\n",
					"                     lit(\"HASH_\"),\n",
					"                     substring(\n",
					"                         hex(xxhash64(concat(col(column_name), lit(salt)))),\n",
					"                         1, \n",
					"                         max_length - 5 if max_length else 15\n",
					"                     )\n",
					"                 )\n",
					"            ).otherwise(col(column_name))\n",
					"        )\n",
					"        \n",
					"        # Apply length limit if specified\n",
					"        if max_length:\n",
					"            df = df.withColumn(f\"{column_name}_hashed\",\n",
					"                substring(col(f\"{column_name}_hashed\"), 1, max_length)\n",
					"            )\n",
					"    \n",
					"    # Replace original column with hashed version\n",
					"    df = df.drop(column_name).withColumnRenamed(f\"{column_name}_hashed\", column_name)\n",
					"    \n",
					"    return df\n",
					"\n",
					"def mask_sensitive_field(df, column_name, max_length=None, field_type=\"string\"):\n",
					"    \"\"\"\n",
					"    Mask a sensitive field with pattern-preserving values\n",
					"    \"\"\"\n",
					"    if column_name not in df.columns:\n",
					"        return df\n",
					"    \n",
					"    logger.info(f\"Masking sensitive field: {column_name}\")\n",
					"    \n",
					"    if field_type == \"date\":\n",
					"        # For dates, shift by a random but consistent amount\n",
					"        df = df.withColumn(f\"{column_name}_masked\",\n",
					"            when(col(column_name).isNotNull(),\n",
					"                 date_add(\n",
					"                     col(column_name),\n",
					"                     # Consistent shift based on hash_id\n",
					"                     ((abs(col(\"hash_id\")) % 3650) + 1)  # 1-10 years shift\n",
					"                 )\n",
					"            ).otherwise(col(column_name))\n",
					"        )\n",
					"    elif field_type == \"name\":\n",
					"        # For names, replace with pattern-preserving values\n",
					"        df = df.withColumn(f\"{column_name}_masked\",\n",
					"            when(col(column_name).isNotNull(),\n",
					"                 concat(\n",
					"                     lit(\"MASKED_\"),\n",
					"                     substring(\n",
					"                         regexp_replace(col(column_name), \"[A-Z]\", \"X\"),\n",
					"                         1,\n",
					"                         max_length - 7 if max_length else 10\n",
					"                     )\n",
					"                 )\n",
					"            ).otherwise(col(column_name))\n",
					"        )\n",
					"    elif field_type == \"address\":\n",
					"        # For addresses, replace with generic test addresses\n",
					"        df = df.withColumn(f\"{column_name}_masked\",\n",
					"            when(col(column_name).isNotNull(),\n",
					"                 concat(\n",
					"                     lit(\"TEST ADDRESS \"),\n",
					"                     (abs(col(\"hash_id\")) % 999 + 1).cast(\"string\")\n",
					"                 )\n",
					"            ).otherwise(col(column_name))\n",
					"        )\n",
					"    elif field_type == \"postcode\":\n",
					"        # For postcodes, create test postcodes with valid UK format\n",
					"        df = df.withColumn(f\"{column_name}_masked\",\n",
					"            when(col(column_name).isNotNull(),\n",
					"                 concat(\n",
					"                     lit(\"TE\"),\n",
					"                     (abs(col(\"hash_id\")) % 9 + 1).cast(\"string\"),\n",
					"                     lit(\" \"),\n",
					"                     (abs(col(\"hash_id\")) % 9 + 1).cast(\"string\"),\n",
					"                     lit(\"XX\")\n",
					"                 )\n",
					"            ).otherwise(col(column_name))\n",
					"        )\n",
					"    else:\n",
					"        # Default masking for other types\n",
					"        df = df.withColumn(f\"{column_name}_masked\",\n",
					"            when(col(column_name).isNotNull(),\n",
					"                 concat(lit(\"MASKED_\"), substring(col(column_name), 1, 5))\n",
					"            ).otherwise(col(column_name))\n",
					"        )\n",
					"    \n",
					"    # Apply length limit if specified\n",
					"    if max_length:\n",
					"        df = df.withColumn(f\"{column_name}_masked\",\n",
					"            substring(col(f\"{column_name}_masked\"), 1, max_length)\n",
					"        )\n",
					"    \n",
					"    # Replace original column with masked version\n",
					"    df = df.drop(column_name).withColumnRenamed(f\"{column_name}_masked\", column_name)\n",
					"    \n",
					"    return df\n",
					"\n",
					"def disable_sensitive_field(df, column_name):\n",
					"    \"\"\"\n",
					"    Disable a sensitive field by setting it to NULL\n",
					"    \"\"\"\n",
					"    if column_name not in df.columns:\n",
					"        return df\n",
					"    \n",
					"    logger.info(f\"Disabling sensitive field: {column_name}\")\n",
					"    \n",
					"    df = df.withColumn(column_name, lit(None))\n",
					"    \n",
					"    return df\n",
					"\n",
					"def protect_sensitive_data(df):\n",
					"    \"\"\"\n",
					"    Protect sensitive data based on the configured protection mode\n",
					"    \"\"\"\n",
					"    logger.info(f\"🔒 Starting sensitive data protection using mode: {PROTECTION_MODE}\")\n",
					"    \n",
					"    initial_count = df.count()\n",
					"    logger.info(f\"Records to protect: {initial_count:,}\")\n",
					"    \n",
					"    protection_stats = {\"hashed\": 0, \"masked\": 0, \"disabled\": 0, \"skipped\": 0}\n",
					"    \n",
					"    for column_name, config in SENSITIVE_COLUMNS.items():\n",
					"        if column_name not in df.columns:\n",
					"            logger.warning(f\"Column {column_name} not found in DataFrame\")\n",
					"            continue\n",
					"            \n",
					"        if not config.get(\"protect\", False):\n",
					"            logger.info(f\"Skipping non-sensitive column: {column_name}\")\n",
					"            protection_stats[\"skipped\"] += 1\n",
					"            continue\n",
					"        \n",
					"        max_length = config.get(\"max_length\")\n",
					"        field_type = config.get(\"type\", \"string\")\n",
					"        \n",
					"        if PROTECTION_MODE == \"HASH\":\n",
					"            df = hash_sensitive_field(df, column_name, max_length, field_type)\n",
					"            protection_stats[\"hashed\"] += 1\n",
					"            \n",
					"        elif PROTECTION_MODE == \"MASK\":\n",
					"            df = mask_sensitive_field(df, column_name, max_length, field_type)\n",
					"            protection_stats[\"masked\"] += 1\n",
					"            \n",
					"        elif PROTECTION_MODE == \"DISABLE\":\n",
					"            df = disable_sensitive_field(df, column_name)\n",
					"            protection_stats[\"disabled\"] += 1\n",
					"            \n",
					"        else:\n",
					"            logger.error(f\"Unknown protection mode: {PROTECTION_MODE}\")\n",
					"            protection_stats[\"skipped\"] += 1\n",
					"    \n",
					"    logger.info(f\"✅ Sensitive data protection complete:\")\n",
					"    logger.info(f\"   - Hashed: {protection_stats['hashed']} columns\")\n",
					"    logger.info(f\"   - Masked: {protection_stats['masked']} columns\")\n",
					"    logger.info(f\"   - Disabled: {protection_stats['disabled']} columns\")\n",
					"    logger.info(f\"   - Skipped: {protection_stats['skipped']} columns\")\n",
					"    \n",
					"    return df\n",
					"\n",
					"def careful_clean_text_fields(df):\n",
					"    \"\"\"\n",
					"    Carefully clean text fields without destroying valid data\n",
					"    \"\"\"\n",
					"    logger.info(\"Starting careful text field cleaning\")\n",
					"    \n",
					"    # Define text columns to clean (excluding sensitive ones that were already processed)\n",
					"    text_columns = [\"part_no\", \"register_lett\", \"new_marker\", \"title\", \n",
					"                    \"flags\", \"perm_disqual\", \"source_id\", \"postcode_start\", \"creation_date\"]\n",
					"    \n",
					"    for col_name in text_columns:\n",
					"        if col_name in df.columns:\n",
					"            df = df.withColumn(col_name,\n",
					"                when(\n",
					"                    (col(col_name).isNull()) |\n",
					"                    (upper(trim(col(col_name))) == \"NAN\") |\n",
					"                    (upper(trim(col(col_name))) == \"NULL\") |\n",
					"                    (upper(trim(col(col_name))) == \"NONE\") |\n",
					"                    (trim(col(col_name)) == \"\"),\n",
					"                    lit(None)\n",
					"                ).otherwise(trim(col(col_name)))\n",
					"            )\n",
					"    \n",
					"    logger.info(\"✅ Completed careful text field cleaning\")\n",
					"    return df\n",
					"\n",
					"def clean_and_convert_timestamps(df):\n",
					"    \"\"\"\n",
					"    Clean and convert timestamp fields properly\n",
					"    \"\"\"\n",
					"    logger.info(\"Cleaning timestamp fields\")\n",
					"    \n",
					"    timestamp_columns = ['dob']\n",
					"    \n",
					"    for col_name in timestamp_columns:\n",
					"        if col_name in df.columns:\n",
					"            df = df.withColumn(col_name,\n",
					"                when(\n",
					"                    (col(col_name).isNull()) |\n",
					"                    (trim(col(col_name)) == \"\") |\n",
					"                    (upper(trim(col(col_name))).isin([\"NULL\", \"NAN\", \"NA\", \"NONE\"])),\n",
					"                    lit(None)\n",
					"                ).otherwise(\n",
					"                    # Try to convert to date if it's a valid date string\n",
					"                    coalesce(\n",
					"                        to_date(col(col_name), \"yyyy-MM-dd\"),\n",
					"                        to_date(col(col_name), \"dd/MM/yyyy\"),\n",
					"                        to_date(col(col_name), \"MM/dd/yyyy\")\n",
					"                    )\n",
					"                )\n",
					"            )\n",
					"    \n",
					"    return df\n",
					"\n",
					"def clean_register_lett_and_poll_number(df):\n",
					"    \"\"\"\n",
					"    Clean register_lett and poll_number fields with proper truncation\n",
					"    \"\"\"\n",
					"    logger.info(\"Cleaning register_lett and poll_number fields\")\n",
					"    \n",
					"    # Clean register_lett: strip whitespace and truncate to 5 chars\n",
					"    if \"register_lett\" in df.columns:\n",
					"        df = df.withColumn(\"register_lett\",\n",
					"            when(col(\"register_lett\").isNull(), lit(None))\n",
					"            .otherwise(substring(trim(col(\"register_lett\").cast(\"string\")), 1, 5))\n",
					"        )\n",
					"        logger.info(\"✅ Cleaned register_lett: stripped and truncated to 5 characters\")\n",
					"    \n",
					"    # Clean poll_number: remove .0 from floats, ensure numeric, truncate to 5 chars\n",
					"    if \"poll_number\" in df.columns:\n",
					"        df = df.withColumn(\"poll_number_temp\",\n",
					"            regexp_replace(col(\"poll_number\").cast(\"string\"), \"\\\\.0$\", \"\")\n",
					"        ).withColumn(\"poll_number_numeric\",\n",
					"            col(\"poll_number_temp\").cast(\"double\")\n",
					"        ).withColumn(\"poll_number\",\n",
					"            when(col(\"poll_number_numeric\").isNull(), lit(None))\n",
					"            .otherwise(substring(col(\"poll_number_numeric\").cast(\"int\").cast(\"string\"), 1, 5))\n",
					"        ).drop(\"poll_number_temp\", \"poll_number_numeric\")\n",
					"        \n",
					"        logger.info(\"✅ Cleaned poll_number: removed .0 suffix, ensured numeric, truncated to 5 characters\")\n",
					"    \n",
					"    return df\n",
					"\n",
					"def validate_and_fix_required_fields(df):\n",
					"    \"\"\"\n",
					"    Validate and fix fields that are required by PostgreSQL schema\n",
					"    \"\"\"\n",
					"    logger.info(\"Validating and fixing required fields\")\n",
					"    \n",
					"    initial_count = df.count()\n",
					"    \n",
					"    # Critical fields that cannot be NULL in PostgreSQL\n",
					"    critical_fields = ['lname', 'fname', 'address', 'hash_id']\n",
					"    \n",
					"    for field in critical_fields:\n",
					"        if field in df.columns:\n",
					"            null_count = df.filter(col(field).isNull()).count()\n",
					"            if null_count > 0:\n",
					"                logger.warning(f\"Found {null_count:,} null values in critical field {field}\")\n",
					"                \n",
					"                # Remove records with null critical fields\n",
					"                df = df.filter(col(field).isNotNull())\n",
					"    \n",
					"    # Ensure address4 is never null (required for PostgreSQL schema)\n",
					"    if \"address4\" in df.columns:\n",
					"        null_address4_count = df.filter(col(\"address4\").isNull()).count()\n",
					"        if null_address4_count > 0:\n",
					"            logger.info(f\"Setting {null_address4_count:,} null address4 values to 'UNKNOWN'\")\n",
					"            df = df.withColumn(\"address4\",\n",
					"                when(col(\"address4\").isNull(), lit(\"UNKNOWN\"))\n",
					"                .otherwise(col(\"address4\"))\n",
					"            )\n",
					"    \n",
					"    final_count = df.count()\n",
					"    removed_count = initial_count - final_count\n",
					"    \n",
					"    if removed_count > 0:\n",
					"        logger.info(f\"Removed {removed_count:,} records with null critical fields\")\n",
					"    \n",
					"    return df\n",
					"\n",
					"def clean_hash_id_and_deduplicate(df):\n",
					"    \"\"\"\n",
					"    Clean hash_id column and handle deduplication\n",
					"    \"\"\"\n",
					"    logger.info(\"Cleaning hash_id and handling deduplication\")\n",
					"    \n",
					"    if \"hash_id\" not in df.columns:\n",
					"        logger.error(\"hash_id column not found in DataFrame\")\n",
					"        return df\n",
					"    \n",
					"    initial_count = df.count()\n",
					"    logger.info(f\"Initial row count: {initial_count:,}\")\n",
					"    \n",
					"    # Clean hash_id values - handle scientific notation and invalid values\n",
					"    df = df.withColumn(\"hash_id_clean\",\n",
					"        when(\n",
					"            (col(\"hash_id\").isNull()) |\n",
					"            (upper(trim(col(\"hash_id\").cast(\"string\"))).isin([\"\", \"NAN\", \"NULL\", \"NONE\"])),\n",
					"            lit(None)\n",
					"        ).otherwise(\n",
					"            # Try to convert scientific notation to bigint\n",
					"            when(\n",
					"                col(\"hash_id\").cast(\"string\").rlike(\".*[eE].*\"),\n",
					"                col(\"hash_id\").cast(\"double\").cast(\"bigint\")\n",
					"            ).otherwise(\n",
					"                col(\"hash_id\").cast(\"bigint\")\n",
					"            )\n",
					"        )\n",
					"    ).drop(\"hash_id\").withColumnRenamed(\"hash_id_clean\", \"hash_id\")\n",
					"    \n",
					"    # Remove rows with null hash_id\n",
					"    df = df.filter(col(\"hash_id\").isNotNull())\n",
					"    \n",
					"    after_null_removal = df.count()\n",
					"    logger.info(f\"Records after null hash_id removal: {after_null_removal:,}\")\n",
					"    \n",
					"    # Check for and handle duplicates\n",
					"    duplicate_count = df.count() - df.select(\"hash_id\").distinct().count()\n",
					"    \n",
					"    if duplicate_count > 0:\n",
					"        logger.info(f\"Found {duplicate_count:,} duplicate hash_id values - removing duplicates\")\n",
					"        \n",
					"        # Keep only the first occurrence of each hash_id\n",
					"        window_spec = Window.partitionBy(\"hash_id\").orderBy(col(\"hash_id\"))\n",
					"        df = df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"               .filter(col(\"row_num\") == 1) \\\n",
					"               .drop(\"row_num\")\n",
					"        \n",
					"        final_count = df.count()\n",
					"        logger.info(f\"Records after deduplication: {final_count:,}\")\n",
					"    \n",
					"    return df\n",
					"\n",
					"def add_missing_columns_and_reorder(df):\n",
					"    \"\"\"\n",
					"    Add missing columns and reorder according to final schema\n",
					"    \"\"\"\n",
					"    logger.info(\"Adding missing columns and reordering\")\n",
					"    \n",
					"    # Add missing columns\n",
					"    for col_name in FINAL_COLUMNS:\n",
					"        if col_name not in df.columns:\n",
					"            if col_name == 'postcode_start':\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"string\"))  # Will be generated by PostgreSQL\n",
					"            elif col_name == 'part_no':\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"string\"))  # Will be generated by PostgreSQL\n",
					"            elif col_name == 'address6':\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"string\"))  # Usually null\n",
					"            else:\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"string\"))\n",
					"            logger.info(f\"Added missing column: {col_name}\")\n",
					"    \n",
					"    # Reorder columns to match PostgreSQL schema\n",
					"    df = df.select(*FINAL_COLUMNS)\n",
					"    \n",
					"    return df\n",
					"\n",
					"\n",
					"\n",
					"\n",
					"\n",
					"def check_input_data():\n",
					"    \"\"\"\n",
					"    Quick function to check input data availability\n",
					"    \"\"\"\n",
					"    try:\n",
					"        logger.info(f\"Checking input data availability: {INPUT_DELTA_PATH}\")\n",
					"        \n",
					"        if not mssparkutils.fs.exists(INPUT_DELTA_PATH):\n",
					"            logger.error(f\"❌ Input path does not exist: {INPUT_DELTA_PATH}\")\n",
					"            return False\n",
					"        \n",
					"        df = spark.read.format(\"delta\").load(INPUT_DELTA_PATH)\n",
					"        total_count = df.count()\n",
					"        logger.info(f\"✅ Total records in Delta table: {total_count:,}\")\n",
					"        \n",
					"        if total_count == 0:\n",
					"            logger.warning(\"⚠️ Delta table exists but contains no records\")\n",
					"            return False\n",
					"        \n",
					"        # Check for critical columns\n",
					"        critical_cols = ['hash_id', 'lname', 'fname', 'address']\n",
					"        missing_cols = [col for col in critical_cols if col not in df.columns]\n",
					"        if missing_cols:\n",
					"            logger.error(f\"❌ Missing critical columns: {missing_cols}\")\n",
					"            return False\n",
					"        \n",
					"        # Check sensitive columns\n",
					"        sensitive_cols = [col for col in SENSITIVE_COLUMNS.keys() if SENSITIVE_COLUMNS[col].get(\"protect\", False)]\n",
					"        found_sensitive = [col for col in sensitive_cols if col in df.columns]\n",
					"        logger.info(f\"📋 Found {len(found_sensitive)} sensitive columns to protect: {found_sensitive}\")\n",
					"        \n",
					"        return True\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"❌ Error checking input data: {str(e)}\")\n",
					"        return False\n",
					"\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def save_as_postgresql_ready_delta_table(df, base_path):\n",
					"    \"\"\"\n",
					"    Save processed DataFrame as PostgreSQL-ready Delta table\n",
					"    \"\"\"\n",
					"    try:\n",
					"        logger = logging.getLogger(__name__)\n",
					"        logger.info(\"🔄 Saving as PostgreSQL-ready Delta table...\")\n",
					"        \n",
					"        # Apply PostgreSQL schema constraints\n",
					"        postgresql_df = prepare_postgresql_schema(df)\n",
					"        \n",
					"        # Create Delta table path\n",
					"        table_name = \"voters_postgresql_ready\"\n",
					"        delta_table_path = f\"{base_path}/{table_name}\"\n",
					"        \n",
					"        # Get record count\n",
					"        record_count = postgresql_df.count()\n",
					"        logger.info(f\"   Records to save: {record_count:,}\")\n",
					"        \n",
					"        # Save as Delta table with partitioning\n",
					"        postgresql_df.write \\\n",
					"            .format(\"delta\") \\\n",
					"            .mode(\"overwrite\") \\\n",
					"            .option(\"overwriteSchema\", \"true\") \\\n",
					"            .option(\"mergeSchema\", \"true\") \\\n",
					"            .partitionBy(\"creation_date\") \\\n",
					"            .save(delta_table_path)\n",
					"        \n",
					"        logger.info(f\"✅ Delta table saved: {delta_table_path}\")\n",
					"        \n",
					"        # Verify the save\n",
					"        verification_df = spark.read.format(\"delta\").load(delta_table_path)\n",
					"        saved_count = verification_df.count()\n",
					"        \n",
					"        if saved_count == record_count:\n",
					"            logger.info(f\"✅ Verification passed: {saved_count:,} records\")\n",
					"            \n",
					"            # Optimize the table\n",
					"            try:\n",
					"                delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
					"                delta_table.optimize().executeCompaction()\n",
					"                delta_table.optimize().executeZOrderBy(\"hash_id\")\n",
					"                logger.info(\"✅ Table optimized\")\n",
					"            except Exception as opt_error:\n",
					"                logger.warning(f\"⚠️ Optimization failed: {opt_error}\")\n",
					"            \n",
					"            return {\n",
					"                \"success\": True,\n",
					"                \"delta_path\": delta_table_path,\n",
					"                \"record_count\": saved_count,\n",
					"                \"table_name\": table_name\n",
					"            }\n",
					"        else:\n",
					"            return {\n",
					"                \"success\": False,\n",
					"                \"error\": f\"Record count mismatch: expected {record_count}, got {saved_count}\"\n",
					"            }\n",
					"            \n",
					"    except Exception as e:\n",
					"        logger.error(f\"❌ Error saving Delta table: {str(e)}\")\n",
					"        return {\n",
					"            \"success\": False,\n",
					"            \"error\": str(e)\n",
					"        }\n",
					"\n",
					"def prepare_postgresql_schema(df):\n",
					"    \"\"\"\n",
					"    Ensure DataFrame schema matches PostgreSQL requirements\n",
					"    \"\"\"\n",
					"    # Schema constraints for PostgreSQL compatibility\n",
					"    schema_map = {\n",
					"        \"part_no\": (\"string\", 10),\n",
					"        \"register_lett\": (\"string\", 5),\n",
					"        \"poll_number\": (\"string\", 5),\n",
					"        \"new_marker\": (\"string\", 1),\n",
					"        \"title\": (\"string\", 10),\n",
					"        \"lname\": (\"string\", 20),\n",
					"        \"fname\": (\"string\", 20),\n",
					"        \"dob\": (\"date\", None),\n",
					"        \"flags\": (\"string\", 2),\n",
					"        \"address\": (\"string\", 35),\n",
					"        \"address2\": (\"string\", 35),\n",
					"        \"address3\": (\"string\", 35),\n",
					"        \"address4\": (\"string\", 35),\n",
					"        \"address5\": (\"string\", 35),\n",
					"        \"address6\": (\"string\", 35),\n",
					"        \"zip\": (\"string\", 10),\n",
					"        \"date_selected1\": (\"date\", None),\n",
					"        \"date_selected2\": (\"date\", None),\n",
					"        \"date_selected3\": (\"date\", None),\n",
					"        \"rec_num\": (\"integer\", None),\n",
					"        \"perm_disqual\": (\"string\", 1),\n",
					"        \"source_id\": (\"string\", 1),\n",
					"        \"postcode_start\": (\"string\", 10),\n",
					"        \"creation_date\": (\"string\", 10),\n",
					"        \"hash_id\": (\"bigint\", None)\n",
					"    }\n",
					"    \n",
					"    # Apply schema constraints\n",
					"    for col_name, (data_type, max_length) in schema_map.items():\n",
					"        if col_name in df.columns:\n",
					"            # Cast to correct type\n",
					"            if data_type == \"string\":\n",
					"                df = df.withColumn(col_name, col(col_name).cast(\"string\"))\n",
					"                if max_length:\n",
					"                    df = df.withColumn(col_name, \n",
					"                                     when(col(col_name).isNotNull(),\n",
					"                                          substring(col(col_name), 1, max_length))\n",
					"                                     .otherwise(col(col_name)))\n",
					"            elif data_type == \"date\":\n",
					"                df = df.withColumn(col_name, col(col_name).cast(\"date\"))\n",
					"            elif data_type == \"integer\":\n",
					"                df = df.withColumn(col_name, col(col_name).cast(\"integer\"))\n",
					"            elif data_type == \"bigint\":\n",
					"                df = df.withColumn(col_name, col(col_name).cast(\"bigint\"))\n",
					"        else:\n",
					"            # Add missing columns\n",
					"            if data_type == \"string\":\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"string\"))\n",
					"            elif data_type == \"date\":\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"date\"))\n",
					"            elif data_type == \"integer\":\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"integer\"))\n",
					"            elif data_type == \"bigint\":\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"bigint\"))\n",
					"    \n",
					"    # Ensure proper column order\n",
					"    ordered_columns = list(schema_map.keys())\n",
					"    existing_columns = [col for col in ordered_columns if col in df.columns]\n",
					"    df = df.select(*existing_columns)\n",
					"    \n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def main_sanitization_pipeline():\n",
					"    \"\"\"\n",
					"    Main sanitization pipeline function with sensitive data protection\n",
					"    \"\"\"\n",
					"    logger.info(\"🚀 Starting Synapse PySpark Data Sanitization Pipeline with Sensitive Data Protection\")\n",
					"    logger.info(f\"🔒 Protection mode: {PROTECTION_MODE}\")\n",
					"    \n",
					"    try:\n",
					"        # Step 1: Read from Delta Lake\n",
					"        logger.info(f\"Step 1: Reading from Delta Lake: {INPUT_DELTA_PATH}\")\n",
					"        \n",
					"        if not mssparkutils.fs.exists(INPUT_DELTA_PATH):\n",
					"            logger.error(f\"❌ Input Delta table not found: {INPUT_DELTA_PATH}\")\n",
					"            return None\n",
					"        \n",
					"        df = spark.read.format(\"delta\").load(INPUT_DELTA_PATH)\n",
					"        initial_count = df.count()\n",
					"        logger.info(f\"✅ Loaded {initial_count:,} records from Delta table\")\n",
					"        \n",
					"        if initial_count == 0:\n",
					"            logger.error(\"❌ No data found in Delta table\")\n",
					"            return None\n",
					"        \n",
					"        # Step 2: Clean basic fields first (before protection)\n",
					"        logger.info(\"Step 2: Cleaning basic fields\")\n",
					"        df = careful_clean_text_fields(df)\n",
					"        df = clean_and_convert_timestamps(df)\n",
					"        df = clean_register_lett_and_poll_number(df)\n",
					"        \n",
					"        # Step 3: Protect sensitive data\n",
					"        logger.info(\"Step 3: Protecting sensitive data\")\n",
					"        df = protect_sensitive_data(df)\n",
					"        \n",
					"        # Step 4: Clean hash_id and deduplicate\n",
					"        logger.info(\"Step 4: Cleaning hash_id and deduplication\")\n",
					"        df = clean_hash_id_and_deduplicate(df)\n",
					"        \n",
					"        # Step 5: Validate and fix required fields\n",
					"        logger.info(\"Step 5: Validating required fields\")\n",
					"        df = validate_and_fix_required_fields(df)\n",
					"        \n",
					"        # Step 6: Add missing columns and reorder\n",
					"        logger.info(\"Step 6: Finalizing schema\")\n",
					"        df = add_missing_columns_and_reorder(df)\n",
					"        \n",
					"        final_count = df.count()\n",
					"        logger.info(f\"✅ Data processing complete: {final_count:,} records\")\n",
					"        logger.info(f\"Records removed during processing: {initial_count - final_count:,}\")\n",
					"        \n",
					"        # Step 7: Export to blob storage\n",
					"        logger.info(\"Step 7: Exporting to blob storage\")\n",
					"        \n",
					"        DELTA_BASE_PATH = \"abfss://juror-etl@yourstorageaccount.dfs.core.windows.net/delta_tables\"\n",
					"    \n",
					"    try:\n",
					"        print(\"🔄 Saving processed data as Delta table...\")\n",
					"        \n",
					"        # Save as PostgreSQL-ready Delta table\n",
					"        result = save_as_postgresql_ready_delta_table(df, DELTA_BASE_PATH)\n",
					"        \n",
					"        if result[\"success\"]:\n",
					"            print(f\"🎉 SUCCESS! Delta table created:\")\n",
					"            print(f\"   📍 Path: {result['delta_path']}\")\n",
					"            print(f\"   📊 Records: {result['record_count']:,}\")\n",
					"            print(f\"   📋 Table: {result['table_name']}\")\n",
					"            print(f\"\\n🚀 Ready for ADF Copy Activity to PostgreSQL!\")\n",
					"            \n",
					"            # Return the path for any downstream processing\n",
					"            return result\n",
					"            \n",
					"        else:\n",
					"            print(f\"❌ FAILED to create Delta table:\")\n",
					"            print(f\"   Error: {result['error']}\")\n",
					"            return None\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"❌ Sanitization pipeline failed: {str(e)}\")\n",
					"        import traceback\n",
					"        print(\"Full traceback:\")\n",
					"        print(traceback.format_exc())\n",
					"        return None"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# ============================================================================\n",
					"# STEP 5: Update your script execution\n",
					"# ============================================================================\n",
					"\n",
					"if __name__ == \"__main__\":\n",
					"    # Configure logging\n",
					"    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"    \n",
					"    print(\"🚀 Starting Enhanced Voter Data Pipeline with Delta Lake\")\n",
					"    \n",
					"    # Run your main pipeline\n",
					"    result = main_sanitization_pipeline()\n",
					"    \n",
					"    if result:\n",
					"        print(\"\\n\" + \"=\"*60)\n",
					"        print(\"🎉 PIPELINE COMPLETED SUCCESSFULLY!\")\n",
					"        print(\"=\"*60)\n",
					"        print(f\"Delta Table Path: {result['delta_path']}\")\n",
					"        print(f\"Records Processed: {result['record_count']:,}\")\n",
					"        print(\"\\nNext Steps:\")\n",
					"        print(\"1. Configure ADF Copy Activity\")\n",
					"        print(\"2. Source: Delta table (Parquet format)\")\n",
					"        print(\"3. Sink: PostgreSQL database\")\n",
					"        print(\"4. Run the ADF pipeline\")\n",
					"        print(\"=\"*60)\n",
					"    else:\n",
					"        print(\"\\n\" + \"=\"*60)\n",
					"        print(\"❌ PIPELINE FAILED\")\n",
					"        print(\"=\"*60)\n",
					"        print(\"Check the error messages above for details.\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# ============================================================================\n",
					"# MIGRATION CHECKLIST:\n",
					"# ============================================================================\n",
					"\n",
					"\"\"\"\n",
					"✅ Step 1: Add Delta imports\n",
					"✅ Step 2: Update Spark session config  \n",
					"✅ Step 3: Add new functions (save_as_postgresql_ready_delta_table, prepare_postgresql_schema)\n",
					"✅ Step 4: Replace export_to_blob_storage call with save_as_postgresql_ready_delta_table\n",
					"✅ Step 5: Update main function ending\n",
					"✅ Step 6: Update storage account name in DELTA_BASE_PATH\n",
					"✅ Step 7: Test the new pipeline\n",
					"✅ Step 8: Configure ADF Copy Activity to use Delta table as source\n",
					"\"\"\""
				],
				"execution_count": null
			}
		]
	}
}