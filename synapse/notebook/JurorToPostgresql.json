{
	"name": "JurorToPostgresql",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkprod",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "2624d6c3-d0af-4b71-99c4-06b4cf92a837"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5ca62022-6aa2-4cee-aaa7-e7536c8d566c/resourceGroups/baubais-data-factory-rg-prod/providers/Microsoft.Synapse/workspaces/baubais-synapse-prod/bigDataPools/sparkprod",
				"name": "sparkprod",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkprod",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"\"\"\"Synapse PySpark Data Sanitization Pipeline with Sensitive Data Hashing/Masking\n",
					"Reads from deduplicated delta lake table, hashes/masks sensitive data, and saves to blob storage\n",
					"Uses production data but protects sensitive columns for test environment\n",
					"\"\"\"\n",
					"\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql.window import Window\n",
					"from notebookutils import mssparkutils\n",
					"import logging\n",
					"from datetime import datetime\n",
					"import time\n",
					"\n",
					"# Configure logging\n",
					"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"logger = logging.getLogger()\n",
					"\n",
					"# Initialize Spark session with Delta Lake support\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"Data Sanitization Pipeline with Sensitive Data Protection\") \\\n",
					"    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
					"    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
					"    .getOrCreate()"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"# Configuration\n",
					"STORAGE_ACCOUNT = \"baubaisadfsaprod\"  # Update with your storage account\n",
					"INPUT_DELTA_PATH = f\"abfss://dl-juror-eric-voters-temp@{STORAGE_ACCOUNT}.dfs.core.windows.net/voters_deduplicated_delta\"\n",
					"OUTPUT_BLOB_PATH = f\"abfss://juror-etl@{STORAGE_ACCOUNT}.dfs.core.windows.net/test_data_export\"\n",
					"\n",
					"# Define final column order for PostgreSQL (same schema)\n",
					"FINAL_COLUMNS = [\n",
					"    \"part_no\", \"register_lett\", \"poll_number\", \"new_marker\", \"title\",\n",
					"    \"lname\", \"fname\", \"dob\", \"flags\", \"address\", \"address2\", \"address3\",\n",
					"    \"address4\", \"address5\", \"address6\", \"zip\", \"date_selected1\",\n",
					"    \"date_selected2\", \"date_selected3\", \"rec_num\", \"perm_disqual\",\n",
					"    \"source_id\", \"postcode_start\", \"creation_date\", \"hash_id\"\n",
					"]\n",
					"\n",
					"# Configuration for sensitive data protection\n",
					"# Set PROTECTION_MODE to control how sensitive data is handled:\n",
					"# \"HASH\" - Hash sensitive data (irreversible but maintains uniqueness)\n",
					"# \"MASK\" - Mask sensitive data (replaces with pattern-preserving values)\n",
					"# \"DISABLE\" - Set sensitive fields to NULL (removes data completely)\n",
					"PROTECTION_MODE = \"HASH\"  # Change to \"MASK\" or \"DISABLE\" as needed\n",
					"\n",
					"# Define sensitive columns that need protection\n",
					"SENSITIVE_COLUMNS = {\n",
					"    \"lname\": {\"protect\": True, \"max_length\": 20, \"type\": \"name\"},\n",
					"    \"fname\": {\"protect\": True, \"max_length\": 20, \"type\": \"name\"},\n",
					"    \"dob\": {\"protect\": True, \"type\": \"date\"},\n",
					"    \"address\": {\"protect\": True, \"max_length\": 35, \"type\": \"address\"},\n",
					"    \"address2\": {\"protect\": True, \"max_length\": 35, \"type\": \"address\"},\n",
					"    \"address3\": {\"protect\": True, \"max_length\": 35, \"type\": \"address\"},\n",
					"    \"address5\": {\"protect\": True, \"max_length\": 35, \"type\": \"address\"},\n",
					"    \"zip\": {\"protect\": True, \"max_length\": 10, \"type\": \"postcode\"},\n",
					"    # Non-sensitive columns (keep as-is)\n",
					"    \"address4\": {\"protect\": False, \"max_length\": 35, \"type\": \"address\"},  # Keep for data integrity\n",
					"    \"register_lett\": {\"protect\": False, \"max_length\": 5, \"type\": \"identifier\"},\n",
					"    \"poll_number\": {\"protect\": False, \"max_length\": 5, \"type\": \"identifier\"},\n",
					"    \"rec_num\": {\"protect\": False, \"type\": \"identifier\"},\n",
					"    \"hash_id\": {\"protect\": False, \"type\": \"identifier\"}\n",
					"}\n",
					"\n",
					"def hash_sensitive_field(df, column_name, max_length=None, field_type=\"string\"):\n",
					"    \"\"\"\n",
					"    Hash a sensitive field using xxhash64 for consistent results\n",
					"    \"\"\"\n",
					"    if column_name not in df.columns:\n",
					"        return df\n",
					"    \n",
					"    logger.info(f\"Hashing sensitive field: {column_name}\")\n",
					"    \n",
					"    # Create hashed version with deterministic salt based on column name\n",
					"    salt = f\"TEST_SALT_{column_name.upper()}\"\n",
					"    \n",
					"    if field_type == \"date\":\n",
					"        # For dates, hash but maintain date format\n",
					"        df = df.withColumn(f\"{column_name}_hashed\",\n",
					"            when(col(column_name).isNotNull(),\n",
					"                 # Create a deterministic date based on hash\n",
					"                 date_add(\n",
					"                     to_date(lit(\"1950-01-01\")),\n",
					"                     (abs(hash(col(\"dob\"))) % 18250).cast(\"int\")  # Key fix: .cast(\"int\")\n",
					"                 )\n",
					"            ).otherwise(col(column_name))\n",
					"        )\n",
					"    else:\n",
					"        # For text fields, create hash-based values\n",
					"        df = df.withColumn(f\"{column_name}_hashed\",\n",
					"            when(col(column_name).isNotNull(),\n",
					"                 concat(\n",
					"                     lit(\"HASH_\"),\n",
					"                     substring(\n",
					"                         hex(xxhash64(concat(col(column_name), lit(salt)))),\n",
					"                         1, \n",
					"                         max_length - 5 if max_length else 15\n",
					"                     )\n",
					"                 )\n",
					"            ).otherwise(col(column_name))\n",
					"        )\n",
					"        \n",
					"        # Apply length limit if specified\n",
					"        if max_length:\n",
					"            df = df.withColumn(f\"{column_name}_hashed\",\n",
					"                substring(col(f\"{column_name}_hashed\"), 1, max_length)\n",
					"            )\n",
					"    \n",
					"    # Replace original column with hashed version\n",
					"    df = df.drop(column_name).withColumnRenamed(f\"{column_name}_hashed\", column_name)\n",
					"    \n",
					"    return df\n",
					"\n",
					"def mask_sensitive_field(df, column_name, max_length=None, field_type=\"string\"):\n",
					"    \"\"\"\n",
					"    Mask a sensitive field with pattern-preserving values\n",
					"    \"\"\"\n",
					"    if column_name not in df.columns:\n",
					"        return df\n",
					"    \n",
					"    logger.info(f\"Masking sensitive field: {column_name}\")\n",
					"    \n",
					"    if field_type == \"date\":\n",
					"        # For dates, shift by a random but consistent amount\n",
					"        df = df.withColumn(f\"{column_name}_masked\",\n",
					"            when(col(column_name).isNotNull(),\n",
					"                 date_add(\n",
					"                     col(column_name),\n",
					"                     # Consistent shift based on hash_id\n",
					"                     ((abs(col(\"hash_id\")) % 3650) + 1)  # 1-10 years shift\n",
					"                 )\n",
					"            ).otherwise(col(column_name))\n",
					"        )\n",
					"    elif field_type == \"name\":\n",
					"        # For names, replace with pattern-preserving values\n",
					"        df = df.withColumn(f\"{column_name}_masked\",\n",
					"            when(col(column_name).isNotNull(),\n",
					"                 concat(\n",
					"                     lit(\"MASKED_\"),\n",
					"                     substring(\n",
					"                         regexp_replace(col(column_name), \"[A-Z]\", \"X\"),\n",
					"                         1,\n",
					"                         max_length - 7 if max_length else 10\n",
					"                     )\n",
					"                 )\n",
					"            ).otherwise(col(column_name))\n",
					"        )\n",
					"    elif field_type == \"address\":\n",
					"        # For addresses, replace with generic test addresses\n",
					"        df = df.withColumn(f\"{column_name}_masked\",\n",
					"            when(col(column_name).isNotNull(),\n",
					"                 concat(\n",
					"                     lit(\"TEST ADDRESS \"),\n",
					"                     (abs(col(\"hash_id\")) % 999 + 1).cast(\"string\")\n",
					"                 )\n",
					"            ).otherwise(col(column_name))\n",
					"        )\n",
					"    elif field_type == \"postcode\":\n",
					"        # For postcodes, create test postcodes with valid UK format\n",
					"        df = df.withColumn(f\"{column_name}_masked\",\n",
					"            when(col(column_name).isNotNull(),\n",
					"                 concat(\n",
					"                     lit(\"TE\"),\n",
					"                     (abs(col(\"hash_id\")) % 9 + 1).cast(\"string\"),\n",
					"                     lit(\" \"),\n",
					"                     (abs(col(\"hash_id\")) % 9 + 1).cast(\"string\"),\n",
					"                     lit(\"XX\")\n",
					"                 )\n",
					"            ).otherwise(col(column_name))\n",
					"        )\n",
					"    else:\n",
					"        # Default masking for other types\n",
					"        df = df.withColumn(f\"{column_name}_masked\",\n",
					"            when(col(column_name).isNotNull(),\n",
					"                 concat(lit(\"MASKED_\"), substring(col(column_name), 1, 5))\n",
					"            ).otherwise(col(column_name))\n",
					"        )\n",
					"    \n",
					"    # Apply length limit if specified\n",
					"    if max_length:\n",
					"        df = df.withColumn(f\"{column_name}_masked\",\n",
					"            substring(col(f\"{column_name}_masked\"), 1, max_length)\n",
					"        )\n",
					"    \n",
					"    # Replace original column with masked version\n",
					"    df = df.drop(column_name).withColumnRenamed(f\"{column_name}_masked\", column_name)\n",
					"    \n",
					"    return df\n",
					"\n",
					"def disable_sensitive_field(df, column_name):\n",
					"    \"\"\"\n",
					"    Disable a sensitive field by setting it to NULL\n",
					"    \"\"\"\n",
					"    if column_name not in df.columns:\n",
					"        return df\n",
					"    \n",
					"    logger.info(f\"Disabling sensitive field: {column_name}\")\n",
					"    \n",
					"    df = df.withColumn(column_name, lit(None))\n",
					"    \n",
					"    return df\n",
					"\n",
					"def protect_sensitive_data(df):\n",
					"    \"\"\"\n",
					"    Protect sensitive data based on the configured protection mode\n",
					"    \"\"\"\n",
					"    logger.info(f\"ðŸ”’ Starting sensitive data protection using mode: {PROTECTION_MODE}\")\n",
					"    \n",
					"    initial_count = df.count()\n",
					"    logger.info(f\"Records to protect: {initial_count:,}\")\n",
					"    \n",
					"    protection_stats = {\"hashed\": 0, \"masked\": 0, \"disabled\": 0, \"skipped\": 0}\n",
					"    \n",
					"    for column_name, config in SENSITIVE_COLUMNS.items():\n",
					"        if column_name not in df.columns:\n",
					"            logger.warning(f\"Column {column_name} not found in DataFrame\")\n",
					"            continue\n",
					"            \n",
					"        if not config.get(\"protect\", False):\n",
					"            logger.info(f\"Skipping non-sensitive column: {column_name}\")\n",
					"            protection_stats[\"skipped\"] += 1\n",
					"            continue\n",
					"        \n",
					"        max_length = config.get(\"max_length\")\n",
					"        field_type = config.get(\"type\", \"string\")\n",
					"        \n",
					"        if PROTECTION_MODE == \"HASH\":\n",
					"            df = hash_sensitive_field(df, column_name, max_length, field_type)\n",
					"            protection_stats[\"hashed\"] += 1\n",
					"            \n",
					"        elif PROTECTION_MODE == \"MASK\":\n",
					"            df = mask_sensitive_field(df, column_name, max_length, field_type)\n",
					"            protection_stats[\"masked\"] += 1\n",
					"            \n",
					"        elif PROTECTION_MODE == \"DISABLE\":\n",
					"            df = disable_sensitive_field(df, column_name)\n",
					"            protection_stats[\"disabled\"] += 1\n",
					"            \n",
					"        else:\n",
					"            logger.error(f\"Unknown protection mode: {PROTECTION_MODE}\")\n",
					"            protection_stats[\"skipped\"] += 1\n",
					"    \n",
					"    logger.info(f\"âœ… Sensitive data protection complete:\")\n",
					"    logger.info(f\"   - Hashed: {protection_stats['hashed']} columns\")\n",
					"    logger.info(f\"   - Masked: {protection_stats['masked']} columns\")\n",
					"    logger.info(f\"   - Disabled: {protection_stats['disabled']} columns\")\n",
					"    logger.info(f\"   - Skipped: {protection_stats['skipped']} columns\")\n",
					"    \n",
					"    return df\n",
					"\n",
					"def careful_clean_text_fields(df):\n",
					"    \"\"\"\n",
					"    Carefully clean text fields without destroying valid data\n",
					"    \"\"\"\n",
					"    logger.info(\"Starting careful text field cleaning\")\n",
					"    \n",
					"    # Define text columns to clean (excluding sensitive ones that were already processed)\n",
					"    text_columns = [\"part_no\", \"register_lett\", \"new_marker\", \"title\", \n",
					"                    \"flags\", \"perm_disqual\", \"source_id\", \"postcode_start\", \"creation_date\"]\n",
					"    \n",
					"    for col_name in text_columns:\n",
					"        if col_name in df.columns:\n",
					"            df = df.withColumn(col_name,\n",
					"                when(\n",
					"                    (col(col_name).isNull()) |\n",
					"                    (upper(trim(col(col_name))) == \"NAN\") |\n",
					"                    (upper(trim(col(col_name))) == \"NULL\") |\n",
					"                    (upper(trim(col(col_name))) == \"NONE\") |\n",
					"                    (trim(col(col_name)) == \"\"),\n",
					"                    lit(None)\n",
					"                ).otherwise(trim(col(col_name)))\n",
					"            )\n",
					"    \n",
					"    logger.info(\"âœ… Completed careful text field cleaning\")\n",
					"    return df\n",
					"\n",
					"def clean_and_convert_timestamps(df):\n",
					"    \"\"\"\n",
					"    Clean and convert timestamp fields properly\n",
					"    \"\"\"\n",
					"    logger.info(\"Cleaning timestamp fields\")\n",
					"    \n",
					"    timestamp_columns = ['dob']\n",
					"    \n",
					"    for col_name in timestamp_columns:\n",
					"        if col_name in df.columns:\n",
					"            df = df.withColumn(col_name,\n",
					"                when(\n",
					"                    (col(col_name).isNull()) |\n",
					"                    (trim(col(col_name)) == \"\") |\n",
					"                    (upper(trim(col(col_name))).isin([\"NULL\", \"NAN\", \"NA\", \"NONE\"])),\n",
					"                    lit(None)\n",
					"                ).otherwise(\n",
					"                    # Try to convert to date if it's a valid date string\n",
					"                    coalesce(\n",
					"                        to_date(col(col_name), \"yyyy-MM-dd\"),\n",
					"                        to_date(col(col_name), \"dd/MM/yyyy\"),\n",
					"                        to_date(col(col_name), \"MM/dd/yyyy\")\n",
					"                    )\n",
					"                )\n",
					"            )\n",
					"    \n",
					"    return df\n",
					"\n",
					"def clean_register_lett_and_poll_number(df):\n",
					"    \"\"\"\n",
					"    Clean register_lett and poll_number fields with proper truncation\n",
					"    \"\"\"\n",
					"    logger.info(\"Cleaning register_lett and poll_number fields\")\n",
					"    \n",
					"    # Clean register_lett: strip whitespace and truncate to 5 chars\n",
					"    if \"register_lett\" in df.columns:\n",
					"        df = df.withColumn(\"register_lett\",\n",
					"            when(col(\"register_lett\").isNull(), lit(None))\n",
					"            .otherwise(substring(trim(col(\"register_lett\").cast(\"string\")), 1, 5))\n",
					"        )\n",
					"        logger.info(\"âœ… Cleaned register_lett: stripped and truncated to 5 characters\")\n",
					"    \n",
					"    # Clean poll_number: remove .0 from floats, ensure numeric, truncate to 5 chars\n",
					"    if \"poll_number\" in df.columns:\n",
					"        df = df.withColumn(\"poll_number_temp\",\n",
					"            regexp_replace(col(\"poll_number\").cast(\"string\"), \"\\\\.0$\", \"\")\n",
					"        ).withColumn(\"poll_number_numeric\",\n",
					"            col(\"poll_number_temp\").cast(\"double\")\n",
					"        ).withColumn(\"poll_number\",\n",
					"            when(col(\"poll_number_numeric\").isNull(), lit(None))\n",
					"            .otherwise(substring(col(\"poll_number_numeric\").cast(\"int\").cast(\"string\"), 1, 5))\n",
					"        ).drop(\"poll_number_temp\", \"poll_number_numeric\")\n",
					"        \n",
					"        logger.info(\"âœ… Cleaned poll_number: removed .0 suffix, ensured numeric, truncated to 5 characters\")\n",
					"    \n",
					"    return df\n",
					"\n",
					"def validate_and_fix_required_fields(df):\n",
					"    \"\"\"\n",
					"    Validate and fix fields that are required by PostgreSQL schema\n",
					"    \"\"\"\n",
					"    logger.info(\"Validating and fixing required fields\")\n",
					"    \n",
					"    initial_count = df.count()\n",
					"    \n",
					"    # Critical fields that cannot be NULL in PostgreSQL\n",
					"    critical_fields = ['lname', 'fname', 'address', 'hash_id']\n",
					"    \n",
					"    for field in critical_fields:\n",
					"        if field in df.columns:\n",
					"            null_count = df.filter(col(field).isNull()).count()\n",
					"            if null_count > 0:\n",
					"                logger.warning(f\"Found {null_count:,} null values in critical field {field}\")\n",
					"                \n",
					"                # Remove records with null critical fields\n",
					"                df = df.filter(col(field).isNotNull())\n",
					"    \n",
					"    # Ensure address4 is never null (required for PostgreSQL schema)\n",
					"    if \"address4\" in df.columns:\n",
					"        null_address4_count = df.filter(col(\"address4\").isNull()).count()\n",
					"        if null_address4_count > 0:\n",
					"            logger.info(f\"Setting {null_address4_count:,} null address4 values to 'UNKNOWN'\")\n",
					"            df = df.withColumn(\"address4\",\n",
					"                when(col(\"address4\").isNull(), lit(\"UNKNOWN\"))\n",
					"                .otherwise(col(\"address4\"))\n",
					"            )\n",
					"    \n",
					"    final_count = df.count()\n",
					"    removed_count = initial_count - final_count\n",
					"    \n",
					"    if removed_count > 0:\n",
					"        logger.info(f\"Removed {removed_count:,} records with null critical fields\")\n",
					"    \n",
					"    return df\n",
					"\n",
					"def clean_hash_id_and_deduplicate(df):\n",
					"    \"\"\"\n",
					"    Clean hash_id column and handle deduplication\n",
					"    \"\"\"\n",
					"    logger.info(\"Cleaning hash_id and handling deduplication\")\n",
					"    \n",
					"    if \"hash_id\" not in df.columns:\n",
					"        logger.error(\"hash_id column not found in DataFrame\")\n",
					"        return df\n",
					"    \n",
					"    initial_count = df.count()\n",
					"    logger.info(f\"Initial row count: {initial_count:,}\")\n",
					"    \n",
					"    # Clean hash_id values - handle scientific notation and invalid values\n",
					"    df = df.withColumn(\"hash_id_clean\",\n",
					"        when(\n",
					"            (col(\"hash_id\").isNull()) |\n",
					"            (upper(trim(col(\"hash_id\").cast(\"string\"))).isin([\"\", \"NAN\", \"NULL\", \"NONE\"])),\n",
					"            lit(None)\n",
					"        ).otherwise(\n",
					"            # Try to convert scientific notation to bigint\n",
					"            when(\n",
					"                col(\"hash_id\").cast(\"string\").rlike(\".*[eE].*\"),\n",
					"                col(\"hash_id\").cast(\"double\").cast(\"bigint\")\n",
					"            ).otherwise(\n",
					"                col(\"hash_id\").cast(\"bigint\")\n",
					"            )\n",
					"        )\n",
					"    ).drop(\"hash_id\").withColumnRenamed(\"hash_id_clean\", \"hash_id\")\n",
					"    \n",
					"    # Remove rows with null hash_id\n",
					"    df = df.filter(col(\"hash_id\").isNotNull())\n",
					"    \n",
					"    after_null_removal = df.count()\n",
					"    logger.info(f\"Records after null hash_id removal: {after_null_removal:,}\")\n",
					"    \n",
					"    # Check for and handle duplicates\n",
					"    duplicate_count = df.count() - df.select(\"hash_id\").distinct().count()\n",
					"    \n",
					"    if duplicate_count > 0:\n",
					"        logger.info(f\"Found {duplicate_count:,} duplicate hash_id values - removing duplicates\")\n",
					"        \n",
					"        # Keep only the first occurrence of each hash_id\n",
					"        window_spec = Window.partitionBy(\"hash_id\").orderBy(col(\"hash_id\"))\n",
					"        df = df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"               .filter(col(\"row_num\") == 1) \\\n",
					"               .drop(\"row_num\")\n",
					"        \n",
					"        final_count = df.count()\n",
					"        logger.info(f\"Records after deduplication: {final_count:,}\")\n",
					"    \n",
					"    return df\n",
					"\n",
					"def add_missing_columns_and_reorder(df):\n",
					"    \"\"\"\n",
					"    Add missing columns and reorder according to final schema\n",
					"    \"\"\"\n",
					"    logger.info(\"Adding missing columns and reordering\")\n",
					"    \n",
					"    # Add missing columns\n",
					"    for col_name in FINAL_COLUMNS:\n",
					"        if col_name not in df.columns:\n",
					"            if col_name == 'postcode_start':\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"string\"))  # Will be generated by PostgreSQL\n",
					"            elif col_name == 'part_no':\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"string\"))  # Will be generated by PostgreSQL\n",
					"            elif col_name == 'address6':\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"string\"))  # Usually null\n",
					"            else:\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"string\"))\n",
					"            logger.info(f\"Added missing column: {col_name}\")\n",
					"    \n",
					"    # Reorder columns to match PostgreSQL schema\n",
					"    df = df.select(*FINAL_COLUMNS)\n",
					"    \n",
					"    return df\n",
					"\n",
					"def export_to_blob_storage(df, output_path, chunk_size=250000):\n",
					"    \"\"\"\n",
					"    Export DataFrame to blob storage in chunks with proper PostgreSQL formatting\n",
					"    \"\"\"\n",
					"    logger.info(f\"Exporting protected data to blob storage: {output_path}\")\n",
					"    \n",
					"    # Ensure output directory exists\n",
					"    try:\n",
					"        if not mssparkutils.fs.exists(output_path):\n",
					"            mssparkutils.fs.mkdirs(output_path)\n",
					"            logger.info(f\"Created output directory: {output_path}\")\n",
					"    except Exception as e:\n",
					"        logger.warning(f\"Could not create directory (may already exist): {e}\")\n",
					"    \n",
					"    total_records = df.count()\n",
					"    logger.info(f\"Total records to export: {total_records:,}\")\n",
					"    \n",
					"    if total_records == 0:\n",
					"        logger.error(\"No records to export\")\n",
					"        return None\n",
					"    \n",
					"    # Format date columns for PostgreSQL (YYYY-MM-DD)\n",
					"    datetime_columns = ['dob']\n",
					"    \n",
					"    for col_name in datetime_columns:\n",
					"        if col_name in df.columns:\n",
					"            df = df.withColumn(col_name,\n",
					"                when(col(col_name).isNull(), lit(None))\n",
					"                .otherwise(date_format(col(col_name), \"yyyy-MM-dd\"))\n",
					"            )\n",
					"    \n",
					"    # Calculate number of chunks\n",
					"    num_chunks = (total_records + chunk_size - 1) // chunk_size\n",
					"    logger.info(f\"Will create {num_chunks} chunks of approximately {chunk_size:,} records each\")\n",
					"    \n",
					"    # Export in chunks\n",
					"    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
					"    \n",
					"    for i in range(num_chunks):\n",
					"        start_idx = i * chunk_size\n",
					"        end_idx = min((i + 1) * chunk_size, total_records)\n",
					"        \n",
					"        # Create chunk using row_number window function\n",
					"        window_spec = Window.orderBy(col(\"hash_id\"))\n",
					"        df_with_row_num = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
					"        \n",
					"        chunk_df = df_with_row_num.filter(\n",
					"            (col(\"row_num\") >= start_idx + 1) & \n",
					"            (col(\"row_num\") <= end_idx)\n",
					"        ).drop(\"row_num\")\n",
					"        \n",
					"        chunk_records = chunk_df.count()\n",
					"        chunk_output_path = f\"{output_path}/protected_chunk_{i+1:03d}_{timestamp}_{PROTECTION_MODE.lower()}\"\n",
					"        \n",
					"        logger.info(f\"Exporting chunk {i+1}/{num_chunks}: {chunk_records:,} records\")\n",
					"        \n",
					"        try:\n",
					"            # Write chunk as CSV with PostgreSQL-compatible options\n",
					"            chunk_df.coalesce(1).write \\\n",
					"                .option(\"header\", \"true\") \\\n",
					"                .option(\"nullValue\", \"\") \\\n",
					"                .option(\"emptyValue\", \"\") \\\n",
					"                .option(\"quote\", '\"') \\\n",
					"                .option(\"escape\", '\"') \\\n",
					"                .option(\"quoteAll\", \"false\") \\\n",
					"                .mode(\"overwrite\") \\\n",
					"                .csv(chunk_output_path)\n",
					"            \n",
					"            logger.info(f\"âœ… Chunk {i+1} exported successfully\")\n",
					"            \n",
					"        except Exception as e:\n",
					"            logger.error(f\"âŒ Failed to export chunk {i+1}: {str(e)}\")\n",
					"            continue\n",
					"    \n",
					"    logger.info(f\"âœ… Export complete! {num_chunks} chunks saved to: {output_path}\")\n",
					"    logger.info(f\"ðŸ“‹ Protection mode used: {PROTECTION_MODE}\")\n",
					"    return output_path\n",
					"\n",
					"def main_sanitization_pipeline():\n",
					"    \"\"\"\n",
					"    Main sanitization pipeline function with sensitive data protection\n",
					"    \"\"\"\n",
					"    logger.info(\"ðŸš€ Starting Synapse PySpark Data Sanitization Pipeline with Sensitive Data Protection\")\n",
					"    logger.info(f\"ðŸ”’ Protection mode: {PROTECTION_MODE}\")\n",
					"    \n",
					"    try:\n",
					"        # Step 1: Read from Delta Lake\n",
					"        logger.info(f\"Step 1: Reading from Delta Lake: {INPUT_DELTA_PATH}\")\n",
					"        \n",
					"        if not mssparkutils.fs.exists(INPUT_DELTA_PATH):\n",
					"            logger.error(f\"âŒ Input Delta table not found: {INPUT_DELTA_PATH}\")\n",
					"            return None\n",
					"        \n",
					"        df = spark.read.format(\"delta\").load(INPUT_DELTA_PATH)\n",
					"        initial_count = df.count()\n",
					"        logger.info(f\"âœ… Loaded {initial_count:,} records from Delta table\")\n",
					"        \n",
					"        if initial_count == 0:\n",
					"            logger.error(\"âŒ No data found in Delta table\")\n",
					"            return None\n",
					"        \n",
					"        # Step 2: Clean basic fields first (before protection)\n",
					"        logger.info(\"Step 2: Cleaning basic fields\")\n",
					"        df = careful_clean_text_fields(df)\n",
					"        df = clean_and_convert_timestamps(df)\n",
					"        df = clean_register_lett_and_poll_number(df)\n",
					"        \n",
					"        # Step 3: Protect sensitive data\n",
					"        logger.info(\"Step 3: Protecting sensitive data\")\n",
					"        df = protect_sensitive_data(df)\n",
					"        \n",
					"        # Step 4: Clean hash_id and deduplicate\n",
					"        logger.info(\"Step 4: Cleaning hash_id and deduplication\")\n",
					"        df = clean_hash_id_and_deduplicate(df)\n",
					"        \n",
					"        # Step 5: Validate and fix required fields\n",
					"        logger.info(\"Step 5: Validating required fields\")\n",
					"        df = validate_and_fix_required_fields(df)\n",
					"        \n",
					"        # Step 6: Add missing columns and reorder\n",
					"        logger.info(\"Step 6: Finalizing schema\")\n",
					"        df = add_missing_columns_and_reorder(df)\n",
					"        \n",
					"        final_count = df.count()\n",
					"        logger.info(f\"âœ… Data processing complete: {final_count:,} records\")\n",
					"        logger.info(f\"Records removed during processing: {initial_count - final_count:,}\")\n",
					"        \n",
					"        # Step 7: Export to blob storage\n",
					"        logger.info(\"Step 7: Exporting to blob storage\")\n",
					"        \n",
					"        export_path = export_to_blob_storage(df, OUTPUT_BLOB_PATH)\n",
					"        \n",
					"        if export_path:\n",
					"            logger.info(\"ðŸŽ‰ Sanitization pipeline completed successfully!\")\n",
					"            logger.info(f\"ðŸ“ Protected data exported to: {export_path}\")\n",
					"            logger.info(f\"ðŸ”’ Protection mode used: {PROTECTION_MODE}\")\n",
					"            \n",
					"            # Show sample of final data (non-sensitive fields only)\n",
					"            logger.info(\"ðŸ“‹ Sample of processed data:\")\n",
					"            sample_data = df.limit(3).select(\"register_lett\", \"poll_number\", \"address4\", \"rec_num\", \"hash_id\").collect()\n",
					"            for i, row in enumerate(sample_data):\n",
					"                logger.info(f\"   Row {i+1}: reg={row.register_lett}, poll={row.poll_number}, addr4={row.address4}, rec={row.rec_num}\")\n",
					"            \n",
					"            return df\n",
					"        else:\n",
					"            logger.error(\"âŒ Export failed\")\n",
					"            return None\n",
					"            \n",
					"    except Exception as e:\n",
					"        logger.error(f\"âŒ Pipeline failed: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(\"Full traceback:\")\n",
					"        logger.error(traceback.format_exc())\n",
					"        return None\n",
					"\n",
					"def check_input_data():\n",
					"    \"\"\"\n",
					"    Quick function to check input data availability\n",
					"    \"\"\"\n",
					"    try:\n",
					"        logger.info(f\"Checking input data availability: {INPUT_DELTA_PATH}\")\n",
					"        \n",
					"        if not mssparkutils.fs.exists(INPUT_DELTA_PATH):\n",
					"            logger.error(f\"âŒ Input path does not exist: {INPUT_DELTA_PATH}\")\n",
					"            return False\n",
					"        \n",
					"        df = spark.read.format(\"delta\").load(INPUT_DELTA_PATH)\n",
					"        total_count = df.count()\n",
					"        logger.info(f\"âœ… Total records in Delta table: {total_count:,}\")\n",
					"        \n",
					"        if total_count == 0:\n",
					"            logger.warning(\"âš ï¸ Delta table exists but contains no records\")\n",
					"            return False\n",
					"        \n",
					"        # Check for critical columns\n",
					"        critical_cols = ['hash_id', 'lname', 'fname', 'address']\n",
					"        missing_cols = [col for col in critical_cols if col not in df.columns]\n",
					"        if missing_cols:\n",
					"            logger.error(f\"âŒ Missing critical columns: {missing_cols}\")\n",
					"            return False\n",
					"        \n",
					"        # Check sensitive columns\n",
					"        sensitive_cols = [col for col in SENSITIVE_COLUMNS.keys() if SENSITIVE_COLUMNS[col].get(\"protect\", False)]\n",
					"        found_sensitive = [col for col in sensitive_cols if col in df.columns]\n",
					"        logger.info(f\"ðŸ“‹ Found {len(found_sensitive)} sensitive columns to protect: {found_sensitive}\")\n",
					"        \n",
					"        return True\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"âŒ Error checking input data: {str(e)}\")\n",
					"        return False\n",
					"\n",
					"# Execute the pipeline\n",
					"if __name__ == \"__main__\":\n",
					"    logger.info(\"ðŸš€ Starting Synapse PySpark Sanitization Pipeline with Sensitive Data Protection\")\n",
					"    logger.info(f\"ðŸ”’ Configuration:\")\n",
					"    logger.info(f\"   - Protection mode: {PROTECTION_MODE}\")\n",
					"    logger.info(f\"   - Input: {INPUT_DELTA_PATH}\")\n",
					"    logger.info(f\"   - Output: {OUTPUT_BLOB_PATH}\")\n",
					"    \n",
					"    # Check input data first\n",
					"    logger.info(\"Step 1: Checking input data availability...\")\n",
					"    if check_input_data():\n",
					"        logger.info(\"âœ… Input data check passed\")\n",
					"        \n",
					"        # Run the sanitization pipeline\n",
					"        logger.info(\"Step 2: Running sanitization pipeline...\")\n",
					"        result_df = main_sanitization_pipeline()\n",
					"        \n",
					"        if result_df is not None:\n",
					"            logger.info(\"ðŸŽ‰ Sanitization pipeline completed successfully\")\n",
					"            logger.info(f\"ðŸ”’ Sensitive data protection applied using {PROTECTION_MODE} mode\")\n",
					"        else:\n",
					"            logger.error(\"âŒ Sanitization pipeline failed\")\n",
					"    else:\n",
					"        logger.error(\"âŒ Input data check failed - cannot proceed\")"
				],
				"execution_count": 3
			}
		]
	}
}