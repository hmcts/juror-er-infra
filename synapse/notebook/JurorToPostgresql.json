{
	"name": "JurorToPostgresql",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkprod",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "10331dd6-4237-4a49-91a1-3d119c79c59c"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5ca62022-6aa2-4cee-aaa7-e7536c8d566c/resourceGroups/baubais-data-factory-rg-prod/providers/Microsoft.Synapse/workspaces/baubais-synapse-prod/bigDataPools/sparkprod",
				"name": "sparkprod",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkprod",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"\"\"\"\n",
					"Synapse PySpark Data Sanitization Pipeline\n",
					"Reads from deduplicated delta lake table and saves sanitized data to blob storage\n",
					"\"\"\"\n",
					"\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql.window import Window\n",
					"from notebookutils import mssparkutils\n",
					"import logging\n",
					"from datetime import datetime\n",
					"import time\n",
					"\n",
					"# Configure logging\n",
					"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"logger = logging.getLogger()\n",
					"\n",
					"# Initialize Spark session with Delta Lake support\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"Data Sanitization Pipeline\") \\\n",
					"    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
					"    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
					"    .getOrCreate()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Configuration\n",
					"STORAGE_ACCOUNT = \"baubaisadfsaprod\"  # Update with your storage account\n",
					"INPUT_DELTA_PATH = f\"abfss://dl-juror-eric-voters-temp@{STORAGE_ACCOUNT}.dfs.core.windows.net/voters_deduplicated_delta\"\n",
					"OUTPUT_BLOB_PATH = f\"abfss://juror-etl@{STORAGE_ACCOUNT}.dfs.core.windows.net/sanitized_export\"\n",
					"\n",
					"# Define final column order for PostgreSQL\n",
					"FINAL_COLUMNS = [\n",
					"    \"part_no\", \"register_lett\", \"poll_number\", \"new_marker\", \"title\",\n",
					"    \"lname\", \"fname\", \"dob\", \"flags\", \"address\", \"address2\", \"address3\",\n",
					"    \"address4\", \"address5\", \"address6\", \"zip\", \"date_selected1\",\n",
					"    \"date_selected2\", \"date_selected3\", \"rec_num\", \"perm_disqual\",\n",
					"    \"source_id\", \"postcode_start\", \"creation_date\", \"hash_id\"\n",
					"]\n",
					"\n",
					"def careful_clean_text_fields(df):\n",
					"    \"\"\"\n",
					"    Carefully clean text fields without destroying valid data\n",
					"    \"\"\"\n",
					"    logger.info(\"Starting careful text field cleaning\")\n",
					"    \n",
					"    # Define text columns to clean\n",
					"    text_columns = [\"part_no\", \"register_lett\", \"new_marker\", \"title\", \"lname\", \"fname\", \n",
					"                    \"flags\", \"address\", \"address2\", \"address3\", \"address4\", \"address5\", \n",
					"                    \"address6\", \"zip\", \"perm_disqual\", \"source_id\", \"postcode_start\", \"creation_date\"]\n",
					"    \n",
					"    for col_name in text_columns:\n",
					"        if col_name in df.columns:\n",
					"            df = df.withColumn(col_name,\n",
					"                when(\n",
					"                    (col(col_name).isNull()) |\n",
					"                    (upper(trim(col(col_name))) == \"NAN\") |\n",
					"                    (upper(trim(col(col_name))) == \"NULL\") |\n",
					"                    (upper(trim(col(col_name))) == \"NONE\") |\n",
					"                    (trim(col(col_name)) == \"\"),\n",
					"                    lit(None)\n",
					"                ).otherwise(trim(col(col_name)))\n",
					"            )\n",
					"    \n",
					"    logger.info(\"✅ Completed careful text field cleaning\")\n",
					"    return df\n",
					"\n",
					"def clean_and_convert_dob(df):\n",
					"    \"\"\"\n",
					"    Clean and convert DOB field to proper date format\n",
					"    \"\"\"\n",
					"    logger.info(\"Starting DOB cleaning and conversion\")\n",
					"    \n",
					"    if 'dob' not in df.columns:\n",
					"        logger.warning(\"DOB column not found\")\n",
					"        return df\n",
					"    \n",
					"    # Convert DOB to proper date format, handling various input formats\n",
					"    df = df.withColumn(\"dob\",\n",
					"        when(\n",
					"            (col(\"dob\").isNull()) |\n",
					"            (trim(col(\"dob\")) == \"\") |\n",
					"            (upper(trim(col(\"dob\"))).isin([\"NULL\", \"NAN\", \"NA\", \"NONE\"])),\n",
					"            lit(None)\n",
					"        ).otherwise(\n",
					"            # Try multiple date formats\n",
					"            coalesce(\n",
					"                to_date(col(\"dob\"), \"dd/MM/yyyy\"),    # DD/MM/YYYY\n",
					"                to_date(col(\"dob\"), \"yyyy-MM-dd\"),    # YYYY-MM-DD\n",
					"                to_date(col(\"dob\"), \"MM/dd/yyyy\"),    # MM/DD/YYYY\n",
					"                to_date(col(\"dob\"), \"dd-MM-yyyy\"),    # DD-MM-YYYY\n",
					"                to_date(col(\"dob\"), \"yyyy/MM/dd\")     # YYYY/MM/DD\n",
					"            )\n",
					"        )\n",
					"    )\n",
					"    \n",
					"    # Count valid dates\n",
					"    valid_dates = df.filter(col(\"dob\").isNotNull()).count()\n",
					"    logger.info(f\"Valid DOB dates after conversion: {valid_dates:,}\")\n",
					"    \n",
					"    return df\n",
					"\n",
					"def clean_register_lett_and_poll_number(df):\n",
					"    \"\"\"\n",
					"    Clean register_lett and poll_number fields with proper truncation\n",
					"    \"\"\"\n",
					"    logger.info(\"Cleaning register_lett and poll_number fields\")\n",
					"    \n",
					"    # Clean register_lett: strip whitespace and truncate to 5 chars\n",
					"    if \"register_lett\" in df.columns:\n",
					"        df = df.withColumn(\"register_lett\",\n",
					"            when(col(\"register_lett\").isNull(), lit(None))\n",
					"            .otherwise(substring(trim(col(\"register_lett\").cast(\"string\")), 1, 5))\n",
					"        )\n",
					"        logger.info(\"✅ Cleaned register_lett: stripped and truncated to 5 characters\")\n",
					"    \n",
					"    # Clean poll_number: remove .0 from floats, ensure numeric, truncate to 5 chars\n",
					"    if \"poll_number\" in df.columns:\n",
					"        df = df.withColumn(\"poll_number_temp\",\n",
					"            # First convert to string and remove .0 suffix if present\n",
					"            regexp_replace(col(\"poll_number\").cast(\"string\"), \"\\\\.0$\", \"\")\n",
					"        ).withColumn(\"poll_number_numeric\",\n",
					"            # Convert to numeric, coercing errors to null\n",
					"            col(\"poll_number_temp\").cast(\"double\")\n",
					"        ).withColumn(\"poll_number\",\n",
					"            # Convert back to string and truncate to 5 chars\n",
					"            when(col(\"poll_number_numeric\").isNull(), lit(None))\n",
					"            .otherwise(substring(col(\"poll_number_numeric\").cast(\"int\").cast(\"string\"), 1, 5))\n",
					"        ).drop(\"poll_number_temp\", \"poll_number_numeric\")\n",
					"        \n",
					"        logger.info(\"✅ Cleaned poll_number: removed .0 suffix, ensured numeric, truncated to 5 characters\")\n",
					"    \n",
					"    return df\n",
					"\n",
					"def truncate_name_fields(df):\n",
					"    \"\"\"\n",
					"    Truncate name fields to 20 characters and validate\n",
					"    \"\"\"\n",
					"    logger.info(\"Truncating and validating name fields\")\n",
					"    \n",
					"    for col_name in [\"lname\", \"fname\"]:\n",
					"        if col_name in df.columns:\n",
					"            initial_count = df.count()\n",
					"            \n",
					"            # Remove rows where names are null or invalid\n",
					"            df = df.filter(\n",
					"                (col(col_name).isNotNull()) &\n",
					"                (trim(col(col_name)) != \"\") &\n",
					"                (~upper(trim(col(col_name))).isin([\"NULL\", \"NA\", \"NAN\", \"NONE\"]))\n",
					"            )\n",
					"            \n",
					"            removed = initial_count - df.count()\n",
					"            if removed > 0:\n",
					"                logger.info(f\"Removed {removed:,} rows with invalid {col_name}\")\n",
					"            \n",
					"            # Check for names longer than 20 characters and truncate\n",
					"            long_names_count = df.filter(length(col(col_name)) > 20).count()\n",
					"            if long_names_count > 0:\n",
					"                logger.info(f\"Truncating {long_names_count:,} {col_name} values > 20 chars\")\n",
					"                df = df.withColumn(col_name, \n",
					"                    when(length(col(col_name)) > 20, \n",
					"                         substring(col(col_name), 1, 20)\n",
					"                    ).otherwise(col(col_name))\n",
					"                )\n",
					"            \n",
					"            logger.info(f\"✅ {col_name} cleaning complete\")\n",
					"    \n",
					"    return df\n",
					"\n",
					"def truncate_address_fields(df):\n",
					"    \"\"\"\n",
					"    Truncate address fields to 35 characters\n",
					"    \"\"\"\n",
					"    logger.info(\"Truncating address fields to 35 characters\")\n",
					"    \n",
					"    address_fields = [\"address\", \"address2\", \"address3\", \"address4\", \"address5\", \"address6\"]\n",
					"    \n",
					"    for col_name in address_fields:\n",
					"        if col_name in df.columns:\n",
					"            # Truncate to 35 chars and clean empty strings\n",
					"            df = df.withColumn(col_name,\n",
					"                when(\n",
					"                    (col(col_name).isNull()) |\n",
					"                    (trim(col(col_name)) == \"\") |\n",
					"                    (upper(trim(col(col_name))).isin([\"NULL\", \"NA\", \"NAN\", \"NONE\"])),\n",
					"                    lit(None)\n",
					"                ).otherwise(\n",
					"                    substring(trim(col(col_name)), 1, 35)\n",
					"                )\n",
					"            )\n",
					"    \n",
					"    logger.info(\"✅ All address fields truncated to 35 chars\")\n",
					"    return df\n",
					"\n",
					"def clean_address4_special_cases(df):\n",
					"    \"\"\"\n",
					"    Handle special address4 cleaning cases\n",
					"    \"\"\"\n",
					"    logger.info(\"Handling address4 special cleaning cases\")\n",
					"    \n",
					"    if \"address4\" in df.columns and \"zip\" in df.columns:\n",
					"        # Remove address4 values that match zip without spaces\n",
					"        df = df.withColumn(\"address4\",\n",
					"            when(\n",
					"                col(\"address4\") == regexp_replace(col(\"zip\"), \" \", \"\"),\n",
					"                lit(None)\n",
					"            ).otherwise(col(\"address4\"))\n",
					"        )\n",
					"        \n",
					"        # Remove explicit \"NAN\" values\n",
					"        df = df.withColumn(\"address4\",\n",
					"            when(upper(trim(col(\"address4\"))) == \"NAN\", lit(None))\n",
					"            .otherwise(col(\"address4\"))\n",
					"        )\n",
					"    \n",
					"    return df\n",
					"\n",
					"def fix_address4_null_values(df):\n",
					"    \"\"\"\n",
					"    Fix null address4 values by shifting from other address fields or setting to UNKNOWN\n",
					"    \"\"\"\n",
					"    logger.info(\"Fixing null address4 values\")\n",
					"    \n",
					"    if \"address4\" not in df.columns:\n",
					"        df = df.withColumn(\"address4\", lit(None).cast(\"string\"))\n",
					"    \n",
					"    # Shift address3 to address4 if address4 is null\n",
					"    if \"address3\" in df.columns:\n",
					"        df = df.withColumn(\"address4\",\n",
					"            when(\n",
					"                (col(\"address4\").isNull()) & \n",
					"                (col(\"address3\").isNotNull()) &\n",
					"                (trim(col(\"address3\")) != \"\"),\n",
					"                col(\"address3\")\n",
					"            ).otherwise(col(\"address4\"))\n",
					"        ).withColumn(\"address3\",\n",
					"            when(\n",
					"                (col(\"address4\") == col(\"address3\")) & \n",
					"                (col(\"address4\").isNotNull()),\n",
					"                lit(None)\n",
					"            ).otherwise(col(\"address3\"))\n",
					"        )\n",
					"    \n",
					"    # Shift address2 to address4 if still null and address2 doesn't start with number\n",
					"    if \"address2\" in df.columns:\n",
					"        df = df.withColumn(\"address4\",\n",
					"            when(\n",
					"                (col(\"address4\").isNull()) & \n",
					"                (col(\"address2\").isNotNull()) &\n",
					"                (trim(col(\"address2\")) != \"\") &\n",
					"                (~col(\"address2\").rlike(\"^[0-9].*\")),  # doesn't start with number\n",
					"                col(\"address2\")\n",
					"            ).otherwise(col(\"address4\"))\n",
					"        ).withColumn(\"address2\",\n",
					"            when(\n",
					"                (col(\"address4\") == col(\"address2\")) & \n",
					"                (col(\"address4\").isNotNull()),\n",
					"                lit(None)\n",
					"            ).otherwise(col(\"address2\"))\n",
					"        )\n",
					"    \n",
					"    # If address4 is still null, try address5 or address6\n",
					"    for addr_col in [\"address5\", \"address6\"]:\n",
					"        if addr_col in df.columns:\n",
					"            df = df.withColumn(\"address4\",\n",
					"                when(\n",
					"                    (col(\"address4\").isNull()) & \n",
					"                    (col(addr_col).isNotNull()) &\n",
					"                    (trim(col(addr_col)) != \"\"),\n",
					"                    col(addr_col)\n",
					"                ).otherwise(col(\"address4\"))\n",
					"            )\n",
					"    \n",
					"    # Set any remaining nulls to \"UNKNOWN\"\n",
					"    remaining_nulls = df.filter(col(\"address4\").isNull()).count()\n",
					"    if remaining_nulls > 0:\n",
					"        logger.info(f\"Setting {remaining_nulls:,} remaining NULL address4 values to 'UNKNOWN'\")\n",
					"        df = df.withColumn(\"address4\",\n",
					"            when(col(\"address4\").isNull(), lit(\"UNKNOWN\"))\n",
					"            .otherwise(col(\"address4\"))\n",
					"        )\n",
					"    \n",
					"    return df\n",
					"\n",
					"def clean_hash_id_and_deduplicate(df):\n",
					"    \"\"\"\n",
					"    Clean hash_id column and handle deduplication\n",
					"    \"\"\"\n",
					"    logger.info(\"Cleaning hash_id and handling deduplication\")\n",
					"    \n",
					"    if \"hash_id\" not in df.columns:\n",
					"        logger.error(\"hash_id column not found in DataFrame\")\n",
					"        return df\n",
					"    \n",
					"    initial_count = df.count()\n",
					"    logger.info(f\"Initial row count: {initial_count:,}\")\n",
					"    \n",
					"    # Clean hash_id values - handle scientific notation and invalid values\n",
					"    df = df.withColumn(\"hash_id_clean\",\n",
					"        when(\n",
					"            (col(\"hash_id\").isNull()) |\n",
					"            (upper(trim(col(\"hash_id\").cast(\"string\"))).isin([\"\", \"NAN\", \"NULL\", \"NONE\"])),\n",
					"            lit(None)\n",
					"        ).otherwise(\n",
					"            # Try to convert scientific notation to bigint\n",
					"            when(\n",
					"                col(\"hash_id\").cast(\"string\").rlike(\".*[eE].*\"),\n",
					"                col(\"hash_id\").cast(\"double\").cast(\"bigint\")\n",
					"            ).otherwise(\n",
					"                col(\"hash_id\").cast(\"bigint\")\n",
					"            )\n",
					"        )\n",
					"    ).drop(\"hash_id\").withColumnRenamed(\"hash_id_clean\", \"hash_id\")\n",
					"    \n",
					"    # Remove rows with null hash_id\n",
					"    df = df.filter(col(\"hash_id\").isNotNull())\n",
					"    \n",
					"    after_null_removal = df.count()\n",
					"    logger.info(f\"Records after null hash_id removal: {after_null_removal:,}\")\n",
					"    \n",
					"    # Check for and handle duplicates\n",
					"    duplicate_count = df.count() - df.select(\"hash_id\").distinct().count()\n",
					"    \n",
					"    if duplicate_count > 0:\n",
					"        logger.info(f\"Found {duplicate_count:,} duplicate hash_id values - removing duplicates\")\n",
					"        \n",
					"        # Keep only the first occurrence of each hash_id\n",
					"        window_spec = Window.partitionBy(\"hash_id\").orderBy(col(\"hash_id\"))\n",
					"        df = df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"               .filter(col(\"row_num\") == 1) \\\n",
					"               .drop(\"row_num\")\n",
					"        \n",
					"        final_count = df.count()\n",
					"        logger.info(f\"Records after deduplication: {final_count:,}\")\n",
					"    \n",
					"    # Verify uniqueness\n",
					"    unique_hash_ids = df.select(\"hash_id\").distinct().count()\n",
					"    total_records = df.count()\n",
					"    \n",
					"    if unique_hash_ids == total_records:\n",
					"        logger.info(\"✅ Hash_id uniqueness verified\")\n",
					"    else:\n",
					"        logger.warning(f\"⚠️ Hash_id uniqueness issue: {total_records} total vs {unique_hash_ids} unique\")\n",
					"    \n",
					"    return df\n",
					"\n",
					"def add_missing_columns_and_reorder(df):\n",
					"    \"\"\"\n",
					"    Add missing columns and reorder according to final schema\n",
					"    \"\"\"\n",
					"    logger.info(\"Adding missing columns and reordering\")\n",
					"    \n",
					"    # Add missing columns\n",
					"    for col_name in FINAL_COLUMNS:\n",
					"        if col_name not in df.columns:\n",
					"            if col_name == 'postcode_start':\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"string\"))  # Explicitly set postcode_start to NULL\n",
					"            elif col_name == 'part_no':\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"string\"))  # part_no will be generated by PostgreSQL\n",
					"            else:\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"string\"))\n",
					"            logger.info(f\"Added missing column: {col_name}\")\n",
					"    \n",
					"    # Reorder columns\n",
					"    df = df.select(*FINAL_COLUMNS)\n",
					"    \n",
					"    return df\n",
					"\n",
					"def final_data_validation(df):\n",
					"    \"\"\"\n",
					"    Perform final data validation before export\n",
					"    \"\"\"\n",
					"    logger.info(\"Performing final data validation\")\n",
					"    \n",
					"    # Check critical fields\n",
					"    critical_fields = ['lname', 'fname', 'address', 'hash_id']\n",
					"    critical_issues = []\n",
					"    \n",
					"    for field in critical_fields:\n",
					"        if field in df.columns:\n",
					"            null_count = df.filter(col(field).isNull()).count()\n",
					"            if null_count > 0:\n",
					"                critical_issues.append(f\"{field}: {null_count:,} NULLs\")\n",
					"    \n",
					"    if critical_issues:\n",
					"        logger.warning(f\"⚠️ CRITICAL FIELD ISSUES:\")\n",
					"        for issue in critical_issues:\n",
					"            logger.warning(f\"   - {issue}\")\n",
					"        \n",
					"        # Remove records with null critical fields\n",
					"        df = df.filter(\n",
					"            col(\"lname\").isNotNull() &\n",
					"            col(\"fname\").isNotNull() &\n",
					"            col(\"address\").isNotNull() &\n",
					"            col(\"hash_id\").isNotNull()\n",
					"        )\n",
					"        \n",
					"        final_count = df.count()\n",
					"        logger.info(f\"Records after removing critical nulls: {final_count:,}\")\n",
					"    else:\n",
					"        logger.info(\"✅ All critical fields validated successfully\")\n",
					"    \n",
					"    # Log final statistics\n",
					"    logger.info(f\"Final DataFrame shape: ({df.count()}, {len(df.columns)})\")\n",
					"    \n",
					"    # Check address4 - should have no nulls\n",
					"    address4_nulls = df.filter(col(\"address4\").isNull()).count()\n",
					"    if address4_nulls > 0:\n",
					"        logger.warning(f\"⚠️ {address4_nulls} records still have null address4\")\n",
					"    else:\n",
					"        logger.info(\"✅ All records have valid address4\")\n",
					"    \n",
					"    return df\n",
					"\n",
					"def export_to_blob_storage(df, output_path, chunk_size=250000):\n",
					"    \"\"\"\n",
					"    Export DataFrame to blob storage in chunks with proper PostgreSQL formatting\n",
					"    \"\"\"\n",
					"    logger.info(f\"Exporting data to blob storage: {output_path}\")\n",
					"    \n",
					"    # Ensure output directory exists\n",
					"    try:\n",
					"        if not mssparkutils.fs.exists(output_path):\n",
					"            mssparkutils.fs.mkdirs(output_path)\n",
					"            logger.info(f\"Created output directory: {output_path}\")\n",
					"    except Exception as e:\n",
					"        logger.warning(f\"Could not create directory (may already exist): {e}\")\n",
					"    \n",
					"    total_records = df.count()\n",
					"    logger.info(f\"Total records to export: {total_records:,}\")\n",
					"    \n",
					"    if total_records == 0:\n",
					"        logger.error(\"No records to export\")\n",
					"        return None\n",
					"    \n",
					"    # Format date columns for PostgreSQL (YYYY-MM-DD)\n",
					"    datetime_columns = ['dob', 'date_selected1', 'date_selected2', 'date_selected3']\n",
					"    \n",
					"    for col_name in datetime_columns:\n",
					"        if col_name in df.columns:\n",
					"            df = df.withColumn(col_name,\n",
					"                when(col(col_name).isNull(), lit(None))\n",
					"                .otherwise(date_format(col(col_name), \"yyyy-MM-dd\"))\n",
					"            )\n",
					"    \n",
					"    # Calculate number of chunks\n",
					"    num_chunks = (total_records + chunk_size - 1) // chunk_size\n",
					"    logger.info(f\"Will create {num_chunks} chunks of approximately {chunk_size:,} records each\")\n",
					"    \n",
					"    # Export in chunks\n",
					"    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
					"    \n",
					"    for i in range(num_chunks):\n",
					"        start_idx = i * chunk_size\n",
					"        end_idx = min((i + 1) * chunk_size, total_records)\n",
					"        \n",
					"        # Create chunk using row_number window function\n",
					"        window_spec = Window.orderBy(col(\"hash_id\"))\n",
					"        df_with_row_num = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
					"        \n",
					"        chunk_df = df_with_row_num.filter(\n",
					"            (col(\"row_num\") >= start_idx + 1) & \n",
					"            (col(\"row_num\") <= end_idx)\n",
					"        ).drop(\"row_num\")\n",
					"        \n",
					"        chunk_records = chunk_df.count()\n",
					"        chunk_output_path = f\"{output_path}/sanitized_chunk_{i+1:03d}_{timestamp}\"\n",
					"        \n",
					"        logger.info(f\"Exporting chunk {i+1}/{num_chunks}: {chunk_records:,} records\")\n",
					"        \n",
					"        try:\n",
					"            # Write chunk as CSV with PostgreSQL-compatible options\n",
					"            chunk_df.coalesce(1).write \\\n",
					"                .option(\"header\", \"true\") \\\n",
					"                .option(\"nullValue\", \"\") \\\n",
					"                .option(\"emptyValue\", \"\") \\\n",
					"                .option(\"quote\", '\"') \\\n",
					"                .option(\"escape\", '\"') \\\n",
					"                .option(\"quoteAll\", \"false\") \\\n",
					"                .mode(\"overwrite\") \\\n",
					"                .csv(chunk_output_path)\n",
					"            \n",
					"            logger.info(f\"✅ Chunk {i+1} exported successfully\")\n",
					"            \n",
					"        except Exception as e:\n",
					"            logger.error(f\"❌ Failed to export chunk {i+1}: {str(e)}\")\n",
					"            continue\n",
					"    \n",
					"    logger.info(f\"✅ Export complete! {num_chunks} chunks saved to: {output_path}\")\n",
					"    return output_path\n",
					"\n",
					"def main_sanitization_pipeline():\n",
					"    \"\"\"\n",
					"    Main sanitization pipeline function\n",
					"    \"\"\"\n",
					"    logger.info(\"🚀 Starting Synapse PySpark Data Sanitization Pipeline\")\n",
					"    \n",
					"    try:\n",
					"        # Step 1: Read from Delta Lake\n",
					"        logger.info(f\"Step 1: Reading from Delta Lake: {INPUT_DELTA_PATH}\")\n",
					"        \n",
					"        if not mssparkutils.fs.exists(INPUT_DELTA_PATH):\n",
					"            logger.error(f\"❌ Input Delta table not found: {INPUT_DELTA_PATH}\")\n",
					"            return None\n",
					"        \n",
					"        df = spark.read.format(\"delta\").load(INPUT_DELTA_PATH)\n",
					"        initial_count = df.count()\n",
					"        logger.info(f\"✅ Loaded {initial_count:,} records from Delta table\")\n",
					"        \n",
					"        if initial_count == 0:\n",
					"            logger.error(\"❌ No data found in Delta table\")\n",
					"            return None\n",
					"        \n",
					"        # Step 2: Apply data cleaning and sanitization\n",
					"        logger.info(\"Step 2: Applying data cleaning and sanitization\")\n",
					"        \n",
					"        # Clean text fields\n",
					"        df = careful_clean_text_fields(df)\n",
					"        \n",
					"        # Clean and convert DOB\n",
					"        df = clean_and_convert_dob(df)\n",
					"        \n",
					"        # Clean register_lett and poll_number\n",
					"        df = clean_register_lett_and_poll_number(df)\n",
					"        \n",
					"        # Truncate name fields\n",
					"        df = truncate_name_fields(df)\n",
					"        \n",
					"        # Truncate address fields\n",
					"        df = truncate_address_fields(df)\n",
					"        \n",
					"        # Handle address4 special cases\n",
					"        df = clean_address4_special_cases(df)\n",
					"        \n",
					"        # Fix null address4 values\n",
					"        df = fix_address4_null_values(df)\n",
					"        \n",
					"        # Clean hash_id and deduplicate\n",
					"        df = clean_hash_id_and_deduplicate(df)\n",
					"        \n",
					"        # Add missing columns and reorder\n",
					"        df = add_missing_columns_and_reorder(df)\n",
					"        \n",
					"        # Final validation\n",
					"        df = final_data_validation(df)\n",
					"        \n",
					"        final_count = df.count()\n",
					"        logger.info(f\"✅ Data sanitization complete: {final_count:,} records\")\n",
					"        logger.info(f\"Records removed during sanitization: {initial_count - final_count:,}\")\n",
					"        \n",
					"        # Step 3: Export to blob storage\n",
					"        logger.info(\"Step 3: Exporting to blob storage\")\n",
					"        \n",
					"        export_path = export_to_blob_storage(df, OUTPUT_BLOB_PATH)\n",
					"        \n",
					"        if export_path:\n",
					"            logger.info(\"🎉 Sanitization pipeline completed successfully!\")\n",
					"            logger.info(f\"📁 Data exported to: {export_path}\")\n",
					"            \n",
					"            # Show sample of final data\n",
					"            logger.info(\"📋 Sample of sanitized data:\")\n",
					"            sample_data = df.limit(3).select(\"lname\", \"fname\", \"address\", \"address4\", \"dob\", \"hash_id\").collect()\n",
					"            for i, row in enumerate(sample_data):\n",
					"                logger.info(f\"   Row {i+1}: {row.lname}, {row.fname}, {row.address4}, {row.dob}, {row.hash_id}\")\n",
					"            \n",
					"            return df\n",
					"        else:\n",
					"            logger.error(\"❌ Export failed\")\n",
					"            return None\n",
					"            \n",
					"    except Exception as e:\n",
					"        logger.error(f\"❌ Pipeline failed: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(\"Full traceback:\")\n",
					"        logger.error(traceback.format_exc())\n",
					"        return None\n",
					"\n",
					"def check_input_data():\n",
					"    \"\"\"\n",
					"    Quick function to check input data availability\n",
					"    \"\"\"\n",
					"    try:\n",
					"        logger.info(f\"Checking input data availability: {INPUT_DELTA_PATH}\")\n",
					"        \n",
					"        if not mssparkutils.fs.exists(INPUT_DELTA_PATH):\n",
					"            logger.error(f\"❌ Input path does not exist: {INPUT_DELTA_PATH}\")\n",
					"            return False\n",
					"        \n",
					"        df = spark.read.format(\"delta\").load(INPUT_DELTA_PATH)\n",
					"        total_count = df.count()\n",
					"        logger.info(f\"✅ Total records in Delta table: {total_count:,}\")\n",
					"        \n",
					"        if total_count == 0:\n",
					"            logger.warning(\"⚠️ Delta table exists but contains no records\")\n",
					"            return False\n",
					"        \n",
					"        # Check sample data structure\n",
					"        logger.info(\"📋 Sample data columns:\")\n",
					"        for col_name in df.columns[:10]:\n",
					"            logger.info(f\"   - {col_name}\")\n",
					"        \n",
					"        if len(df.columns) > 10:\n",
					"            logger.info(f\"   ... and {len(df.columns) - 10} more columns\")\n",
					"        \n",
					"        # Check for critical columns\n",
					"        critical_cols = ['hash_id', 'lname', 'fname', 'address']\n",
					"        missing_cols = [col for col in critical_cols if col not in df.columns]\n",
					"        if missing_cols:\n",
					"            logger.error(f\"❌ Missing critical columns: {missing_cols}\")\n",
					"            return False\n",
					"        \n",
					"        return True\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"❌ Error checking input data: {str(e)}\")\n",
					"        return False\n",
					"\n",
					"# Execute the pipeline\n",
					"if __name__ == \"__main__\":\n",
					"    logger.info(\"🚀 Starting Synapse PySpark Sanitization Pipeline\")\n",
					"    \n",
					"    # Check input data first\n",
					"    logger.info(\"Step 1: Checking input data availability...\")\n",
					"    if check_input_data():\n",
					"        logger.info(\"✅ Input data check passed\")\n",
					"        \n",
					"        # Run the sanitization pipeline\n",
					"        logger.info(\"Step 2: Running sanitization pipeline...\")\n",
					"        result_df = main_sanitization_pipeline()\n",
					"        \n",
					"        if result_df is not None:\n",
					"            logger.info(\"🎉 Sanitization pipeline completed successfully\")\n",
					"        else:\n",
					"            logger.error(\"❌ Sanitization pipeline failed\")\n",
					"    else:\n",
					"        logger.error(\"❌ Input data check failed - cannot proceed\")"
				],
				"execution_count": null
			}
		]
	}
}