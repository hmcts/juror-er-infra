{
	"name": "JurorToPostgresql",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkprod",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "fa03e9ac-c205-4739-99d2-5c51631073bb"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5ca62022-6aa2-4cee-aaa7-e7536c8d566c/resourceGroups/baubais-data-factory-rg-prod/providers/Microsoft.Synapse/workspaces/baubais-synapse-prod/bigDataPools/sparkprod",
				"name": "sparkprod",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkprod",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"\"\"\"Synapse PySpark Data Sanitization Pipeline with Sensitive Data Hashing/Masking\n",
					"Reads from deduplicated delta lake table, hashes/masks sensitive data, and saves to blob storage\n",
					"Uses production data but protects sensitive columns for test environment\n",
					"\"\"\"\n",
					"\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql.window import Window\n",
					"from notebookutils import mssparkutils\n",
					"import logging\n",
					"from datetime import datetime\n",
					"import time\n",
					"\n",
					"# Configure logging\n",
					"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"logger = logging.getLogger()\n",
					"\n",
					"# Initialize Spark session with Delta Lake support\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"Voters Data Pipeline\") \\\n",
					"    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
					"    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
					"    .config(\"spark.executor.memory\", \"4g\") \\\n",
					"    .config(\"spark.driver.memory\", \"4g\") \\\n",
					"    .getOrCreate()"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"# ============================================================================\n",
					"# INTEGRATION GUIDE: Adding Delta Table Export to Your Existing Script\n",
					"# ============================================================================\n",
					"\n",
					"# STEP 1: Add these imports at the top of your existing script\n",
					"# (Add to your existing imports section)\n",
					"\n",
					"from delta.tables import DeltaTable\n",
					"from datetime import datetime\n",
					"import logging\n",
					"\n",
					"# ============================================================================\n",
					"# STEP 2: Add Delta Lake support to your Spark session\n",
					"# ============================================================================\n",
					"\n",
					"# REPLACE your existing Spark session creation with this:\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"Voters Data Pipeline\") \\\n",
					"    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
					"    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
					"    .config(\"spark.executor.memory\", \"4g\") \\\n",
					"    .config(\"spark.driver.memory\", \"4g\") \\\n",
					"    .getOrCreate()\n",
					"\n",
					"# ============================================================================\n",
					"# STEP 3: Add these functions to your script\n",
					"# ============================================================================\n",
					"\n",
					"def save_as_postgresql_ready_delta_table(df, base_path):\n",
					"    \"\"\"\n",
					"    Save processed DataFrame as PostgreSQL-ready Delta table\n",
					"    \"\"\"\n",
					"    try:\n",
					"        logger = logging.getLogger(__name__)\n",
					"        logger.info(\"üîÑ Saving as PostgreSQL-ready Delta table...\")\n",
					"        \n",
					"        # Apply PostgreSQL schema constraints\n",
					"        postgresql_df = prepare_postgresql_schema(df)\n",
					"        \n",
					"        # Create Delta table path\n",
					"        table_name = \"voters_postgresql_ready\"\n",
					"        delta_table_path = f\"{base_path}/{table_name}\"\n",
					"        \n",
					"        # Get record count\n",
					"        record_count = postgresql_df.count()\n",
					"        logger.info(f\"   Records to save: {record_count:,}\")\n",
					"        \n",
					"        # Save as Delta table with partitioning\n",
					"        postgresql_df.write \\\n",
					"            .format(\"delta\") \\\n",
					"            .mode(\"overwrite\") \\\n",
					"            .option(\"overwriteSchema\", \"true\") \\\n",
					"            .option(\"mergeSchema\", \"true\") \\\n",
					"            .partitionBy(\"creation_date\") \\\n",
					"            .save(delta_table_path)\n",
					"        \n",
					"        logger.info(f\"‚úÖ Delta table saved: {delta_table_path}\")\n",
					"        \n",
					"        # Verify the save\n",
					"        verification_df = spark.read.format(\"delta\").load(delta_table_path)\n",
					"        saved_count = verification_df.count()\n",
					"        \n",
					"        if saved_count == record_count:\n",
					"            logger.info(f\"‚úÖ Verification passed: {saved_count:,} records\")\n",
					"            \n",
					"            # Optimize the table\n",
					"            try:\n",
					"                delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
					"                delta_table.optimize().executeCompaction()\n",
					"                delta_table.optimize().executeZOrderBy(\"hash_id\")\n",
					"                logger.info(\"‚úÖ Table optimized\")\n",
					"            except Exception as opt_error:\n",
					"                logger.warning(f\"‚ö†Ô∏è Optimization failed: {opt_error}\")\n",
					"            \n",
					"            return {\n",
					"                \"success\": True,\n",
					"                \"delta_path\": delta_table_path,\n",
					"                \"record_count\": saved_count,\n",
					"                \"table_name\": table_name\n",
					"            }\n",
					"        else:\n",
					"            return {\n",
					"                \"success\": False,\n",
					"                \"error\": f\"Record count mismatch: expected {record_count}, got {saved_count}\"\n",
					"            }\n",
					"            \n",
					"    except Exception as e:\n",
					"        logger.error(f\"‚ùå Error saving Delta table: {str(e)}\")\n",
					"        return {\n",
					"            \"success\": False,\n",
					"            \"error\": str(e)\n",
					"        }\n",
					"\n",
					"def prepare_postgresql_schema(df):\n",
					"    \"\"\"\n",
					"    Ensure DataFrame schema matches PostgreSQL requirements\n",
					"    \"\"\"\n",
					"    # Schema constraints for PostgreSQL compatibility\n",
					"    schema_map = {\n",
					"        \"part_no\": (\"string\", 10),\n",
					"        \"register_lett\": (\"string\", 5),\n",
					"        \"poll_number\": (\"string\", 5),\n",
					"        \"new_marker\": (\"string\", 1),\n",
					"        \"title\": (\"string\", 10),\n",
					"        \"lname\": (\"string\", 20),\n",
					"        \"fname\": (\"string\", 20),\n",
					"        \"dob\": (\"date\", None),\n",
					"        \"flags\": (\"string\", 2),\n",
					"        \"address\": (\"string\", 35),\n",
					"        \"address2\": (\"string\", 35),\n",
					"        \"address3\": (\"string\", 35),\n",
					"        \"address4\": (\"string\", 35),\n",
					"        \"address5\": (\"string\", 35),\n",
					"        \"address6\": (\"string\", 35),\n",
					"        \"zip\": (\"string\", 10),\n",
					"        \"date_selected1\": (\"date\", None),\n",
					"        \"date_selected2\": (\"date\", None),\n",
					"        \"date_selected3\": (\"date\", None),\n",
					"        \"rec_num\": (\"integer\", None),\n",
					"        \"perm_disqual\": (\"string\", 1),\n",
					"        \"source_id\": (\"string\", 1),\n",
					"        \"postcode_start\": (\"string\", 10),\n",
					"        \"creation_date\": (\"string\", 10),\n",
					"        \"hash_id\": (\"bigint\", None)\n",
					"    }\n",
					"    \n",
					"    # Apply schema constraints\n",
					"    for col_name, (data_type, max_length) in schema_map.items():\n",
					"        if col_name in df.columns:\n",
					"            # Cast to correct type\n",
					"            if data_type == \"string\":\n",
					"                df = df.withColumn(col_name, col(col_name).cast(\"string\"))\n",
					"                if max_length:\n",
					"                    df = df.withColumn(col_name, \n",
					"                                     when(col(col_name).isNotNull(),\n",
					"                                          substring(col(col_name), 1, max_length))\n",
					"                                     .otherwise(col(col_name)))\n",
					"            elif data_type == \"date\":\n",
					"                df = df.withColumn(col_name, col(col_name).cast(\"date\"))\n",
					"            elif data_type == \"integer\":\n",
					"                df = df.withColumn(col_name, col(col_name).cast(\"integer\"))\n",
					"            elif data_type == \"bigint\":\n",
					"                df = df.withColumn(col_name, col(col_name).cast(\"bigint\"))\n",
					"        else:\n",
					"            # Add missing columns\n",
					"            if data_type == \"string\":\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"string\"))\n",
					"            elif data_type == \"date\":\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"date\"))\n",
					"            elif data_type == \"integer\":\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"integer\"))\n",
					"            elif data_type == \"bigint\":\n",
					"                df = df.withColumn(col_name, lit(None).cast(\"bigint\"))\n",
					"    \n",
					"    # Ensure proper column order\n",
					"    ordered_columns = list(schema_map.keys())\n",
					"    existing_columns = [col for col in ordered_columns if col in df.columns]\n",
					"    df = df.select(*existing_columns)\n",
					"    \n",
					"    return df\n",
					"\n",
					"# ============================================================================\n",
					"# STEP 4: REPLACE your existing main function ending\n",
					"# ============================================================================\n",
					"\n",
					"def main_sanitization_pipeline():\n",
					"    \"\"\"\n",
					"    Your existing main function - MODIFY THE ENDING\n",
					"    \"\"\"\n",
					"    \n",
					"    # ... YOUR EXISTING CODE ...\n",
					"    # All your current data processing logic stays the same\n",
					"    \n",
					"    # ========================================================================\n",
					"    # OLD CODE (REMOVE THIS):\n",
					"    # ========================================================================\n",
					"    # try:\n",
					"    #     export_path = export_to_blob_storage(df, OUTPUT_BLOB_PATH)  # ‚ùå REMOVE\n",
					"    #     if export_path:\n",
					"    #         print(\"‚úÖ Export successful\")\n",
					"    #     else:\n",
					"    #         print(\"‚ùå Export failed\")\n",
					"    # except Exception as e:\n",
					"    #     print(f\"‚ùå Export error: {e}\")\n",
					"    \n",
					"    # ========================================================================\n",
					"    # NEW CODE (ADD THIS AT THE END):\n",
					"    # ========================================================================\n",
					"    \n",
					"    # Configuration\n",
					"    DELTA_BASE_PATH = \"abfss://juror-etl@yourstorageaccount.dfs.core.windows.net/delta_tables\"\n",
					"    \n",
					"    try:\n",
					"        print(\"üîÑ Saving processed data as Delta table...\")\n",
					"        \n",
					"        # Save as PostgreSQL-ready Delta table\n",
					"        result = save_as_postgresql_ready_delta_table(df, DELTA_BASE_PATH)\n",
					"        \n",
					"        if result[\"success\"]:\n",
					"            print(f\"üéâ SUCCESS! Delta table created:\")\n",
					"            print(f\"   üìç Path: {result['delta_path']}\")\n",
					"            print(f\"   üìä Records: {result['record_count']:,}\")\n",
					"            print(f\"   üìã Table: {result['table_name']}\")\n",
					"            print(f\"\\nüöÄ Ready for ADF Copy Activity to PostgreSQL!\")\n",
					"            \n",
					"            # Return the path for any downstream processing\n",
					"            return result\n",
					"            \n",
					"        else:\n",
					"            print(f\"‚ùå FAILED to create Delta table:\")\n",
					"            print(f\"   Error: {result['error']}\")\n",
					"            return None\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"‚ùå Sanitization pipeline failed: {str(e)}\")\n",
					"        import traceback\n",
					"        print(\"Full traceback:\")\n",
					"        print(traceback.format_exc())\n",
					"        return None\n",
					"\n",
					"# ============================================================================\n",
					"# STEP 5: Update your script execution\n",
					"# ============================================================================\n",
					"\n",
					"if __name__ == \"__main__\":\n",
					"    # Configure logging\n",
					"    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"    \n",
					"    print(\"üöÄ Starting Enhanced Voter Data Pipeline with Delta Lake\")\n",
					"    \n",
					"    # Run your main pipeline\n",
					"    result = main_sanitization_pipeline()\n",
					"    \n",
					"    if result:\n",
					"        print(\"\\n\" + \"=\"*60)\n",
					"        print(\"üéâ PIPELINE COMPLETED SUCCESSFULLY!\")\n",
					"        print(\"=\"*60)\n",
					"        print(f\"Delta Table Path: {result['delta_path']}\")\n",
					"        print(f\"Records Processed: {result['record_count']:,}\")\n",
					"        print(\"\\nNext Steps:\")\n",
					"        print(\"1. Configure ADF Copy Activity\")\n",
					"        print(\"2. Source: Delta table (Parquet format)\")\n",
					"        print(\"3. Sink: PostgreSQL database\")\n",
					"        print(\"4. Run the ADF pipeline\")\n",
					"        print(\"=\"*60)\n",
					"    else:\n",
					"        print(\"\\n\" + \"=\"*60)\n",
					"        print(\"‚ùå PIPELINE FAILED\")\n",
					"        print(\"=\"*60)\n",
					"        print(\"Check the error messages above for details.\")\n",
					"\n",
					"# ============================================================================\n",
					"# CONFIGURATION NOTES:\n",
					"# ============================================================================\n",
					"\n",
					"\"\"\"\n",
					"IMPORTANT: Update these paths in your script:\n",
					"\n",
					"1. DELTA_BASE_PATH: \n",
					"   Replace \"yourstorageaccount\" with your actual storage account name\n",
					"\n",
					"2. Spark Configuration:\n",
					"   The script adds Delta Lake extensions automatically\n",
					"\n",
					"3. Output Structure:\n",
					"   - Creates: delta_tables/voters_postgresql_ready/\n",
					"   - Partitioned by: creation_date\n",
					"   - Optimized for: PostgreSQL import via ADF\n",
					"\n",
					"4. ADF Configuration (for next step):\n",
					"   Source Type: Parquet\n",
					"   Source Path: delta_tables/voters_postgresql_ready/\n",
					"   Sink Type: PostgreSQL\n",
					"\"\"\"\n",
					"\n",
					"# ============================================================================\n",
					"# MIGRATION CHECKLIST:\n",
					"# ============================================================================\n",
					"\n",
					"\"\"\"\n",
					"‚úÖ Step 1: Add Delta imports\n",
					"‚úÖ Step 2: Update Spark session config  \n",
					"‚úÖ Step 3: Add new functions (save_as_postgresql_ready_delta_table, prepare_postgresql_schema)\n",
					"‚úÖ Step 4: Replace export_to_blob_storage call with save_as_postgresql_ready_delta_table\n",
					"‚úÖ Step 5: Update main function ending\n",
					"‚úÖ Step 6: Update storage account name in DELTA_BASE_PATH\n",
					"‚úÖ Step 7: Test the new pipeline\n",
					"‚úÖ Step 8: Configure ADF Copy Activity to use Delta table as source\n",
					"\"\"\""
				],
				"execution_count": 4
			}
		]
	}
}