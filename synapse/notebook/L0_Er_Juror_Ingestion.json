{
	"name": "L0_Er_Juror_Ingestion",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkstg",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "fd07e341-5ec4-47c4-98da-18aecaea16dd"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/74dacd4f-a248-45bb-a2f0-af700dc4cf68/resourceGroups/baubais-data-factory-rg-stg/providers/Microsoft.Synapse/workspaces/baubais-synapse-stg/bigDataPools/sparkstg",
				"name": "sparkstg",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-stg.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkstg",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# L0 ER Juror Ingestion Pipeline\n",
					"\n",
					"## Overview\n",
					"This script implements an ETL pipeline for processing juror-related files stored in Azure Data Lake Storage (ADLS). It handles various file types (CSV, Excel, ZIP), moves files to appropriate folders (e.g., quarantine, metadata, overseas), and converts files to Parquet format for efficient storage and querying.\n",
					"\n",
					"## Features\n",
					"- **File Type Support**: Processes CSV, Excel, and ZIP files.\n",
					"- **Error Handling**: Gracefully handles errors and moves problematic files to quarantine.\n",
					"- **Distributed Processing**: Uses Apache Spark for scalable file processing.\n",
					"- **Azure Data Lake Integration**: Reads and writes files to ADLS.\n",
					"- **Metadata and Logging**: Adds metadata columns to processed files and maintains detailed logs.\n",
					"\n",
					"## Workflow\n",
					"1. **Initialisation**:\n",
					"   - Configures logging and initializes a Spark session.\n",
					"2. **File Identification**:\n",
					"   - Identifies file types (CSV/Excel) and checks for metadata or empty files.\n",
					"3. **File Movement**:\n",
					"   - Moves files to appropriate folders (e.g., quarantine, metadata, overseas).\n",
					"4. **File Processing**:\n",
					"   - Converts CSV/Excel files to Parquet format and merges files by date and LA folder.\n",
					"5. **Folder Scanning**:\n",
					"   - Recursively scans directories for files to process.\n",
					"6. **Logging**:\n",
					"   - Tracks operations and saves logs to ADLS.\n",
					"\n",
					"## Key Functions\n",
					"`is_metadata_file(file_path)`\n",
					"Checks if a file is a metadata file based on its name.\n",
					"\n",
					"`should_process_file(file_path)`\n",
					"Determines if a file should be processed (CSV or Excel).\n",
					"\n",
					"`is_empty_file(file_path)`\n",
					"Checks if a file is empty or contains only a header.\n",
					"\n",
					"`unzip_file(file_path, extract_to, storage_account)`\n",
					"Extracts a ZIP file and uploads its contents to ADLS.\n",
					"\n",
					"`process_file(source_path, date_folder, la_folder, file_name, storage_account)`\n",
					"Processes a CSV or Excel file and converts it to Parquet format.\n",
					"\n",
					"`move_to_quarantine(file_path, storage_account, moved_files, reason=None)`\n",
					"Moves a file to the quarantine folder for manual inspection.\n",
					"\n",
					"`create_file_index(source_path, storage_account)`\n",
					"Creates a JSON index of all files in the source path.\n",
					"\n",
					"`main()`\n",
					"The main function that orchestrates the ETL workflow.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import logging\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"import time\n",
					"from notebookutils import mssparkutils\n",
					"import os\n",
					"import zipfile\n",
					"import shutil\n",
					"import tempfile\n",
					"import re\n",
					"import json  # Add this line\n",
					"from collections import defaultdict\n",
					"import logging\n",
					"import traceback"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# Configure logging\n",
					"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"\n",
					"# Initialize Spark session with the required packages\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"JurorIngestion\") \\\n",
					"    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.5,org.apache.xmlbeans:xmlbeans:3.1.0,org.apache.poi:poi-ooxml-schemas:4.1.2,org.apache.poi:poi-ooxml:4.1.2,org.apache.poi:poi:4.1.2\") \\\n",
					"    .getOrCreate()\n",
					"\n",
					"# Global counter to track file sequences by folder structure\n",
					"file_counters = defaultdict(int)\n",
					""
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"source": [
					"def is_metadata_file(file_path):\n",
					"    \"\"\"Check if file is metadata related\"\"\"\n",
					"    metadata_indicators = [\n",
					"        'metadata.txt',\n",
					"        'readme.txt'\n",
					"    ]\n",
					"    return any(indicator in file_path.lower() for indicator in metadata_indicators)"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"source": [
					"def should_process_file(file_path):\n",
					"    \"\"\"Check if file should be processed (CSV or Excel) - case insensitive\"\"\"\n",
					"    extensions = ['.csv', '.xlsx', '.xls']\n",
					"    return any(file_path.lower().endswith(ext) for ext in extensions)\n",
					"\n",
					"def get_file_type(file_path):\n",
					"    \"\"\"Determine if file is CSV or Excel\"\"\"\n",
					"    if file_path.lower().endswith(('.xlsx', '.xls')):\n",
					"        return 'excel'\n",
					"    elif file_path.lower().endswith('.csv'):\n",
					"        return 'csv'\n",
					"    return None\n",
					"\n",
					"def is_zip_file(file_path):\n",
					"    \"\"\"Check if file is a zip file\"\"\"\n",
					"    return file_path.lower().endswith('.zip')"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"source": [
					"def is_empty_file(file_path):\n",
					"    \"\"\"Check if a file is empty or very small with improved reliability\"\"\"\n",
					"    try:\n",
					"        # First check if file exists\n",
					"        if not mssparkutils.fs.exists(file_path):\n",
					"            print(f\"File doesn't exist, can't check if empty: {file_path}\")\n",
					"            return False\n",
					"            \n",
					"        # For CSV files, use a more direct approach\n",
					"        if file_path.lower().endswith('.csv'):\n",
					"            # Try reading the first few lines directly\n",
					"            try:\n",
					"                text_df = spark.read.text(file_path).limit(5)\n",
					"                count = text_df.count()\n",
					"                \n",
					"                # If no rows or only header\n",
					"                if count == 0:\n",
					"                    return True\n",
					"                if count == 1:\n",
					"                    # Check if it's just a header line\n",
					"                    first_row = text_df.first()[0]\n",
					"                    if len(first_row.strip()) < 10:\n",
					"                        return True\n",
					"                return False\n",
					"            except:\n",
					"                # Fall back to CSV reader with header option\n",
					"                try:\n",
					"                    csv_df = spark.read.option(\"header\", \"true\").csv(file_path).limit(5)\n",
					"                    return csv_df.count() == 0\n",
					"                except:\n",
					"                    # If both approaches fail, assume it's not an empty file\n",
					"                    return False\n",
					"                    \n",
					"        # For Excel files, check differently\n",
					"        elif file_path.lower().endswith(('.xlsx', '.xls')):\n",
					"            try:\n",
					"                # Just check if we can read any rows\n",
					"                excel_df = spark.read.format(\"com.crealytics.spark.excel\").option(\"header\", \"true\").load(file_path).limit(5)\n",
					"                return excel_df.count() == 0\n",
					"            except:\n",
					"                # If read fails, don't assume it's empty\n",
					"                return False\n",
					"                \n",
					"        # Default approach for other file types\n",
					"        try:\n",
					"            # Use a basic text read as fallback\n",
					"            test_df = spark.read.text(file_path).limit(5)\n",
					"            return test_df.count() == 0\n",
					"        except Exception as e:\n",
					"            print(f\"Error checking if file is empty: {str(e)}\")\n",
					"            return False\n",
					"    except Exception as e:\n",
					"        print(f\"Error in is_empty_file check: {str(e)}\")\n",
					"        # Default to false if we can't determine\n",
					"        return False"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"source": [
					"def unzip_file(file_path, extract_to, storage_account):\n",
					"    \"\"\"Unzip a file to the specified directory with improved error handling\"\"\"\n",
					"    try:\n",
					"        print(f\"Attempting to unzip: {file_path}\")\n",
					"        \n",
					"        # Create a temporary local directory\n",
					"        with tempfile.TemporaryDirectory() as tmpdirname:\n",
					"            local_zip_path = os.path.join(tmpdirname, os.path.basename(file_path))\n",
					"            \n",
					"            # Download the zip file to the local temporary directory\n",
					"            print(f\"Downloading zip file from {file_path} to {local_zip_path}\")\n",
					"            mssparkutils.fs.cp(file_path, f\"file://{local_zip_path}\")\n",
					"            \n",
					"            # Check if zip file is password protected before trying to extract\n",
					"            try:\n",
					"                zip_file = zipfile.ZipFile(local_zip_path)\n",
					"                # Try to read the first file to see if it's password protected\n",
					"                for zip_info in zip_file.infolist():\n",
					"                    if zip_info.flag_bits & 0x1:\n",
					"                        print(f\"ZIP file is encrypted/password protected: {file_path}\")\n",
					"                        return False, \"password_protected\"\n",
					"                    \n",
					"                    # Try to read a little bit of data to check for password\n",
					"                    try:\n",
					"                        zip_file.open(zip_info.filename).read(1)\n",
					"                    except RuntimeError as e:\n",
					"                        if 'password required' in str(e).lower() or 'bad password' in str(e).lower():\n",
					"                            print(f\"Password required for ZIP file: {file_path}\")\n",
					"                            return False, \"password_protected\"\n",
					"                        raise\n",
					"                \n",
					"                # If we got here, try to extract\n",
					"                print(f\"Extracting ZIP file to {extract_to}\")\n",
					"                zip_file.extractall(path=extract_to)\n",
					"                zip_file.close()\n",
					"                \n",
					"            except zipfile.BadZipFile as e:\n",
					"                print(f\"Bad ZIP file: {file_path} - {str(e)}\")\n",
					"                return False, \"bad_zip_file\"\n",
					"            except RuntimeError as e:\n",
					"                error_msg = str(e).lower()\n",
					"                if 'password required' in error_msg or 'encrypted' in error_msg or 'bad password' in error_msg:\n",
					"                    print(f\"Password protected ZIP file detected: {file_path}\")\n",
					"                    return False, \"password_protected\"\n",
					"                raise\n",
					"            \n",
					"        # If extraction succeeded, upload the extracted files back to ADLS\n",
					"        print(f\"Uploading extracted files to Azure Data Lake\")\n",
					"        for root, dirs, files in os.walk(extract_to):\n",
					"            for file in files:\n",
					"                local_file_path = os.path.join(root, file)\n",
					"                relative_path = os.path.relpath(local_file_path, extract_to)\n",
					"                remote_dir = os.path.dirname(file_path)\n",
					"                remote_file_path = f\"{remote_dir}/{relative_path}\"\n",
					"                \n",
					"                print(f\"Uploading: {local_file_path} to {remote_file_path}\")\n",
					"                mssparkutils.fs.cp(f\"file://{local_file_path}\", remote_file_path)\n",
					"        \n",
					"        print(f\"Successfully unzipped file: {file_path}\")\n",
					"        return True, None\n",
					"        \n",
					"    except RuntimeError as e:\n",
					"        error_msg = str(e).lower()\n",
					"        if 'password required' in error_msg or 'encrypted' in error_msg or 'bad password' in error_msg:\n",
					"            print(f\"Password protected ZIP file detected: {file_path}\")\n",
					"            return False, \"password_protected\"\n",
					"        else:\n",
					"            print(f\"Runtime error unzipping file: {str(e)}\")\n",
					"            return False, str(e)\n",
					"    except Exception as e:\n",
					"        print(f\"Error unzipping file: {file_path} - {str(e)}\")\n",
					"        return False, str(e)"
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"source": [
					"def check_for_overseas_columns(file_path, file_type):\n",
					"    \"\"\"Check if file contains overseas address columns and move to overseas container if found\"\"\"\n",
					"    try:\n",
					"        print(f\"  Checking for overseas address columns in: {file_path}\")\n",
					"        \n",
					"        # Read only the header row to check columns\n",
					"        if file_type == 'csv':\n",
					"            # For CSV files\n",
					"            df = spark.read.option(\"header\", \"true\").csv(file_path).limit(0)\n",
					"        elif file_type == 'excel':\n",
					"            # For Excel files\n",
					"            df = spark.read.format(\"com.crealytics.spark.excel\").option(\"header\", \"true\").load(file_path).limit(0)\n",
					"        else:\n",
					"            print(f\"  Unsupported file type for overseas check: {file_type}\")\n",
					"            return False, None\n",
					"            \n",
					"        # Get column names and check if overseas columns exist\n",
					"        columns = df.columns\n",
					"        has_overseas_columns = any(col in columns for col in ['Overseas Address', 'UK Qualifying Address'])\n",
					"        \n",
					"        if has_overseas_columns:\n",
					"            print(f\"  Found overseas address columns in file: {file_path}\")\n",
					"            return True, columns\n",
					"        else:\n",
					"            print(f\"  No overseas address columns found in file: {file_path}\")\n",
					"            return False, None\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"  Error checking for overseas columns: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"  Traceback: {traceback.format_exc()}\")\n",
					"        return False, None\n",
					"\n",
					"def move_to_overseas(file_path, storage_account, moved_files):\n",
					"    \"\"\"Move a file to the overseas folder\"\"\"\n",
					"    try:\n",
					"        # Make sure overseas directory exists\n",
					"        overseas_dir = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/overseas\"\n",
					"        if not mssparkutils.fs.exists(overseas_dir):\n",
					"            print(f\"Creating overseas directory: {overseas_dir}\")\n",
					"            mssparkutils.fs.mkdirs(overseas_dir)\n",
					"        \n",
					"        # Create a unique filename to avoid conflicts\n",
					"        file_name = os.path.basename(file_path)\n",
					"        timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
					"        overseas_path = f\"{overseas_dir}/{timestamp}_{file_name}\"\n",
					"        \n",
					"        print(f\"Moving file to overseas: {file_path} -> {overseas_path}\")\n",
					"        \n",
					"        # Copy file to overseas directory\n",
					"        mssparkutils.fs.cp(file_path, overseas_path, True)\n",
					"        print(f\"Successfully copied file to overseas container\")\n",
					"        \n",
					"        # Update tracking dictionary (add 'overseas' category if it doesn't exist)\n",
					"        if 'overseas' not in moved_files:\n",
					"            moved_files['overseas'] = []\n",
					"        moved_files['overseas'].append(overseas_path)\n",
					"        \n",
					"        return True, overseas_path\n",
					"    except Exception as e:\n",
					"        print(f\"Error moving file to overseas container {file_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"Traceback: {traceback.format_exc()}\")\n",
					"        return False, None"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"source": [
					"def move_to_quarantine(file_path, storage_account, moved_files, reason=None):\n",
					"    \"\"\"Copy a file to the quarantine folder without deleting the original\"\"\"\n",
					"    try:\n",
					"        # Make sure quarantine directory exists\n",
					"        quarantine_dir = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/quarantine\"\n",
					"        if not mssparkutils.fs.exists(quarantine_dir):\n",
					"            print(f\"Creating quarantine directory: {quarantine_dir}\")\n",
					"            mssparkutils.fs.mkdirs(quarantine_dir)\n",
					"        \n",
					"        # Create a unique filename to avoid conflicts\n",
					"        file_name = os.path.basename(file_path)\n",
					"        timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
					"        \n",
					"        # Add reason to filename if provided\n",
					"        reason_tag = f\"_{reason}\" if reason else \"\"\n",
					"        quarantine_path = f\"{quarantine_dir}/{timestamp}{reason_tag}_{file_name}\"\n",
					"        \n",
					"        print(f\"Copying file to quarantine: {file_path} -> {quarantine_path}\")\n",
					"        \n",
					"        # Execute copy with detailed error handling\n",
					"        try:\n",
					"            mssparkutils.fs.cp(file_path, quarantine_path, True)\n",
					"            print(f\"Copy to quarantine completed (source preserved)\")\n",
					"            \n",
					"            # Verify the file was copied\n",
					"            if mssparkutils.fs.exists(quarantine_path):\n",
					"                print(f\"Verified file exists in quarantine: {quarantine_path}\")\n",
					"                \n",
					"                # Update tracking dictionary\n",
					"                moved_files['quarantine'].append(quarantine_path)\n",
					"                \n",
					"                # REMOVED THE DELETION CODE\n",
					"                \n",
					"                return True, quarantine_path\n",
					"            else:\n",
					"                print(f\"ERROR: Copy succeeded but file not found in quarantine: {quarantine_path}\")\n",
					"                return False, \"file_not_found_after_copy\"\n",
					"        except Exception as cp_e:\n",
					"            print(f\"Error during copy to quarantine: {str(cp_e)}\")\n",
					"            return False, str(cp_e)\n",
					"    except Exception as e:\n",
					"        print(f\"Error moving file to quarantine: {str(e)}\")\n",
					"        print(f\"Traceback: {traceback.format_exc()}\")\n",
					"        return False, str(e)"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"source": [
					"def process_metadata_file(file_path, file_name, storage_account):\n",
					"    \"\"\"Process metadata file\"\"\"\n",
					"    try:\n",
					"        # Debug info\n",
					"        print(f\"  process_metadata_file received: path={file_path}, name={file_name}\")\n",
					"        # Make sure metadata directory exists\n",
					"        metadata_dir = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/metadata\"\n",
					"        if not mssparkutils.fs.exists(metadata_dir):\n",
					"            print(f\"Creating metadata directory: {metadata_dir}\")\n",
					"            mssparkutils.fs.mkdirs(metadata_dir)\n",
					"        \n",
					"        # Create a unique name by including part of the original file path\n",
					"        unique_name = file_path.replace(f\"abfss://juror-ingestion@{storage_account}.dfs.core.windows.net/\", \"\").replace(\"/\", \"_\")\n",
					"        destination_path = f\"{metadata_dir}/{unique_name}\"\n",
					"        \n",
					"        print(f\"Moving metadata file: {file_path} -> {destination_path}\")\n",
					"        \n",
					"        # Copy file to metadata directory\n",
					"        mssparkutils.fs.cp(file_path, destination_path, True)\n",
					"        print(f\"Successfully copied metadata file\")\n",
					"        \n",
					"        print(f\"Moved metadata file to {destination_path}\")\n",
					"        return True, destination_path\n",
					"    except Exception as e:\n",
					"        print(f\"Error moving metadata file {file_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"Traceback: {traceback.format_exc()}\")\n",
					"        return False, None"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"source": [
					"def process_empty_file(file_path, storage_account, moved_files):\n",
					"    \"\"\"Move an empty file to the empty folder\"\"\"\n",
					"    try:\n",
					"        # Make sure empty directory exists\n",
					"        empty_dir = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/empty\"\n",
					"        if not mssparkutils.fs.exists(empty_dir):\n",
					"            print(f\"Creating empty files directory: {empty_dir}\")\n",
					"            mssparkutils.fs.mkdirs(empty_dir)\n",
					"        \n",
					"        # Create a unique filename\n",
					"        file_name = os.path.basename(file_path)\n",
					"        timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
					"        empty_path = f\"{empty_dir}/{timestamp}_{file_name}\"\n",
					"        \n",
					"        print(f\"Moving empty file: {file_path} -> {empty_path}\")\n",
					"        \n",
					"        # Copy file to empty directory\n",
					"        mssparkutils.fs.cp(file_path, empty_path, True)\n",
					"        print(f\"Successfully copied empty file\")\n",
					"        \n",
					"        # Update tracking dictionary (add 'empty' category if it doesn't exist)\n",
					"        if 'empty' not in moved_files:\n",
					"            moved_files['empty'] = []\n",
					"        moved_files['empty'].append(empty_path)\n",
					"        \n",
					"        return True, empty_path\n",
					"    except Exception as e:\n",
					"        print(f\"Error moving empty file {file_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"Traceback: {traceback.format_exc()}\")\n",
					"        return False, None"
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"source": [
					"def explicit_file_copy(source_path, dest_path, storage_account):\n",
					"    \"\"\"Explicitly copy a file with better error handling and verification but no deletion\"\"\"\n",
					"    try:\n",
					"        # Debug info\n",
					"        print(f\"  Copying {source_path} to {dest_path}\")\n",
					"        \n",
					"        # Ensure destination directory exists\n",
					"        dest_dir = os.path.dirname(dest_path)\n",
					"        if not mssparkutils.fs.exists(dest_dir):\n",
					"            print(f\"  Creating destination directory: {dest_dir}\")\n",
					"            mssparkutils.fs.mkdirs(dest_dir)\n",
					"        \n",
					"        # Check if source exists\n",
					"        if not mssparkutils.fs.exists(source_path):\n",
					"            print(f\"  Source file does not exist: {source_path}\")\n",
					"            return False\n",
					"        \n",
					"        # Execute the copy\n",
					"        mssparkutils.fs.cp(source_path, dest_path, True)\n",
					"        \n",
					"        # Verify the file was copied\n",
					"        if mssparkutils.fs.exists(dest_path):\n",
					"            print(f\"  Verified file copied successfully\")\n",
					"            \n",
					"            # REMOVED THE DELETION CODE HERE\n",
					"            \n",
					"            return True\n",
					"        else:\n",
					"            print(f\"  ERROR: Copy failed, destination file not found: {dest_path}\")\n",
					"            return False\n",
					"    except Exception as e:\n",
					"        print(f\"  Error during file copy: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"  Traceback: {traceback.format_exc()}\")\n",
					"        return False"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"source": [
					"def process_file(source_path, date_folder, la_folder, file_name, storage_account):\n",
					"    \"\"\"Process CSV or Excel file and convert to Parquet format\"\"\"\n",
					"    try:\n",
					"        # Create the folder name in the format YYYYMMDD_LA-Name\n",
					"        folder_key = f\"{date_folder}_{la_folder}\"\n",
					"        file_type = get_file_type(source_path)\n",
					"        \n",
					"        if not file_type:\n",
					"            print(f\"  Cannot determine file type for {source_path}\")\n",
					"            return False, None\n",
					"        \n",
					"        # Destination path for Parquet file (same regardless of source type)\n",
					"        dest_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/files/{folder_key}.parquet\"\n",
					"        \n",
					"        # Read the source file using Spark based on file type\n",
					"        try:\n",
					"            if file_type == 'excel':\n",
					"                print(f\"  Reading Excel file: {source_path}\")\n",
					"                df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
					"                    .option(\"header\", \"true\") \\\n",
					"                    .option(\"inferSchema\", \"true\") \\\n",
					"                    .load(source_path)\n",
					"            else:  # csv\n",
					"                print(f\"  Reading CSV file: {source_path}\")\n",
					"                df = spark.read \\\n",
					"                    .option(\"header\", \"true\") \\\n",
					"                    .option(\"inferSchema\", \"true\") \\\n",
					"                    .csv(source_path)\n",
					"            \n",
					"            # Add metadata columns for tracking\n",
					"            df = df.withColumn(\"source_file\", lit(source_path))\n",
					"            df = df.withColumn(\"source_type\", lit(file_type))\n",
					"            df = df.withColumn(\"ingestion_date\", lit(current_timestamp()))\n",
					"            df = df.withColumn(\"date_folder\", lit(date_folder))\n",
					"            df = df.withColumn(\"la_folder\", lit(la_folder))\n",
					"            \n",
					"            # Write to Parquet format\n",
					"            print(f\"  Writing to Parquet: {dest_path}\")\n",
					"            df.write.mode(\"overwrite\").parquet(dest_path)\n",
					"            \n",
					"            # Verify the Parquet file was created\n",
					"            if mssparkutils.fs.exists(dest_path):\n",
					"                print(f\"  Successfully converted to Parquet at: {dest_path}\")\n",
					"                return True, f\"parquet/{folder_key}.parquet\"\n",
					"            else:\n",
					"                print(f\"  ERROR: Parquet file not created at: {dest_path}\")\n",
					"                return False, None\n",
					"                \n",
					"        except Exception as read_error:\n",
					"            print(f\"  ERROR reading or converting file: {str(read_error)}\")\n",
					"            import traceback\n",
					"            print(f\"  Traceback: {traceback.format_exc()}\")\n",
					"            \n",
					"            # Fallback to simple copy without conversion if there's an error\n",
					"            print(f\"  Falling back to direct file copy without conversion\")\n",
					"            if file_type == 'excel':\n",
					"                fallback_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/files/{folder_key}.xlsx\"\n",
					"            else:  # csv\n",
					"                fallback_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/files/{folder_key}.csv\"\n",
					"            \n",
					"            mssparkutils.fs.cp(source_path, fallback_path, True)\n",
					"            \n",
					"            if mssparkutils.fs.exists(fallback_path):\n",
					"                print(f\"  Successfully copied file using fallback method\")\n",
					"                return True, f\"{file_type}/{folder_key}.{file_type}\"\n",
					"            else:\n",
					"                print(f\"  ERROR: Fallback copy failed\")\n",
					"                return False, None\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"  ERROR in process_file: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"  Traceback: {traceback.format_exc()}\")\n",
					"        return False, None"
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"source": [
					"def find_la_data_in_path(file_path):\n",
					"    \"\"\"Find LA_Data in path and extract date and LA folders with improved detection\"\"\"\n",
					"    parts = file_path.split('/')\n",
					"    print(f\"Path parts: {parts}\")\n",
					"    \n",
					"    # Try to find LA_Data in the path\n",
					"    la_index = -1\n",
					"    for i, part in enumerate(parts):\n",
					"        if part.lower() == 'la_data':\n",
					"            la_index = i\n",
					"            break\n",
					"    \n",
					"    if la_index == -1:\n",
					"        # Try more flexible matching (in case of case differences or formatting issues)\n",
					"        for i, part in enumerate(parts):\n",
					"            if 'la' in part.lower() and 'data' in part.lower():\n",
					"                print(f\"Found likely LA_Data folder with name: {part}\")\n",
					"                la_index = i\n",
					"                break\n",
					"    \n",
					"    if la_index == -1:\n",
					"        print(\"Could not find LA_Data or similar folder in path\")\n",
					"        return None, None\n",
					"        \n",
					"    print(f\"Found LA_Data at index {la_index}\")\n",
					"    \n",
					"    # Get date folder (before LA_Data)\n",
					"    date_folder = None\n",
					"    if la_index > 0:\n",
					"        date_folder = parts[la_index - 1]\n",
					"        print(f\"Date folder: {date_folder}\")\n",
					"    \n",
					"    # Get LA folder (after LA_Data)\n",
					"    la_folder = None\n",
					"    if len(parts) > la_index + 1:\n",
					"        la_folder = parts[la_index + 1]\n",
					"        print(f\"LA folder: {la_folder}\")\n",
					"    \n",
					"    return date_folder, la_folder"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"source": [
					"def get_folder_parts(file_path):\n",
					"    \"\"\"Extract folder information with quarantine flagging for problematic patterns\"\"\"\n",
					"    parts = file_path.split('/')\n",
					"    print(f\"Path parts: {parts}\")\n",
					"    \n",
					"    # Initialize with defaults\n",
					"    date_folder = None\n",
					"    la_folder = None\n",
					"    confidence = \"low\"\n",
					"    \n",
					"    # Check for standard pattern with LA_Data\n",
					"    la_index = -1\n",
					"    for i, part in enumerate(parts):\n",
					"        if part.lower() == 'la_data':\n",
					"            la_index = i\n",
					"            break\n",
					"    \n",
					"    if la_index != -1:\n",
					"        # Get date folder (before LA_Data)\n",
					"        if la_index > 0:\n",
					"            date_folder = parts[la_index - 1]\n",
					"        \n",
					"        # Get LA folder (after LA_Data)\n",
					"        if len(parts) > la_index + 1:\n",
					"            la_folder = parts[la_index + 1]\n",
					"        \n",
					"        confidence = \"high\" if date_folder and la_folder else \"low\"\n",
					"    \n",
					"    # Check for problematic pattern like \"20231205/NT - White Ladies Aston.XLSX\"\n",
					"    filename = os.path.basename(file_path)\n",
					"    if not la_folder and \"-\" in filename:\n",
					"        # This is a non-standard pattern - flag for quarantine\n",
					"        confidence = \"very_low\"\n",
					"        \n",
					"        # Still extract something usable for the quarantine filename\n",
					"        date_pattern = re.findall(r'\\d{8}', '/'.join(parts))\n",
					"        if date_pattern:\n",
					"            date_folder = date_pattern[0]\n",
					"        else:\n",
					"            date_folder = time.strftime(\"%Y%m%d\")\n",
					"        \n",
					"        # Extract a clean identifier from the filename\n",
					"        name_part = os.path.splitext(filename)[0]\n",
					"        clean_name = re.sub(r'[^\\w\\-]', '_', name_part)\n",
					"        la_folder = f\"unknown_{clean_name}\"\n",
					"    \n",
					"    # Final fallbacks\n",
					"    if not date_folder:\n",
					"        date_folder = time.strftime(\"%Y%m%d\")\n",
					"        confidence = \"very_low\"\n",
					"    \n",
					"    if not la_folder:\n",
					"        name_part = os.path.splitext(filename)[0]\n",
					"        clean_name = re.sub(r'[^\\w\\-]', '_', name_part)\n",
					"        la_folder = f\"unknown_{clean_name}\"\n",
					"        confidence = \"very_low\"\n",
					"    \n",
					"    print(f\"Extracted: date_folder={date_folder}, la_folder={la_folder}, confidence={confidence}\")\n",
					"    return date_folder, la_folder, confidence"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"source": [
					"def process_folder(path, storage_account, moved_files, depth=0):\n",
					"    \"\"\"Process a folder recursively and return all files with improved ZIP handling\"\"\"\n",
					"    all_files = []\n",
					"    \n",
					"    # Add depth indicator for debugging\n",
					"    indent = \"  \" * depth\n",
					"    try:\n",
					"        print(f\"{indent}Scanning directory (depth {depth}): {path}\")\n",
					"        files = mssparkutils.fs.ls(path)\n",
					"        print(f\"{indent}Found {len(files)} items in {path}\")\n",
					"        \n",
					"        for file_info in files:\n",
					"            if file_info.isDir:\n",
					"                # Process subdirectories recursively\n",
					"                sub_files = process_folder(file_info.path, storage_account, moved_files, depth + 1)\n",
					"                all_files.extend(sub_files)\n",
					"                print(f\"{indent}Found {len(sub_files)} files in subdirectory {file_info.path}\")\n",
					"            else:\n",
					"                # Check for ZIP files explicitly\n",
					"                if file_info.path.lower().endswith('.zip'):\n",
					"                    print(f\"{indent}Found ZIP file: {file_info.path}\")\n",
					"                    \n",
					"                    # Always quarantine ZIP files\n",
					"                    print(f\"{indent}Moving ZIP file to quarantine: {file_info.path}\")\n",
					"                    \n",
					"                    # Debug to verify file exists\n",
					"                    print(f\"{indent}DEBUG: ZIP file exists: {mssparkutils.fs.exists(file_info.path)}\")\n",
					"                    \n",
					"                    # Move ZIP to quarantine with explicit reason\n",
					"                    success, quarantine_path = move_to_quarantine(file_info.path, storage_account, moved_files, \"zip_file\")\n",
					"                    \n",
					"                    if success:\n",
					"                        print(f\"{indent}Successfully moved ZIP to quarantine: {quarantine_path}\")\n",
					"                    else:\n",
					"                        print(f\"{indent}Failed to move ZIP to quarantine, will attempt to process contents: {file_info.path}\")\n",
					"                        # Add file to list only if quarantine fails\n",
					"                        all_files.append(file_info.path)\n",
					"                \n",
					"                # Add all non-ZIP files to process normally\n",
					"                elif 'File Structure Definitions.xlsx' not in file_info.path:\n",
					"                    print(f\"{indent}Found file: {file_info.path}\")\n",
					"                    all_files.append(file_info.path)\n",
					"    except Exception as e:\n",
					"        print(f\"{indent}Error accessing path {path}: {str(e)}\")\n",
					"        print(f\"{indent}Traceback: {traceback.format_exc()}\")\n",
					"    \n",
					"    return all_files"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"source": [
					"def create_file_index(source_path, storage_account):\n",
					"    \"\"\"Create a JSON index of all files in the source path\"\"\"\n",
					"    print(f\"Creating index of files in {source_path}\")\n",
					"    \n",
					"    file_index = {\n",
					"        \"created_at\": time.strftime(\"%Y%m%d%H%M%S\"),\n",
					"        \"files\": []\n",
					"    }\n",
					"    \n",
					"    # Process the folder recursively - simplified version that doesn't process ZIP files\n",
					"    def process_folder_for_index(path, depth=0):\n",
					"        indent = \"  \" * depth\n",
					"        try:\n",
					"            print(f\"{indent}Scanning directory (depth {depth}): {path}\")\n",
					"            files = mssparkutils.fs.ls(path)\n",
					"            \n",
					"            for file_info in files:\n",
					"                if file_info.isDir:\n",
					"                    process_folder_for_index(file_info.path, depth + 1)\n",
					"                else:\n",
					"                    print(f\"{indent}Found file: {file_info.path}\")\n",
					"                    \n",
					"                    # Extract date_folder and la_folder\n",
					"                    date_folder, la_folder, confidence = get_folder_parts(file_info.path)\n",
					"                    \n",
					"                    # Add file to index\n",
					"                    file_index[\"files\"].append({\n",
					"                        \"path\": file_info.path,\n",
					"                        \"file_name\": os.path.basename(file_info.path),\n",
					"                        \"date_folder\": date_folder,\n",
					"                        \"la_folder\": la_folder,\n",
					"                        \"confidence\": confidence,\n",
					"                        \"file_type\": get_file_type(file_info.path),\n",
					"                        \"is_empty\": is_empty_file(file_info.path),\n",
					"                        \"is_metadata\": is_metadata_file(file_info.path),\n",
					"                        \"should_process\": should_process_file(file_info.path) and 'File Structure Definitions.xlsx' not in file_info.path,\n",
					"                        \"processed\": False\n",
					"                    })\n",
					"        except Exception as e:\n",
					"            print(f\"{indent}Error accessing path {path}: {str(e)}\")\n",
					"    \n",
					"    # Start the recursive process\n",
					"    process_folder_for_index(source_path)\n",
					"    \n",
					"    # Write the index to a JSON file\n",
					"    # Write the index to a JSON file\n",
					"    index_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/index/file_index.json\"\n",
					"    \n",
					"    # Ensure the index directory exists\n",
					"    index_dir = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/index\"\n",
					"    if not mssparkutils.fs.exists(index_dir):\n",
					"        print(f\"Creating index directory: {index_dir}\")\n",
					"        mssparkutils.fs.mkdirs(index_dir)\n",
					"    \n",
					"    # Convert to a proper DataFrame using a list of Row objects\n",
					"    from pyspark.sql import Row\n",
					"    index_rows = []\n",
					"    \n",
					"    # Add each file as a separate row in the DataFrame\n",
					"    for file_data in file_index[\"files\"]:\n",
					"        # Convert the individual file data to a Row\n",
					"        file_row = Row(\n",
					"            path=file_data[\"path\"],\n",
					"            file_name=file_data[\"file_name\"],\n",
					"            date_folder=file_data[\"date_folder\"],\n",
					"            la_folder=file_data[\"la_folder\"],\n",
					"            confidence=file_data[\"confidence\"],\n",
					"            file_type=file_data[\"file_type\"],\n",
					"            is_empty=file_data[\"is_empty\"],\n",
					"            is_metadata=file_data[\"is_metadata\"],\n",
					"            should_process=file_data[\"should_process\"],\n",
					"            processed=file_data[\"processed\"]\n",
					"        )\n",
					"        index_rows.append(file_row)\n",
					"    \n",
					"    # Create DataFrame from the rows\n",
					"    index_df = spark.createDataFrame(index_rows)\n",
					"    \n",
					"    # Write the DataFrame as a Parquet file (more robust than text)\n",
					"    index_df.write.mode(\"overwrite\").parquet(index_path)\n",
					"    \n",
					"    print(f\"Created index with {len(file_index['files'])} files at {index_path}\")\n",
					"    return file_index"
				],
				"execution_count": 36
			},
			{
				"cell_type": "code",
				"source": [
					"def process_individual_file(file_info, storage_account, moved_files):\n",
					"    \"\"\"Process a single file based on its index information\"\"\"\n",
					"    try:\n",
					"        # Extract info from the file_info dictionary\n",
					"        file_path = file_info[\"path\"]\n",
					"        file_name = file_info[\"file_name\"]  \n",
					"        date_folder = file_info[\"date_folder\"]\n",
					"        la_folder = file_info[\"la_folder\"]\n",
					"        confidence = file_info[\"confidence\"]\n",
					"        file_type = file_info[\"file_type\"]  # Get file type from info\n",
					"        \n",
					"        print(f\"\\nProcessing file: {file_path}\")\n",
					"        \n",
					"        # Check if file still exists\n",
					"        if not mssparkutils.fs.exists(file_path):\n",
					"            print(f\"  File no longer exists, might have been processed already: {file_path}\")\n",
					"            return False\n",
					"        \n",
					"        # Check for problematic \"NT - Location\" pattern - move directly to quarantine\n",
					"        if \"-\" in file_name and not (file_name.startswith(\"unknown_\") or \"la_data\" in file_path.lower()):\n",
					"            # This might be one of the problematic patterns\n",
					"            quarantine_reason = \"non_standard_la_code\"\n",
					"            print(f\"  Moving file to quarantine due to: {quarantine_reason}\")\n",
					"            success, quarantine_path = move_to_quarantine(file_path, storage_account, moved_files, quarantine_reason)\n",
					"            if success:\n",
					"                print(f\"  Successfully moved to quarantine: {quarantine_path}\")\n",
					"                return True\n",
					"            else:\n",
					"                print(f\"  Failed to move to quarantine\")\n",
					"                moved_files['failed'].append(f\"{file_path} (quarantine failed - {quarantine_reason})\")\n",
					"                return False\n",
					"        \n",
					"        # Check if file is empty\n",
					"        if file_info[\"is_empty\"]:\n",
					"            print(f\"  File is empty, moving to empty files folder\")\n",
					"            success, empty_path = process_empty_file(file_path, storage_account, moved_files)\n",
					"            if success:\n",
					"                print(f\"  Successfully processed empty file to: {empty_path}\")\n",
					"                return True\n",
					"            else:\n",
					"                moved_files['failed'].append(f\"{file_path} (empty file move failed)\")\n",
					"                print(f\"  Failed to process empty file\")\n",
					"                return False\n",
					"        \n",
					"        # Check if it's a metadata file\n",
					"        if file_info[\"is_metadata\"]:\n",
					"            print(f\"  Identified as metadata file\")\n",
					"            success, new_path = process_metadata_file(file_path, file_name, storage_account)\n",
					"            if success:\n",
					"                moved_files['metadata_success'].append(new_path)\n",
					"                print(f\"  Successfully processed metadata file to: {new_path}\")\n",
					"                return True\n",
					"            else:\n",
					"                moved_files['failed'].append(f\"{file_path} (metadata move failed)\")\n",
					"                print(f\"  Failed to process metadata file\")\n",
					"                return False\n",
					"        \n",
					"        # Check if it's a supported file type\n",
					"        if not file_info[\"should_process\"]:\n",
					"            print(f\"  Skipping unsupported file format: {file_path}\")\n",
					"            return False\n",
					"        else:\n",
					"            print(f\"  Supported file format: {file_info['file_type']}\")\n",
					"            \n",
					"        # NEW STEP: Check for overseas address columns\n",
					"        if file_type in ('csv', 'excel'):\n",
					"            has_overseas, columns = check_for_overseas_columns(file_path, file_type)\n",
					"            if has_overseas:\n",
					"                print(f\"  File contains overseas columns, moving to overseas container\")\n",
					"                success, overseas_path = move_to_overseas(file_path, storage_account, moved_files)\n",
					"                if success:\n",
					"                    print(f\"  Successfully moved to overseas container: {overseas_path}\")\n",
					"                    return True\n",
					"                else:\n",
					"                    print(f\"  Failed to move to overseas container\")\n",
					"                    moved_files['failed'].append(f\"{file_path} (overseas move failed)\")\n",
					"                    return False\n",
					"        \n",
					"        # Check if we should move to quarantine for manual inspection\n",
					"        should_quarantine = False\n",
					"        quarantine_reason = \"\"\n",
					"        \n",
					"        # Improved quarantine conditions:\n",
					"        if not date_folder or not la_folder:\n",
					"            should_quarantine = True\n",
					"            quarantine_reason = \"missing_folder_parts\"\n",
					"        elif la_folder and \"la_data\" in la_folder.lower():\n",
					"            # Quarantine if LA folder is just \"LA_Data\"\n",
					"            should_quarantine = True\n",
					"            quarantine_reason = \"invalid_la_code_la_data\"\n",
					"        elif confidence == \"very_low\":\n",
					"            should_quarantine = True\n",
					"            quarantine_reason = f\"invalid_la_code_format_{la_folder}\"\n",
					"        elif confidence == \"low\" and not re.match(r'^\\\\d{8}$', date_folder):\n",
					"            should_quarantine = True\n",
					"            quarantine_reason = f\"low_confidence_structure_{date_folder}_{la_folder}\"\n",
					"        \n",
					"        if should_quarantine:\n",
					"            print(f\"  Moving file to quarantine due to: {quarantine_reason}\")\n",
					"            success, quarantine_path = move_to_quarantine(file_path, storage_account, moved_files, quarantine_reason)\n",
					"            if success:\n",
					"                print(f\"  Successfully moved to quarantine: {quarantine_path}\")\n",
					"                return True\n",
					"            else:\n",
					"                print(f\"  Failed to move to quarantine\")\n",
					"                moved_files['failed'].append(f\"{file_path} (quarantine failed - {quarantine_reason})\")\n",
					"                return False\n",
					"        \n",
					"        # Process the file if it passed validation\n",
					"        print(f\"  Valid folder structure, processing file...\")\n",
					"        success, new_path = process_file(file_path, date_folder, la_folder, file_name, storage_account)\n",
					"        if success:\n",
					"            moved_files['data_success'].append(new_path)\n",
					"            print(f\"  Successfully processed to: {new_path}\")\n",
					"            return True\n",
					"        else:\n",
					"            moved_files['failed'].append(f\"{file_info['path']} (processing failed)\")\n",
					"            print(f\"  Failed to process file\")\n",
					"            return False\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error processing {file_path}: {str(e)}\")\n",
					"        print(f\"  Error processing {file_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"  Traceback: {traceback.format_exc()}\")\n",
					"        moved_files['failed'].append(f\"{file_path} (processing error)\")\n",
					"        return False"
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"source": [
					"def process_file_group(file_group, date_folder, la_folder, storage_account, moved_files):\n",
					"    \"\"\"Process a group of files with the same date and LA folder and convert to Parquet\"\"\"\n",
					"    try:\n",
					"        print(f\"\\nProcessing file group: date={date_folder}, LA={la_folder}, files={len(file_group)}\")\n",
					"        \n",
					"        # Create the folder name in the format YYYYMMDD_LA-Name\n",
					"        folder_key = f\"{date_folder}_{la_folder}\"\n",
					"        \n",
					"        # Ensure the destination directory exists\n",
					"        dest_dir = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/files\"\n",
					"        if not mssparkutils.fs.exists(dest_dir):\n",
					"            print(f\"Creating destination directory: {dest_dir}\")\n",
					"            mssparkutils.fs.mkdirs(dest_dir)\n",
					"        \n",
					"        # Destination path for Parquet file\n",
					"        dest_path = f\"{dest_dir}/{folder_key}.parquet\"\n",
					"        \n",
					"        # Check which files still exist and get their types\n",
					"        valid_files = []\n",
					"        for info in file_group:\n",
					"            path = info[\"path\"]\n",
					"            if mssparkutils.fs.exists(path):\n",
					"                valid_files.append(info)\n",
					"                print(f\"  File exists and is valid: {path}\")\n",
					"            else:\n",
					"                print(f\"  File doesn't exist, skipping from group: {path}\")\n",
					"        \n",
					"        if not valid_files:\n",
					"            print(\"  No valid files remain in this group\")\n",
					"            return False, None\n",
					"            \n",
					"        # Group files by type for easier processing\n",
					"        csv_files = [info for info in valid_files if info[\"file_type\"] == \"csv\"]\n",
					"        excel_files = [info for info in valid_files if info[\"file_type\"] == \"excel\"]\n",
					"        \n",
					"        print(f\"  Found {len(csv_files)} CSV files and {len(excel_files)} Excel files to process\")\n",
					"        \n",
					"        # Process all files and merge into a single Parquet file\n",
					"        dfs = []\n",
					"        \n",
					"        # Process CSV files if present\n",
					"        for info in csv_files:\n",
					"            try:\n",
					"                print(f\"  Reading CSV file: {info['path']}\")\n",
					"                df = spark.read \\\n",
					"                    .option(\"header\", \"true\") \\\n",
					"                    .option(\"inferSchema\", \"true\") \\\n",
					"                    .csv(info[\"path\"])\n",
					"                \n",
					"                # Add metadata\n",
					"                df = df.withColumn(\"source_file\", lit(info[\"path\"]))\n",
					"                df = df.withColumn(\"source_type\", lit(\"csv\"))\n",
					"                \n",
					"                dfs.append(df)\n",
					"                print(f\"  Successfully read CSV file with {df.count()} rows and {len(df.columns)} columns\")\n",
					"            except Exception as e:\n",
					"                print(f\"  Error reading CSV file {info['path']}: {str(e)}\")\n",
					"                moved_files['failed'].append(f\"{info['path']} (CSV read failed)\")\n",
					"        \n",
					"        # Process Excel files if present\n",
					"        for i, info in enumerate(excel_files):\n",
					"            try:\n",
					"                print(f\"  Reading Excel file {i+1}/{len(excel_files)}: {info['path']}\")\n",
					"                \n",
					"                # First, try without specifying sheet (uses default/active sheet)\n",
					"                df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
					"                    .option(\"header\", \"true\") \\\n",
					"                    .option(\"inferSchema\", \"true\") \\\n",
					"                    .option(\"treatEmptyValuesAsNulls\", \"true\") \\\n",
					"                    .load(info[\"path\"])\n",
					"                \n",
					"                # Add metadata\n",
					"                df = df.withColumn(\"source_file\", lit(info[\"path\"]))\n",
					"                df = df.withColumn(\"source_type\", lit(\"excel\"))\n",
					"                \n",
					"                dfs.append(df)\n",
					"                print(f\"  Successfully read Excel file {i+1} with {df.count()} rows and {len(df.columns)} columns\")\n",
					"                \n",
					"            except Exception as e:\n",
					"                print(f\"  Error reading Excel file {info['path']} with default sheet: {str(e)}\")\n",
					"                \n",
					"                # Try with specific sheet indices (0, 1, 2)\n",
					"                success = False\n",
					"                \n",
					"                for sheet_index in range(3):  # Try the first 3 sheets (0, 1, 2)\n",
					"                    try:\n",
					"                        print(f\"  Retrying with sheet index: {sheet_index}\")\n",
					"                        df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
					"                            .option(\"header\", \"true\") \\\n",
					"                            .option(\"inferSchema\", \"true\") \\\n",
					"                            .option(\"sheetIndex\", sheet_index) \\\n",
					"                            .option(\"treatEmptyValuesAsNulls\", \"true\") \\\n",
					"                            .load(info[\"path\"])\n",
					"                        \n",
					"                        # Add metadata\n",
					"                        df = df.withColumn(\"source_file\", lit(info[\"path\"]))\n",
					"                        df = df.withColumn(\"source_type\", lit(\"excel\"))\n",
					"                        \n",
					"                        dfs.append(df)\n",
					"                        print(f\"  Successfully read Excel file with sheet index {sheet_index}: {df.count()} rows and {len(df.columns)} columns\")\n",
					"                        success = True\n",
					"                        break\n",
					"                    except Exception as retry_e:\n",
					"                        print(f\"  Failed with sheet index {sheet_index}: {str(retry_e)}\")\n",
					"                \n",
					"                if not success:\n",
					"                    # One last attempt - try all available options to load the Excel file\n",
					"                    try:\n",
					"                        print(f\"  Final attempt with minimal options...\")\n",
					"                        df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
					"                            .option(\"header\", \"true\") \\\n",
					"                            .load(info[\"path\"])\n",
					"                        \n",
					"                        # Add metadata\n",
					"                        df = df.withColumn(\"source_file\", lit(info[\"path\"]))\n",
					"                        df = df.withColumn(\"source_type\", lit(\"excel\"))\n",
					"                        \n",
					"                        dfs.append(df)\n",
					"                        print(f\"  Successfully read Excel file with minimal options: {df.count()} rows and {len(df.columns)} columns\")\n",
					"                        success = True\n",
					"                    except Exception as final_e:\n",
					"                        print(f\"  Failed all attempts to read Excel file: {str(final_e)}\")\n",
					"                        moved_files['failed'].append(f\"{info['path']} (Excel read failed)\")\n",
					"        \n",
					"        if dfs:\n",
					"            print(f\"  Successfully read {len(dfs)} files, now merging...\")\n",
					"            \n",
					"            # Add common metadata\n",
					"            for i in range(len(dfs)):\n",
					"                dfs[i] = dfs[i].withColumn(\"ingestion_date\", lit(current_timestamp()))\n",
					"                dfs[i] = dfs[i].withColumn(\"date_folder\", lit(date_folder))\n",
					"                dfs[i] = dfs[i].withColumn(\"la_folder\", lit(la_folder))\n",
					"            \n",
					"            # Merge the dataframes\n",
					"            try:\n",
					"                # Print schema of each DataFrame to help debug unionByName issues\n",
					"                for i, df in enumerate(dfs):\n",
					"                    print(f\"  DataFrame {i+1} schema:\")\n",
					"                    df.printSchema()\n",
					"                \n",
					"                if len(dfs) == 1:\n",
					"                    merged_df = dfs[0]\n",
					"                    print(\"  Only one DataFrame to process, no merging needed\")\n",
					"                else:\n",
					"                    # Calculate schema differences for debugging\n",
					"                    base_cols = set(dfs[0].columns)\n",
					"                    for i, df in enumerate(dfs[1:]):\n",
					"                        joining_cols = set(df.columns)\n",
					"                        only_in_base = base_cols - joining_cols\n",
					"                        only_in_joining = joining_cols - base_cols\n",
					"                        \n",
					"                        if only_in_base or only_in_joining:\n",
					"                            print(f\"  Schema differences between DataFrame 1 and {i+2}:\")\n",
					"                            print(f\"    Only in DataFrame 1: {only_in_base}\")\n",
					"                            print(f\"    Only in DataFrame {i+2}: {only_in_joining}\")\n",
					"                    \n",
					"                    # Merge dataframes\n",
					"                    merged_df = dfs[0]\n",
					"                    for i, df in enumerate(dfs[1:]):\n",
					"                        print(f\"  Merging DataFrame {i+2}...\")\n",
					"                        try:\n",
					"                            # Use allowMissingColumns to handle schema differences\n",
					"                            merged_df = merged_df.unionByName(df, allowMissingColumns=True)\n",
					"                            print(f\"  Successfully merged DataFrame {i+2}\")\n",
					"                        except Exception as union_error:\n",
					"                            print(f\"  Error in unionByName: {str(union_error)}\")\n",
					"                            print(\"  Attempting more aggressive schema alignment...\")\n",
					"                            \n",
					"                            # Get all unique column names across both dataframes\n",
					"                            all_columns = set(merged_df.columns).union(set(df.columns))\n",
					"                            \n",
					"                            # Add missing columns to each dataframe\n",
					"                            for col_name in all_columns:\n",
					"                                if col_name not in merged_df.columns:\n",
					"                                    merged_df = merged_df.withColumn(col_name, lit(None))\n",
					"                                if col_name not in df.columns:\n",
					"                                    df = df.withColumn(col_name, lit(None))\n",
					"                            \n",
					"                            # Try the union again with identical schemas\n",
					"                            merged_df = merged_df.unionByName(df)\n",
					"                            print(\"  Merge successful after schema alignment\")\n",
					"                \n",
					"                # Write the merged dataframe as Parquet\n",
					"                print(f\"  Writing merged data to Parquet: {dest_path}\")\n",
					"                merged_df.write.mode(\"overwrite\").parquet(dest_path)\n",
					"                \n",
					"                # Check if Parquet file was created\n",
					"                if mssparkutils.fs.exists(dest_path):\n",
					"                    print(f\"  Successfully created merged Parquet file with {merged_df.count()} rows\")\n",
					"                    \n",
					"                    # Update success tracking\n",
					"                    for info in valid_files:\n",
					"                        moved_files['data_success'].append(dest_path)\n",
					"                    \n",
					"                    return True, dest_path\n",
					"                else:\n",
					"                    print(f\"  ERROR: Parquet file not created at {dest_path}\")\n",
					"                    return False, None\n",
					"            except Exception as merge_error:\n",
					"                print(f\"  Error merging dataframes: {str(merge_error)}\")\n",
					"                import traceback\n",
					"                print(f\"  Traceback: {traceback.format_exc()}\")\n",
					"                \n",
					"                # Fallback to processing files individually\n",
					"                print(f\"  Falling back to individual file processing\")\n",
					"                for info in valid_files:\n",
					"                    process_individual_file(info, storage_account, moved_files)\n",
					"                \n",
					"                return False, None\n",
					"        else:\n",
					"            print(f\"  No dataframes created from input files\")\n",
					"            return False, None\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"Error processing file group: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"Traceback: {traceback.format_exc()}\")\n",
					"        return False, None"
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"source": [
					"def print_detailed_summary(moved_files):\n",
					"    \"\"\"Print detailed summary of moved files\"\"\"\n",
					"    print(\"\\nSummary of Moved Files:\")\n",
					"    print(f\"Data Files Successfully Moved: {len(moved_files['data_success'])}\")\n",
					"    for file in moved_files['data_success']:\n",
					"        print(f\"  - {file}\")\n",
					"    print(f\"Metadata Files Successfully Moved: {len(moved_files['metadata_success'])}\")\n",
					"    for file in moved_files['metadata_success']:\n",
					"        print(f\"  - {file}\")\n",
					"    # Add overseas files summary\n",
					"    overseas_count = len(moved_files.get('overseas', []))\n",
					"    print(f\"Overseas Files Moved: {overseas_count}\")\n",
					"    for file in moved_files.get('overseas', []):\n",
					"        print(f\"  - {file}\")\n",
					"    print(f\"Failed Files: {len(moved_files['failed'])}\")\n",
					"    for file in moved_files['failed']:\n",
					"        print(f\"  - {file}\")\n",
					"    print(f\"Quarantined Files: {len(moved_files['quarantine'])}\")\n",
					"    for file in moved_files['quarantine']:\n",
					"        print(f\"  - {file}\")"
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"source": [
					"def main():\n",
					"    storage_account = \"baubaisadfsastg\"\n",
					"    source_path = f\"abfss://juror-ingestion@{storage_account}.dfs.core.windows.net\"\n",
					"    index_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/index/file_index.json\"\n",
					"    log_file_path = f\"abfss://juror-ingestion@{storage_account}.dfs.core.windows.net/processed_files_log.json\"\n",
					"    \n",
					"    # Add a detailed log of all operations\n",
					"    operations_log = []\n",
					"    def log_operation(operation, file_path, status, details=None):\n",
					"        entry = {\n",
					"            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
					"            \"operation\": operation,\n",
					"            \"file_path\": file_path,\n",
					"            \"status\": status,\n",
					"            \"details\": details or {}\n",
					"        }\n",
					"        operations_log.append(entry)\n",
					"        print(f\"LOG: {entry['timestamp']} | {operation} | {file_path} | {status}\")\n",
					"    \n",
					"    moved_files = {\n",
					"        'data_success': [],\n",
					"        'metadata_success': [],\n",
					"        'empty': [],\n",
					"        'overseas': [],\n",
					"        'failed': [],\n",
					"        'quarantine': []\n",
					"    }\n",
					"    \n",
					"    # Verify source path exists\n",
					"    if not mssparkutils.fs.exists(source_path):\n",
					"        logging.error(f\"Source path does not exist: {source_path}\")\n",
					"        print(f\"ERROR: Source path does not exist: {source_path}\")\n",
					"        return\n",
					"    else:\n",
					"        logging.info(f\"Source path exists: {source_path}\")\n",
					"        print(f\"Source path exists: {source_path}\")\n",
					"    \n",
					"    # Ensure destination directories exist\n",
					"    dest_dirs = [\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/files\",\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/metadata\",\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/quarantine\",\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/empty\",\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/index\",\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/logs\",\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/overseas\" \n",
					"    ]\n",
					"    \n",
					"    for dir_path in dest_dirs:\n",
					"        if not mssparkutils.fs.exists(dir_path):\n",
					"            print(f\"Creating destination directory: {dir_path}\")\n",
					"            mssparkutils.fs.mkdirs(dir_path)\n",
					"            log_operation(\"create_directory\", dir_path, \"success\")\n",
					"    \n",
					"    # Create/update the file index\n",
					"    file_index = {\"files\": []}\n",
					"    try: \n",
					"        print(\"Scanning for files to process...\")\n",
					"        all_files = process_folder(source_path, storage_account, moved_files)\n",
					"        print(f\"Found {len(all_files)} total files\")\n",
					"        \n",
					"        # Create a simple file index structure\n",
					"        file_index = {\n",
					"            \"created_at\": time.strftime(\"%Y%m%d%H%M%S\"),\n",
					"            \"files\": []\n",
					"        }\n",
					"        \n",
					"        # Process each file to get its metadata\n",
					"        for file_path in all_files:\n",
					"            try:\n",
					"                file_name = os.path.basename(file_path)\n",
					"                date_folder, la_folder, confidence = get_folder_parts(file_path)\n",
					"                \n",
					"                file_info = {\n",
					"                    \"path\": file_path,\n",
					"                    \"file_name\": file_name,\n",
					"                    \"date_folder\": date_folder,\n",
					"                    \"la_folder\": la_folder,\n",
					"                    \"confidence\": confidence,\n",
					"                    \"file_type\": get_file_type(file_path),\n",
					"                    \"is_empty\": is_empty_file(file_path),\n",
					"                    \"is_metadata\": is_metadata_file(file_path),\n",
					"                    \"should_process\": should_process_file(file_path) and 'File Structure Definitions.xlsx' not in file_path,\n",
					"                    \"processed\": False\n",
					"                }\n",
					"                file_index[\"files\"].append(file_info)\n",
					"                log_operation(\"index_file\", file_path, \"success\", {\"file_info\": file_info})\n",
					"            except Exception as e:\n",
					"                print(f\"Error indexing file {file_path}: {str(e)}\")\n",
					"                log_operation(\"index_file\", file_path, \"error\", {\"error\": str(e)})\n",
					"    except Exception as e:\n",
					"        print(f\"Error scanning files: {str(e)}\")\n",
					"        print(f\"Traceback: {traceback.format_exc()}\")\n",
					"        log_operation(\"scan_files\", source_path, \"error\", {\"error\": str(e)})\n",
					"    \n",
					"    print(f\"Working with index containing {len(file_index['files'])} files\")\n",
					"    \n",
					"    # Read the log of processed files\n",
					"    try:\n",
					"        processed_files_log = spark.read.json(log_file_path).select(\"file\").rdd.flatMap(lambda x: x).collect()\n",
					"        logging.info(f\"Read processed files log with {len(processed_files_log)} entries\")\n",
					"        print(f\"\\nProcessed files log contains {len(processed_files_log)} entries\")\n",
					"        log_operation(\"read_log\", log_file_path, \"success\", {\"count\": len(processed_files_log)})\n",
					"    except Exception as e:\n",
					"        logging.warning(f\"Could not read processed files log: {str(e)}\")\n",
					"        print(f\"\\nCould not read processed files log: {str(e)}\")\n",
					"        processed_files_log = []\n",
					"        log_operation(\"read_log\", log_file_path, \"error\", {\"error\": str(e)})\n",
					"    \n",
					"    # Filter out files that have already been processed\n",
					"    new_files = []\n",
					"    for file_info in file_index[\"files\"]:\n",
					"        if file_info[\"path\"] not in processed_files_log:\n",
					"            new_files.append(file_info)\n",
					"    \n",
					"    print(f\"\\nNew files to process: {len(new_files)}\")\n",
					"    log_operation(\"filter_files\", \"\", \"success\", {\"total\": len(file_index[\"files\"]), \"new\": len(new_files)})\n",
					"    \n",
					"    # Group files by date and LA folder \n",
					"    file_groups = {}\n",
					"    individual_files = []\n",
					"    \n",
					"    for file_info in new_files:\n",
					"        # Check if the file still exists\n",
					"        if not mssparkutils.fs.exists(file_info[\"path\"]):\n",
					"            print(f\"File no longer exists, skipping: {file_info['path']}\")\n",
					"            log_operation(\"check_file\", file_info[\"path\"], \"skip\", {\"reason\": \"not_exists\"})\n",
					"            continue\n",
					"            \n",
					"        # Files that should be processed individually\n",
					"        if file_info[\"is_empty\"] or file_info[\"is_metadata\"] or file_info[\"confidence\"] == \"very_low\" or not file_info[\"should_process\"]:\n",
					"            individual_files.append(file_info)\n",
					"            continue\n",
					"            \n",
					"        # Group by date and LA folder\n",
					"        group_key = (file_info[\"date_folder\"], file_info[\"la_folder\"])\n",
					"        if group_key not in file_groups:\n",
					"            file_groups[group_key] = []\n",
					"        file_groups[group_key].append(file_info)\n",
					"    \n",
					"    print(f\"Grouped into {len(file_groups)} file groups and {len(individual_files)} individual files\")\n",
					"    log_operation(\"group_files\", \"\", \"success\", {\"groups\": len(file_groups), \"individual\": len(individual_files)})\n",
					"    \n",
					"    # Process individual files first\n",
					"    for file_info in individual_files:\n",
					"        try:\n",
					"            if not isinstance(file_info, dict):\n",
					"                print(f\"ERROR: file_info is not a dictionary: {type(file_info)}\")\n",
					"                log_operation(\"process_file\", \"unknown\", \"error\", {\"error\": \"invalid_file_info_type\"})\n",
					"                continue\n",
					"            \n",
					"            if \"path\" not in file_info:\n",
					"                print(f\"ERROR: file_info does not have a 'path' key: {file_info}\")\n",
					"                log_operation(\"process_file\", \"unknown\", \"error\", {\"error\": \"missing_path_key\"})\n",
					"                continue\n",
					"            \n",
					"            success = process_individual_file(file_info, storage_account, moved_files)\n",
					"            log_operation(\"process_individual\", file_info[\"path\"], \"success\" if success else \"error\")\n",
					"        except Exception as e:\n",
					"            print(f\"Error processing individual file: {str(e)}\")\n",
					"            print(f\"File info: {file_info}\")\n",
					"            print(f\"Traceback: {traceback.format_exc()}\")\n",
					"            log_operation(\"process_individual\", file_info.get(\"path\", \"unknown\"), \"error\", {\"error\": str(e)})\n",
					"            if isinstance(file_info, dict) and \"path\" in file_info:\n",
					"                moved_files['failed'].append(f\"{file_info['path']} (processing error: {str(e)})\")\n",
					"            else:\n",
					"                moved_files['failed'].append(f\"Unknown file (processing error)\")\n",
					"    \n",
					"    # Process file groups\n",
					"    for (date_folder, la_folder), file_group in file_groups.items():\n",
					"        try:\n",
					"            # For clearer logs\n",
					"            group_id = f\"{date_folder}_{la_folder}\"\n",
					"            log_operation(\"process_group\", group_id, \"start\", {\"count\": len(file_group)})\n",
					"            \n",
					"            # Only process groups with more than one file\n",
					"            if len(file_group) > 1:\n",
					"                success, path = process_file_group(file_group, date_folder, la_folder, storage_account, moved_files)\n",
					"                log_operation(\"process_group\", group_id, \"success\" if success else \"error\")\n",
					"                \n",
					"                if not success:\n",
					"                    # If group processing failed, try individual processing\n",
					"                    print(f\"Group processing failed, trying individual processing\")\n",
					"                    log_operation(\"fallback_to_individual\", group_id, \"start\")\n",
					"                    for file_info in file_group:\n",
					"                        process_individual_file(file_info, storage_account, moved_files)\n",
					"            else:\n",
					"                # For single file \"groups\", just process individually\n",
					"                process_individual_file(file_group[0], storage_account, moved_files)\n",
					"                log_operation(\"process_single_in_group\", file_group[0][\"path\"], \"complete\")\n",
					"        except Exception as e:\n",
					"            group_id = f\"{date_folder}_{la_folder}\"\n",
					"            print(f\"Error processing file group {group_id}: {str(e)}\")\n",
					"            print(f\"Traceback: {traceback.format_exc()}\")\n",
					"            log_operation(\"process_group\", group_id, \"error\", {\"error\": str(e)})\n",
					"            \n",
					"            for file_info in file_group:\n",
					"                moved_files['failed'].append(f\"{file_info['path']} (group processing error)\")\n",
					"    \n",
					"    print_detailed_summary(moved_files)\n",
					"    \n",
					"    # Update the log of processed files\n",
					"    try:\n",
					"        new_processed_files = [file_info[\"path\"] for file_info in new_files]\n",
					"        new_processed_files_log = spark.createDataFrame([(file,) for file in new_processed_files], [\"file\"])\n",
					"        \n",
					"        if processed_files_log:\n",
					"            existing_log_df = spark.createDataFrame([(file,) for file in processed_files_log], [\"file\"])\n",
					"            updated_log_df = existing_log_df.union(new_processed_files_log).distinct()\n",
					"        else:\n",
					"            updated_log_df = new_processed_files_log\n",
					"            \n",
					"        updated_log_df.write.mode(\"overwrite\").json(log_file_path)\n",
					"        logging.info(\"Updated processed files log\")\n",
					"        log_operation(\"update_log\", log_file_path, \"success\", {\"count\": len(new_processed_files)})\n",
					"    except Exception as e:\n",
					"        print(f\"Error updating processed files log: {str(e)}\")\n",
					"        print(f\"Traceback: {traceback.format_exc()}\")\n",
					"        log_operation(\"update_log\", log_file_path, \"error\", {\"error\": str(e)})\n",
					"    \n",
					"    # Save the operations log\n",
					"    try:\n",
					"        # Convert operations log to DataFrame\n",
					"        if operations_log:\n",
					"            # Create a proper schema for the operations log\n",
					"            from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
					"            \n",
					"            schema = StructType([\n",
					"                StructField(\"timestamp\", StringType(), True),\n",
					"                StructField(\"operation\", StringType(), True),\n",
					"                StructField(\"file_path\", StringType(), True),\n",
					"                StructField(\"status\", StringType(), True)\n",
					"            ])\n",
					"            \n",
					"            # Convert to rows and create DataFrame\n",
					"            log_rows = [(log[\"timestamp\"], log[\"operation\"], log[\"file_path\"], log[\"status\"]) \n",
					"                        for log in operations_log]\n",
					"            \n",
					"            operations_log_df = spark.createDataFrame(log_rows, schema)\n",
					"            log_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/logs/operations_{time.strftime('%Y%m%d%H%M%S')}.parquet\"\n",
					"            operations_log_df.write.mode(\"overwrite\").parquet(log_path)\n",
					"            print(f\"Saved detailed operations log to: {log_path}\")\n",
					"    except Exception as e:\n",
					"        print(f\"Error saving operations log: {str(e)}\")\n",
					"        print(f\"Traceback: {traceback.format_exc()}\")\n",
					"    \n",
					"    print(\"\\nSummary totals:\")\n",
					"    print(f\"Files processed: {len(new_files)}\")\n",
					"    print(f\"Data files successfully moved: {len(moved_files['data_success'])}\")\n",
					"    print(f\"Metadata files successfully moved: {len(moved_files['metadata_success'])}\")\n",
					"    print(f\"Empty files moved: {len(moved_files.get('empty', []))}\")\n",
					"    print(f\"Files quarantined: {len(moved_files['quarantine'])}\")\n",
					"    print(f\"Files failed: {len(moved_files['failed'])}\")\n",
					"if __name__ == \"__main__\":\n",
					"    main()"
				],
				"execution_count": 40
			}
		]
	}
}