{
	"name": "L0_Er_Juror_Ingestion",
	"properties": {
		"folder": {
			"name": "crime"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkprod",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "63f68183-a328-4ca7-b6b1-e08e23c02df2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5ca62022-6aa2-4cee-aaa7-e7536c8d566c/resourceGroups/baubais-data-factory-rg-prod/providers/Microsoft.Synapse/workspaces/baubais-synapse-prod/bigDataPools/sparkprod",
				"name": "sparkprod",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkprod",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# L0 ER Juror Ingestion Pipeline\n",
					"\n",
					"## Overview\n",
					"This notebook implements an ETL pipeline for processing juror-related files stored in Azure Data Lake Storage (ADLS). It handles various file types (CSV, Excel, ZIP), moves files to appropriate folders (e.g., quarantine, metadata, overseas), and converts files to Parquet format for efficient storage and querying.\n",
					"\n",
					"## Features\n",
					"- **File Type Support**: Processes CSV, Excel, and ZIP files.\n",
					"- **Error Handling**: Gracefully handles errors and moves problematic files to quarantine.\n",
					"- **Distributed Processing**: Uses Apache Spark for scalable file processing.\n",
					"- **Azure Data Lake Integration**: Reads and writes files to ADLS.\n",
					"- **Metadata and Logging**: Adds metadata columns to processed files and maintains detailed logs.\n",
					"\n",
					"## Workflow\n",
					"1. **Initialization**:\n",
					"   - Configures logging and initializes a Spark session.\n",
					"2. **File Identification**:\n",
					"   - Identifies file types (CSV/Excel) and checks for metadata or empty files.\n",
					"3. **File Movement**:\n",
					"   - Moves files to appropriate folders (e.g., quarantine, metadata, overseas).\n",
					"4. **File Processing**:\n",
					"   - Converts CSV/Excel files to Parquet format and merges files by date and LA folder.\n",
					"5. **Folder Scanning**:\n",
					"   - Recursively scans directories for files to process.\n",
					"6. **Logging**:\n",
					"   - Tracks operations and saves logs to ADLS.\n",
					"\n",
					"## Key Functions\n",
					"`is_metadata_file(file_path)`\n",
					"Checks if a file is a metadata file based on its name.\n",
					"\n",
					"`should_process_file(file_path)`\n",
					"Determines if a file should be processed (CSV or Excel).\n",
					"\n",
					"`is_empty_file(file_path)`\n",
					"Checks if a file is empty or contains only a header.\n",
					"\n",
					"`unzip_file(file_path, extract_to, storage_account)`\n",
					"Extracts a ZIP file and uploads its contents to ADLS.\n",
					"\n",
					"`process_file(source_path, date_folder, la_folder, file_name, storage_account)`\n",
					"Processes a CSV or Excel file and converts it to Parquet format.\n",
					"\n",
					"`move_to_quarantine(file_path, storage_account, moved_files, reason=None)`\n",
					"Moves a file to the quarantine folder for manual inspection.\n",
					"\n",
					"`create_file_index(source_path, storage_account)`\n",
					"Creates a JSON index of all files in the source path.\n",
					"\n",
					"`main()`\n",
					"The main function that orchestrates the ETL workflow.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import logging\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"import time\n",
					"from notebookutils import mssparkutils\n",
					"import os\n",
					"import zipfile\n",
					"import shutil\n",
					"import tempfile\n",
					"import re\n",
					"import json  # Add this line\n",
					"from collections import defaultdict\n",
					"import logging\n",
					"import traceback"
				],
				"execution_count": 41
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# Configure logging\n",
					"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"\n",
					"# Initialize Spark session with the required packages\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"JurorIngestion\") \\\n",
					"    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.5,org.apache.xmlbeans:xmlbeans:3.1.0,org.apache.poi:poi-ooxml-schemas:4.1.2,org.apache.poi:poi-ooxml:4.1.2,org.apache.poi:poi:4.1.2\") \\\n",
					"    .getOrCreate()\n",
					"\n",
					"# Global counter to track file sequences by folder structure\n",
					"file_counters = defaultdict(int)\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def is_metadata_file(file_path):\n",
					"    \"\"\"Check if file is metadata related - enhanced version\"\"\"\n",
					"    metadata_indicators = [\n",
					"        'metadata.txt',\n",
					"        'readme.txt',\n",
					"        'readme',\n",
					"        'metadata',\n",
					"        'notes',\n",
					"        'information',\n",
					"        'instructions',\n",
					"        'guide'\n",
					"    ]\n",
					"    \n",
					"    # Case-insensitive check for indicators in filename\n",
					"    file_name_lower = os.path.basename(file_path).lower()\n",
					"    if any(indicator in file_name_lower for indicator in metadata_indicators):\n",
					"        return True\n",
					"        \n",
					"    # Check for files in metadata-like folders\n",
					"    path_lower = file_path.lower()\n",
					"    if 'metadata' in path_lower or 'docs' in path_lower or 'documentation' in path_lower:\n",
					"        return True\n",
					"        \n",
					"    return False"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def should_process_file(file_path):\n",
					"    \"\"\"Check if file should be processed (CSV or Excel) - case insensitive\"\"\"\n",
					"    extensions = ['.csv', '.xlsx', '.xls']\n",
					"    return any(file_path.lower().endswith(ext) for ext in extensions)\n",
					"\n",
					"def get_file_type(file_path):\n",
					"    \"\"\"Determine if file is CSV or Excel\"\"\"\n",
					"    if file_path.lower().endswith(('.xlsx', '.xls')):\n",
					"        return 'excel'\n",
					"    elif file_path.lower().endswith('.csv'):\n",
					"        return 'csv'\n",
					"    return None\n",
					"\n",
					"def is_zip_file(file_path):\n",
					"    \"\"\"Check if file is a zip file\"\"\"\n",
					"    return file_path.lower().endswith('.zip')"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def is_empty_file(file_path):\n",
					"    \"\"\"Check if a file is empty or very small with improved reliability\"\"\"\n",
					"    try:\n",
					"        # First check if file exists\n",
					"        if not mssparkutils.fs.exists(file_path):\n",
					"            print(f\"File doesn't exist, can't check if empty: {file_path}\")\n",
					"            return False\n",
					"            \n",
					"        # For CSV files, use a more direct approach\n",
					"        if file_path.lower().endswith('.csv'):\n",
					"            # Try reading the first few lines directly\n",
					"            try:\n",
					"                text_df = spark.read.text(file_path).limit(5)\n",
					"                count = text_df.count()\n",
					"                \n",
					"                # If no rows or only header\n",
					"                if count == 0:\n",
					"                    return True\n",
					"                if count == 1:\n",
					"                    # Check if it's just a header line\n",
					"                    first_row = text_df.first()[0]\n",
					"                    if len(first_row.strip()) < 10:\n",
					"                        return True\n",
					"                return False\n",
					"            except:\n",
					"                # Fall back to CSV reader with header option\n",
					"                try:\n",
					"                    csv_df = spark.read.option(\"header\", \"true\").csv(file_path).limit(5)\n",
					"                    return csv_df.count() == 0\n",
					"                except:\n",
					"                    # If both approaches fail, assume it's not an empty file\n",
					"                    return False\n",
					"                    \n",
					"        # For Excel files, use pandas\n",
					"        elif file_path.lower().endswith(('.xlsx', '.xls')):\n",
					"            try:\n",
					"                import pandas as pd\n",
					"                import tempfile\n",
					"                import os\n",
					"                \n",
					"                # Create a temporary file\n",
					"                temp_dir = tempfile.mkdtemp()\n",
					"                temp_file = os.path.join(temp_dir, \"temp_excel.xlsx\")\n",
					"                \n",
					"                # Download the file\n",
					"                mssparkutils.fs.cp(file_path, f\"file://{temp_file}\")\n",
					"                \n",
					"                # Read with pandas\n",
					"                excel_df = pd.read_excel(temp_file)\n",
					"                \n",
					"                # Clean up\n",
					"                try:\n",
					"                    os.remove(temp_file)\n",
					"                    os.rmdir(temp_dir)\n",
					"                except:\n",
					"                    pass\n",
					"                \n",
					"                # Check if empty\n",
					"                return len(excel_df) == 0\n",
					"            except:\n",
					"                # If read fails, don't assume it's empty\n",
					"                return False\n",
					"                \n",
					"        # Default approach for other file types\n",
					"        try:\n",
					"            # Use a basic text read as fallback\n",
					"            test_df = spark.read.text(file_path).limit(5)\n",
					"            return test_df.count() == 0\n",
					"        except Exception as e:\n",
					"            print(f\"Error checking if file is empty: {str(e)}\")\n",
					"            return False\n",
					"    except Exception as e:\n",
					"        print(f\"Error in is_empty_file check: {str(e)}\")\n",
					"        # Default to false if we can't determine\n",
					"        return False"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def unzip_file(file_path, extract_to, storage_account):\n",
					"    \"\"\"Unzip a file to the specified directory with improved error handling\"\"\"\n",
					"    try:\n",
					"        print(f\"Attempting to unzip: {file_path}\")\n",
					"        \n",
					"        # Create a temporary local directory\n",
					"        with tempfile.TemporaryDirectory() as tmpdirname:\n",
					"            local_zip_path = os.path.join(tmpdirname, os.path.basename(file_path))\n",
					"            \n",
					"            # Download the zip file to the local temporary directory\n",
					"            print(f\"Downloading zip file from {file_path} to {local_zip_path}\")\n",
					"            mssparkutils.fs.cp(file_path, f\"file://{local_zip_path}\")\n",
					"            \n",
					"            # Check if zip file is password protected before trying to extract\n",
					"            try:\n",
					"                zip_file = zipfile.ZipFile(local_zip_path)\n",
					"                # Try to read the first file to see if it's password protected\n",
					"                for zip_info in zip_file.infolist():\n",
					"                    if zip_info.flag_bits & 0x1:\n",
					"                        print(f\"ZIP file is encrypted/password protected: {file_path}\")\n",
					"                        return False, \"password_protected\"\n",
					"                    \n",
					"                    # Try to read a little bit of data to check for password\n",
					"                    try:\n",
					"                        zip_file.open(zip_info.filename).read(1)\n",
					"                    except RuntimeError as e:\n",
					"                        if 'password required' in str(e).lower() or 'bad password' in str(e).lower():\n",
					"                            print(f\"Password required for ZIP file: {file_path}\")\n",
					"                            return False, \"password_protected\"\n",
					"                        raise\n",
					"                \n",
					"                # If we got here, try to extract\n",
					"                print(f\"Extracting ZIP file to {extract_to}\")\n",
					"                zip_file.extractall(path=extract_to)\n",
					"                zip_file.close()\n",
					"                \n",
					"            except zipfile.BadZipFile as e:\n",
					"                print(f\"Bad ZIP file: {file_path} - {str(e)}\")\n",
					"                return False, \"bad_zip_file\"\n",
					"            except RuntimeError as e:\n",
					"                error_msg = str(e).lower()\n",
					"                if 'password required' in error_msg or 'encrypted' in error_msg or 'bad password' in error_msg:\n",
					"                    print(f\"Password protected ZIP file detected: {file_path}\")\n",
					"                    return False, \"password_protected\"\n",
					"                raise\n",
					"            \n",
					"        # If extraction succeeded, upload the extracted files back to ADLS\n",
					"        print(f\"Uploading extracted files to Azure Data Lake\")\n",
					"        for root, dirs, files in os.walk(extract_to):\n",
					"            for file in files:\n",
					"                local_file_path = os.path.join(root, file)\n",
					"                relative_path = os.path.relpath(local_file_path, extract_to)\n",
					"                remote_dir = os.path.dirname(file_path)\n",
					"                remote_file_path = f\"{remote_dir}/{relative_path}\"\n",
					"                \n",
					"                print(f\"Uploading: {local_file_path} to {remote_file_path}\")\n",
					"                mssparkutils.fs.cp(f\"file://{local_file_path}\", remote_file_path)\n",
					"        \n",
					"        print(f\"Successfully unzipped file: {file_path}\")\n",
					"        return True, None\n",
					"        \n",
					"    except RuntimeError as e:\n",
					"        error_msg = str(e).lower()\n",
					"        if 'password required' in error_msg or 'encrypted' in error_msg or 'bad password' in error_msg:\n",
					"            print(f\"Password protected ZIP file detected: {file_path}\")\n",
					"            return False, \"password_protected\"\n",
					"        else:\n",
					"            print(f\"Runtime error unzipping file: {str(e)}\")\n",
					"            return False, str(e)\n",
					"    except Exception as e:\n",
					"        print(f\"Error unzipping file: {file_path} - {str(e)}\")\n",
					"        return False, str(e)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def check_for_overseas_columns(file_path, file_type):\n",
					"    \"\"\"Check if file contains overseas address columns and move to overseas container if found\"\"\"\n",
					"    try:\n",
					"        print(f\"  Checking for overseas address columns in: {file_path}\")\n",
					"        \n",
					"        # Read only the header row to check columns\n",
					"        if file_type == 'csv':\n",
					"            # For CSV files\n",
					"            df = spark.read.option(\"header\", \"true\").csv(file_path).limit(0)\n",
					"        elif file_type == 'excel':\n",
					"            # For Excel files\n",
					"            df = spark.read.format(\"com.crealytics.spark.excel\").option(\"header\", \"true\").load(file_path).limit(0)\n",
					"        else:\n",
					"            print(f\"  Unsupported file type for overseas check: {file_type}\")\n",
					"            return False, None\n",
					"            \n",
					"        # Get column names and check if overseas columns exist\n",
					"        columns = df.columns\n",
					"        has_overseas_columns = any(col in columns for col in ['Overseas Address','OverseasAddress1','QualifyingAddress1','UK Qualifying Address'])\n",
					"        \n",
					"        if has_overseas_columns:\n",
					"            print(f\"  Found overseas address columns in file: {file_path}\")\n",
					"            return True, columns\n",
					"        else:\n",
					"            print(f\"  No overseas address columns found in file: {file_path}\")\n",
					"            return False, None\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"  Error checking for overseas columns: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"  Traceback: {traceback.format_exc()}\")\n",
					"        return False, None\n",
					"\n",
					"def move_to_overseas(file_path, storage_account, moved_files):\n",
					"    \"\"\"Move a file to the overseas folder\"\"\"\n",
					"    try:\n",
					"        # Make sure overseas directory exists\n",
					"        overseas_dir = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/overseas\"\n",
					"        if not mssparkutils.fs.exists(overseas_dir):\n",
					"            print(f\"Creating overseas directory: {overseas_dir}\")\n",
					"            mssparkutils.fs.mkdirs(overseas_dir)\n",
					"        \n",
					"        # Create a unique filename to avoid conflicts\n",
					"        file_name = os.path.basename(file_path)\n",
					"        timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
					"        overseas_path = f\"{overseas_dir}/{timestamp}_{file_name}\"\n",
					"        \n",
					"        print(f\"Moving file to overseas: {file_path} -> {overseas_path}\")\n",
					"        \n",
					"        # Copy file to overseas directory\n",
					"        mssparkutils.fs.cp(file_path, overseas_path, True)\n",
					"        print(f\"Successfully copied file to overseas container\")\n",
					"        \n",
					"        # Update tracking dictionary (add 'overseas' category if it doesn't exist)\n",
					"        if 'overseas' not in moved_files:\n",
					"            moved_files['overseas'] = []\n",
					"        moved_files['overseas'].append(overseas_path)\n",
					"        \n",
					"        return True, overseas_path\n",
					"    except Exception as e:\n",
					"        print(f\"Error moving file to overseas container {file_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"Traceback: {traceback.format_exc()}\")\n",
					"        return False, None"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def move_to_quarantine(file_path, storage_account, moved_files, reason=None):\n",
					"    \"\"\"Enhanced function to move files to quarantine with date folder prefix instead of timestamp\"\"\"\n",
					"    try:\n",
					"        # Check if this file was already moved to quarantine\n",
					"        for existing_path in moved_files['quarantine']:\n",
					"            if os.path.basename(existing_path).endswith(os.path.basename(file_path)):\n",
					"                print(f\"File already in quarantine: {file_path}\")\n",
					"                return True, existing_path\n",
					"        \n",
					"        # Make sure quarantine directory exists\n",
					"        quarantine_dir = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/quarantine\"\n",
					"        if not mssparkutils.fs.exists(quarantine_dir):\n",
					"            print(f\"Creating quarantine directory: {quarantine_dir}\")\n",
					"            mssparkutils.fs.mkdirs(quarantine_dir)\n",
					"        \n",
					"        # Extract date_folder from the path\n",
					"        parts = file_path.split('/')\n",
					"        \n",
					"        # Try to find a date-formatted folder (8 digits for YYYYMMDD)\n",
					"        date_folder = None\n",
					"        for part in parts:\n",
					"            if re.match(r'^\\d{8}$', part):\n",
					"                date_folder = part\n",
					"                break\n",
					"        \n",
					"        # If not found, use current date\n",
					"        if not date_folder:\n",
					"            date_folder = time.strftime(\"%Y%m%d\")\n",
					"        \n",
					"        # Create a filename using date_folder as prefix\n",
					"        file_name = os.path.basename(file_path)\n",
					"        reason_tag = f\"_{reason}\" if reason else \"\"\n",
					"        quarantine_path = f\"{quarantine_dir}/{date_folder}{reason_tag}_{file_name}\"\n",
					"        \n",
					"        # Copy to quarantine\n",
					"        print(f\"Moving to quarantine: {file_path} -> {quarantine_path}\")\n",
					"        mssparkutils.fs.cp(file_path, quarantine_path, True)\n",
					"        \n",
					"        # Verify file was copied\n",
					"        if mssparkutils.fs.exists(quarantine_path):\n",
					"            print(f\"Successfully moved to quarantine\")\n",
					"            moved_files['quarantine'].append(quarantine_path)\n",
					"            return True, quarantine_path\n",
					"        else:\n",
					"            print(f\"Failed to move to quarantine\")\n",
					"            return False, None\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"Error in quarantine processing: {str(e)}\")\n",
					"        return False, None"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def process_metadata_file(file_path, file_name, storage_account):\n",
					"    \"\"\"Process metadata file\"\"\"\n",
					"    try:\n",
					"        # Debug info\n",
					"        print(f\"  process_metadata_file received: path={file_path}, name={file_name}\")\n",
					"        # Make sure metadata directory exists\n",
					"        metadata_dir = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/metadata\"\n",
					"        if not mssparkutils.fs.exists(metadata_dir):\n",
					"            print(f\"Creating metadata directory: {metadata_dir}\")\n",
					"            mssparkutils.fs.mkdirs(metadata_dir)\n",
					"        \n",
					"        # Create a unique name by including part of the original file path\n",
					"        unique_name = file_path.replace(f\"abfss://juror-ingestion@{storage_account}.dfs.core.windows.net/\", \"\").replace(\"/\", \"_\")\n",
					"        destination_path = f\"{metadata_dir}/{unique_name}\"\n",
					"        \n",
					"        print(f\"Moving metadata file: {file_path} -> {destination_path}\")\n",
					"        \n",
					"        # Copy file to metadata directory\n",
					"        mssparkutils.fs.cp(file_path, destination_path, True)\n",
					"        print(f\"Successfully copied metadata file\")\n",
					"        \n",
					"        print(f\"Moved metadata file to {destination_path}\")\n",
					"        return True, destination_path\n",
					"    except Exception as e:\n",
					"        print(f\"Error moving metadata file {file_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"Traceback: {traceback.format_exc()}\")\n",
					"        return False, None"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def process_empty_file(file_path, storage_account, moved_files):\n",
					"    \"\"\"Move an empty file to the empty folder\"\"\"\n",
					"    try:\n",
					"        # Make sure empty directory exists\n",
					"        empty_dir = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/empty\"\n",
					"        if not mssparkutils.fs.exists(empty_dir):\n",
					"            print(f\"Creating empty files directory: {empty_dir}\")\n",
					"            mssparkutils.fs.mkdirs(empty_dir)\n",
					"        \n",
					"        # Create a unique filename\n",
					"        file_name = os.path.basename(file_path)\n",
					"        timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
					"        empty_path = f\"{empty_dir}/{timestamp}_{file_name}\"\n",
					"        \n",
					"        print(f\"Moving empty file: {file_path} -> {empty_path}\")\n",
					"        \n",
					"        # Copy file to empty directory\n",
					"        mssparkutils.fs.cp(file_path, empty_path, True)\n",
					"        print(f\"Successfully copied empty file\")\n",
					"        \n",
					"        # Update tracking dictionary (add 'empty' category if it doesn't exist)\n",
					"        if 'empty' not in moved_files:\n",
					"            moved_files['empty'] = []\n",
					"        moved_files['empty'].append(empty_path)\n",
					"        \n",
					"        return True, empty_path\n",
					"    except Exception as e:\n",
					"        print(f\"Error moving empty file {file_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"Traceback: {traceback.format_exc()}\")\n",
					"        return False, None"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def explicit_file_copy(source_path, dest_path, storage_account):\n",
					"    \"\"\"Explicitly copy a file with better error handling and verification but no deletion\"\"\"\n",
					"    try:\n",
					"        # Debug info\n",
					"        print(f\"  Copying {source_path} to {dest_path}\")\n",
					"        \n",
					"        # Ensure destination directory exists\n",
					"        dest_dir = os.path.dirname(dest_path)\n",
					"        if not mssparkutils.fs.exists(dest_dir):\n",
					"            print(f\"  Creating destination directory: {dest_dir}\")\n",
					"            mssparkutils.fs.mkdirs(dest_dir)\n",
					"        \n",
					"        # Check if source exists\n",
					"        if not mssparkutils.fs.exists(source_path):\n",
					"            print(f\"  Source file does not exist: {source_path}\")\n",
					"            return False\n",
					"        \n",
					"        # Execute the copy\n",
					"        mssparkutils.fs.cp(source_path, dest_path, True)\n",
					"        \n",
					"        # Verify the file was copied\n",
					"        if mssparkutils.fs.exists(dest_path):\n",
					"            print(f\"  Verified file copied successfully\")\n",
					"            \n",
					"            # REMOVED THE DELETION CODE HERE\n",
					"            \n",
					"            return True\n",
					"        else:\n",
					"            print(f\"  ERROR: Copy failed, destination file not found: {dest_path}\")\n",
					"            return False\n",
					"    except Exception as e:\n",
					"        print(f\"  Error during file copy: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"  Traceback: {traceback.format_exc()}\")\n",
					"        return False"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def process_file(source_path, date_folder, la_folder, file_name, storage_account):\n",
					"    \"\"\"Process CSV or Excel file and convert to Parquet format\"\"\"\n",
					"    try:\n",
					"        # Create the folder name in the format YYYYMMDD_LA-Name\n",
					"        folder_key = f\"{date_folder}_{la_folder}\"\n",
					"        file_type = get_file_type(source_path)\n",
					"        \n",
					"        if not file_type:\n",
					"            print(f\"  Cannot determine file type for {source_path}\")\n",
					"            return False, None\n",
					"        \n",
					"        # Destination path for Parquet file (same regardless of source type)\n",
					"        dest_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/files/{folder_key}.parquet\"\n",
					"        \n",
					"        # Read the source file using different methods based on file type\n",
					"        try:\n",
					"            if file_type == 'excel':\n",
					"                print(f\"  Reading Excel file: {source_path}\")\n",
					"                \n",
					"                # Use pandas to read Excel\n",
					"                import pandas as pd\n",
					"                import tempfile\n",
					"                import os\n",
					"                \n",
					"                # Create a temporary file\n",
					"                temp_dir = tempfile.mkdtemp()\n",
					"                temp_file = os.path.join(temp_dir, \"temp_excel.xlsx\")\n",
					"                \n",
					"                # Download the file\n",
					"                mssparkutils.fs.cp(source_path, f\"file://{temp_file}\")\n",
					"                \n",
					"                # Read with pandas\n",
					"                excel_df = pd.read_excel(temp_file)\n",
					"                \n",
					"                # Convert to Spark DataFrame\n",
					"                df = spark.createDataFrame(excel_df)\n",
					"                \n",
					"                # Clean up\n",
					"                try:\n",
					"                    os.remove(temp_file)\n",
					"                    os.rmdir(temp_dir)\n",
					"                except:\n",
					"                    pass\n",
					"                \n",
					"            else:  # csv\n",
					"                print(f\"  Reading CSV file: {source_path}\")\n",
					"                df = spark.read \\\n",
					"                    .option(\"header\", \"true\") \\\n",
					"                    .option(\"inferSchema\", \"true\") \\\n",
					"                    .csv(source_path)\n",
					"            \n",
					"            # Add metadata columns for tracking\n",
					"            df = df.withColumn(\"source_file\", lit(source_path))\n",
					"            df = df.withColumn(\"source_type\", lit(file_type))\n",
					"            df = df.withColumn(\"ingestion_date\", lit(current_timestamp()))\n",
					"            df = df.withColumn(\"date_folder\", lit(date_folder))\n",
					"            df = df.withColumn(\"la_folder\", lit(la_folder))\n",
					"            \n",
					"            # Write to Parquet format\n",
					"            print(f\"  Writing to Parquet: {dest_path}\")\n",
					"            df.write.mode(\"overwrite\").parquet(dest_path)\n",
					"            \n",
					"            # Verify the Parquet file was created\n",
					"            if mssparkutils.fs.exists(dest_path):\n",
					"                print(f\"  Successfully converted to Parquet at: {dest_path}\")\n",
					"                return True, f\"parquet/{folder_key}.parquet\"\n",
					"            else:\n",
					"                print(f\"  ERROR: Parquet file not created at: {dest_path}\")\n",
					"                return False, None\n",
					"                \n",
					"        except Exception as read_error:\n",
					"            print(f\"  ERROR reading or converting file: {str(read_error)}\")\n",
					"            import traceback\n",
					"            print(f\"  Traceback: {traceback.format_exc()}\")\n",
					"            \n",
					"            # Fallback to simple copy without conversion if there's an error\n",
					"            print(f\"  Falling back to direct file copy without conversion\")\n",
					"            if file_type == 'excel':\n",
					"                fallback_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/files/{folder_key}.xlsx\"\n",
					"            else:  # csv\n",
					"                fallback_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/files/{folder_key}.csv\"\n",
					"            \n",
					"            mssparkutils.fs.cp(source_path, fallback_path, True)\n",
					"            \n",
					"            if mssparkutils.fs.exists(fallback_path):\n",
					"                print(f\"  Successfully copied file using fallback method\")\n",
					"                return True, f\"{file_type}/{folder_key}.{file_type}\"\n",
					"            else:\n",
					"                print(f\"  ERROR: Fallback copy failed\")\n",
					"                return False, None\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"  ERROR in process_file: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"  Traceback: {traceback.format_exc()}\")\n",
					"        return False, None"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def find_la_data_in_path(file_path):\n",
					"    \"\"\"Find LA_Data in path and extract date and LA folders with improved detection\"\"\"\n",
					"    parts = file_path.split('/')\n",
					"    print(f\"Path parts: {parts}\")\n",
					"    \n",
					"    # Try to find LA_Data in the path\n",
					"    la_index = -1\n",
					"    for i, part in enumerate(parts):\n",
					"        if part.lower() == 'la_data':\n",
					"            la_index = i\n",
					"            break\n",
					"    \n",
					"    if la_index == -1:\n",
					"        # Try more flexible matching (in case of case differences or formatting issues)\n",
					"        for i, part in enumerate(parts):\n",
					"            if 'la' in part.lower() and 'data' in part.lower():\n",
					"                print(f\"Found likely LA_Data folder with name: {part}\")\n",
					"                la_index = i\n",
					"                break\n",
					"    \n",
					"    if la_index == -1:\n",
					"        print(\"Could not find LA_Data or similar folder in path\")\n",
					"        return None, None\n",
					"        \n",
					"    print(f\"Found LA_Data at index {la_index}\")\n",
					"    \n",
					"    # Get date folder (before LA_Data)\n",
					"    date_folder = None\n",
					"    if la_index > 0:\n",
					"        date_folder = parts[la_index - 1]\n",
					"        print(f\"Date folder: {date_folder}\")\n",
					"    \n",
					"    # Get LA folder (after LA_Data)\n",
					"    la_folder = None\n",
					"    if len(parts) > la_index + 1:\n",
					"        la_folder = parts[la_index + 1]\n",
					"        print(f\"LA folder: {la_folder}\")\n",
					"    \n",
					"    return date_folder, la_folder"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def get_folder_parts(file_path):\n",
					"    \"\"\"Extract folder information with quarantine flagging for problematic patterns\"\"\"\n",
					"    parts = file_path.split('/')\n",
					"    print(f\"Path parts: {parts}\")\n",
					"    \n",
					"    # Initialize with defaults\n",
					"    date_folder = None\n",
					"    la_folder = None\n",
					"    confidence = \"low\"\n",
					"    \n",
					"    # Check for standard pattern with LA_Data\n",
					"    la_index = -1\n",
					"    for i, part in enumerate(parts):\n",
					"        if part.lower() == 'la_data':\n",
					"            la_index = i\n",
					"            break\n",
					"    \n",
					"    if la_index != -1:\n",
					"        # Get date folder (before LA_Data)\n",
					"        if la_index > 0:\n",
					"            date_folder = parts[la_index - 1]\n",
					"        \n",
					"        # Get LA folder (after LA_Data)\n",
					"        if len(parts) > la_index + 1:\n",
					"            la_folder = parts[la_index + 1]\n",
					"        \n",
					"        confidence = \"high\" if date_folder and la_folder else \"low\"\n",
					"    \n",
					"    # Check for problematic pattern like \"20231205/NT - White Ladies Aston.XLSX\"\n",
					"    filename = os.path.basename(file_path)\n",
					"    if not la_folder and \"-\" in filename:\n",
					"        # This is a non-standard pattern - flag for quarantine\n",
					"        confidence = \"very_low\"\n",
					"        \n",
					"        # Still extract something usable for the quarantine filename\n",
					"        date_pattern = re.findall(r'\\d{8}', '/'.join(parts))\n",
					"        if date_pattern:\n",
					"            date_folder = date_pattern[0]\n",
					"        else:\n",
					"            date_folder = time.strftime(\"%Y%m%d\")\n",
					"        \n",
					"        # Extract a clean identifier from the filename\n",
					"        name_part = os.path.splitext(filename)[0]\n",
					"        clean_name = re.sub(r'[^\\w\\-]', '_', name_part)\n",
					"        la_folder = f\"unknown_{clean_name}\"\n",
					"    \n",
					"    # Final fallbacks\n",
					"    if not date_folder:\n",
					"        date_folder = time.strftime(\"%Y%m%d\")\n",
					"        confidence = \"very_low\"\n",
					"    \n",
					"    if not la_folder:\n",
					"        name_part = os.path.splitext(filename)[0]\n",
					"        clean_name = re.sub(r'[^\\w\\-]', '_', name_part)\n",
					"        la_folder = f\"unknown_{clean_name}\"\n",
					"        confidence = \"very_low\"\n",
					"    \n",
					"    print(f\"Extracted: date_folder={date_folder}, la_folder={la_folder}, confidence={confidence}\")\n",
					"    return date_folder, la_folder, confidence"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def process_folder(path, storage_account, moved_files, depth=0):\n",
					"    \"\"\"Process a folder recursively and return all files with improved ZIP handling\"\"\"\n",
					"    all_files = []\n",
					"    \n",
					"    # Add depth indicator for debugging\n",
					"    indent = \"  \" * depth\n",
					"    try:\n",
					"        print(f\"{indent}Scanning directory (depth {depth}): {path}\")\n",
					"        files = mssparkutils.fs.ls(path)\n",
					"        print(f\"{indent}Found {len(files)} items in {path}\")\n",
					"        \n",
					"        for file_info in files:\n",
					"            if file_info.isDir:\n",
					"                # Process subdirectories recursively\n",
					"                sub_files = process_folder(file_info.path, storage_account, moved_files, depth + 1)\n",
					"                all_files.extend(sub_files)\n",
					"                print(f\"{indent}Found {len(sub_files)} files in subdirectory {file_info.path}\")\n",
					"            else:\n",
					"                # Check for ZIP files explicitly\n",
					"                if file_info.path.lower().endswith('.zip'):\n",
					"                    print(f\"{indent}Found ZIP file: {file_info.path}\")\n",
					"                    \n",
					"                    # Always quarantine ZIP files\n",
					"                    print(f\"{indent}Moving ZIP file to quarantine: {file_info.path}\")\n",
					"                    \n",
					"                    # Debug to verify file exists\n",
					"                    print(f\"{indent}DEBUG: ZIP file exists: {mssparkutils.fs.exists(file_info.path)}\")\n",
					"                    \n",
					"                    # Move ZIP to quarantine with explicit reason\n",
					"                    success, quarantine_path = move_to_quarantine(file_info.path, storage_account, moved_files, \"zip_file\")\n",
					"                    \n",
					"                    if success:\n",
					"                        print(f\"{indent}Successfully moved ZIP to quarantine: {quarantine_path}\")\n",
					"                    else:\n",
					"                        print(f\"{indent}Failed to move ZIP to quarantine, will attempt to process contents: {file_info.path}\")\n",
					"                        # Add file to list only if quarantine fails\n",
					"                        all_files.append(file_info.path)\n",
					"                \n",
					"                # Add all non-ZIP files to process normally\n",
					"                elif 'File Structure Definitions.xlsx' not in file_info.path:\n",
					"                    print(f\"{indent}Found file: {file_info.path}\")\n",
					"                    all_files.append(file_info.path)\n",
					"    except Exception as e:\n",
					"        print(f\"{indent}Error accessing path {path}: {str(e)}\")\n",
					"        print(f\"{indent}Traceback: {traceback.format_exc()}\")\n",
					"    \n",
					"    return all_files"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def create_file_index(source_path, storage_account):\n",
					"    \"\"\"Create a JSON index of all files in the source path\"\"\"\n",
					"    print(f\"Creating index of files in {source_path}\")\n",
					"    \n",
					"    file_index = {\n",
					"        \"created_at\": time.strftime(\"%Y%m%d%H%M%S\"),\n",
					"        \"files\": []\n",
					"    }\n",
					"    \n",
					"    # Process the folder recursively - simplified version that doesn't process ZIP files\n",
					"    def process_folder_for_index(path, depth=0):\n",
					"        indent = \"  \" * depth\n",
					"        try:\n",
					"            print(f\"{indent}Scanning directory (depth {depth}): {path}\")\n",
					"            files = mssparkutils.fs.ls(path)\n",
					"            \n",
					"            for file_info in files:\n",
					"                if file_info.isDir:\n",
					"                    process_folder_for_index(file_info.path, depth + 1)\n",
					"                else:\n",
					"                    print(f\"{indent}Found file: {file_info.path}\")\n",
					"                    \n",
					"                    # Extract date_folder and la_folder\n",
					"                    date_folder, la_folder, confidence = get_folder_parts(file_info.path)\n",
					"                    \n",
					"                    # Add file to index\n",
					"                    file_index[\"files\"].append({\n",
					"                        \"path\": file_info.path,\n",
					"                        \"file_name\": os.path.basename(file_info.path),\n",
					"                        \"date_folder\": date_folder,\n",
					"                        \"la_folder\": la_folder,\n",
					"                        \"confidence\": confidence,\n",
					"                        \"file_type\": get_file_type(file_info.path),\n",
					"                        \"is_empty\": is_empty_file(file_info.path),\n",
					"                        \"is_metadata\": is_metadata_file(file_info.path),\n",
					"                        \"should_process\": should_process_file(file_info.path) and 'File Structure Definitions.xlsx' not in file_info.path,\n",
					"                        \"processed\": False\n",
					"                    })\n",
					"        except Exception as e:\n",
					"            print(f\"{indent}Error accessing path {path}: {str(e)}\")\n",
					"    \n",
					"    # Start the recursive process\n",
					"    process_folder_for_index(source_path)\n",
					"    \n",
					"    # Write the index to a JSON file\n",
					"    # Write the index to a JSON file\n",
					"    index_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/index/file_index.json\"\n",
					"    \n",
					"    # Ensure the index directory exists\n",
					"    index_dir = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/index\"\n",
					"    if not mssparkutils.fs.exists(index_dir):\n",
					"        print(f\"Creating index directory: {index_dir}\")\n",
					"        mssparkutils.fs.mkdirs(index_dir)\n",
					"    \n",
					"    # Convert to a proper DataFrame using a list of Row objects\n",
					"    from pyspark.sql import Row\n",
					"    index_rows = []\n",
					"    \n",
					"    # Add each file as a separate row in the DataFrame\n",
					"    for file_data in file_index[\"files\"]:\n",
					"        # Convert the individual file data to a Row\n",
					"        file_row = Row(\n",
					"            path=file_data[\"path\"],\n",
					"            file_name=file_data[\"file_name\"],\n",
					"            date_folder=file_data[\"date_folder\"],\n",
					"            la_folder=file_data[\"la_folder\"],\n",
					"            confidence=file_data[\"confidence\"],\n",
					"            file_type=file_data[\"file_type\"],\n",
					"            is_empty=file_data[\"is_empty\"],\n",
					"            is_metadata=file_data[\"is_metadata\"],\n",
					"            should_process=file_data[\"should_process\"],\n",
					"            processed=file_data[\"processed\"]\n",
					"        )\n",
					"        index_rows.append(file_row)\n",
					"    \n",
					"    # Create DataFrame from the rows\n",
					"    index_df = spark.createDataFrame(index_rows)\n",
					"    \n",
					"    # Write the DataFrame as a Parquet file (more robust than text)\n",
					"    index_df.write.mode(\"overwrite\").parquet(index_path)\n",
					"    \n",
					"    print(f\"Created index with {len(file_index['files'])} files at {index_path}\")\n",
					"    return file_index"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def process_individual_file(file_info, storage_account, moved_files):\n",
					"    \"\"\"Process a single file based on its index information\"\"\"\n",
					"    try:\n",
					"        # Extract info from the file_info dictionary\n",
					"        file_path = file_info[\"path\"]\n",
					"        file_name = file_info[\"file_name\"]  \n",
					"        date_folder = file_info[\"date_folder\"]\n",
					"        la_folder = file_info[\"la_folder\"]\n",
					"        confidence = file_info[\"confidence\"]\n",
					"        file_type = file_info[\"file_type\"]  # Get file type from info\n",
					"        \n",
					"        print(f\"\\nProcessing file: {file_path}\")\n",
					"        \n",
					"        # Check if file still exists\n",
					"        if not mssparkutils.fs.exists(file_path):\n",
					"            print(f\"  File no longer exists, might have been processed already: {file_path}\")\n",
					"            return False\n",
					"        \n",
					"        # Check for problematic \"NT - Location\" pattern - move directly to quarantine\n",
					"        if \"-\" in file_name and not (file_name.startswith(\"unknown_\") or \"la_data\" in file_path.lower()):\n",
					"            # This might be one of the problematic patterns\n",
					"            quarantine_reason = \"non_standard_la_code\"\n",
					"            print(f\"  Moving file to quarantine due to: {quarantine_reason}\")\n",
					"            success, quarantine_path = move_to_quarantine(file_path, storage_account, moved_files, quarantine_reason)\n",
					"            if success:\n",
					"                print(f\"  Successfully moved to quarantine: {quarantine_path}\")\n",
					"                return True\n",
					"            else:\n",
					"                print(f\"  Failed to move to quarantine\")\n",
					"                moved_files['failed'].append(f\"{file_path} (quarantine failed - {quarantine_reason})\")\n",
					"                return False\n",
					"        \n",
					"        # Check if file is empty\n",
					"        if file_info[\"is_empty\"]:\n",
					"            print(f\"  File is empty, moving to empty files folder\")\n",
					"            success, empty_path = process_empty_file(file_path, storage_account, moved_files)\n",
					"            if success:\n",
					"                print(f\"  Successfully processed empty file to: {empty_path}\")\n",
					"                return True\n",
					"            else:\n",
					"                moved_files['failed'].append(f\"{file_path} (empty file move failed)\")\n",
					"                print(f\"  Failed to process empty file\")\n",
					"                return False\n",
					"        \n",
					"        # Check if it's a metadata file\n",
					"        if file_info[\"is_metadata\"]:\n",
					"            print(f\"  Identified as metadata file\")\n",
					"            success, new_path = process_metadata_file(file_path, file_name, storage_account)\n",
					"            if success:\n",
					"                moved_files['metadata_success'].append(new_path)\n",
					"                print(f\"  Successfully processed metadata file to: {new_path}\")\n",
					"                return True\n",
					"            else:\n",
					"                moved_files['failed'].append(f\"{file_path} (metadata move failed)\")\n",
					"                print(f\"  Failed to process metadata file\")\n",
					"                return False\n",
					"        \n",
					"        # Check if it's a supported file type\n",
					"        if not file_info[\"should_process\"]:\n",
					"            print(f\"  Skipping unsupported file format: {file_path}\")\n",
					"            return False\n",
					"        else:\n",
					"            print(f\"  Supported file format: {file_info['file_type']}\")\n",
					"            \n",
					"        # NEW STEP: Check for overseas address columns\n",
					"        if file_type in ('csv', 'excel'):\n",
					"            has_overseas, columns = check_for_overseas_columns(file_path, file_type)\n",
					"            if has_overseas:\n",
					"                print(f\"  File contains overseas columns, moving to overseas container\")\n",
					"                success, overseas_path = move_to_overseas(file_path, storage_account, moved_files)\n",
					"                if success:\n",
					"                    print(f\"  Successfully moved to overseas container: {overseas_path}\")\n",
					"                    return True\n",
					"                else:\n",
					"                    print(f\"  Failed to move to overseas container\")\n",
					"                    moved_files['failed'].append(f\"{file_path} (overseas move failed)\")\n",
					"                    return False\n",
					"        \n",
					"        # Check if we should move to quarantine for manual inspection\n",
					"        should_quarantine = False\n",
					"        quarantine_reason = \"\"\n",
					"        \n",
					"        # Improved quarantine conditions:\n",
					"        if not date_folder or not la_folder:\n",
					"            should_quarantine = True\n",
					"            quarantine_reason = \"missing_folder_parts\"\n",
					"        elif la_folder and \"la_data\" in la_folder.lower():\n",
					"            # Quarantine if LA folder is just \"LA_Data\"\n",
					"            should_quarantine = True\n",
					"            quarantine_reason = \"invalid_la_code_la_data\"\n",
					"        elif confidence == \"very_low\":\n",
					"            should_quarantine = True\n",
					"            quarantine_reason = f\"invalid_la_code_format_{la_folder}\"\n",
					"        elif confidence == \"low\" and not re.match(r'^\\\\d{8}$', date_folder):\n",
					"            should_quarantine = True\n",
					"            quarantine_reason = f\"low_confidence_structure_{date_folder}_{la_folder}\"\n",
					"        \n",
					"        if should_quarantine:\n",
					"            print(f\"  Moving file to quarantine due to: {quarantine_reason}\")\n",
					"            success, quarantine_path = move_to_quarantine(file_path, storage_account, moved_files, quarantine_reason)\n",
					"            if success:\n",
					"                print(f\"  Successfully moved to quarantine: {quarantine_path}\")\n",
					"                return True\n",
					"            else:\n",
					"                print(f\"  Failed to move to quarantine\")\n",
					"                moved_files['failed'].append(f\"{file_path} (quarantine failed - {quarantine_reason})\")\n",
					"                return False\n",
					"        \n",
					"        # Process the file if it passed validation\n",
					"        print(f\"  Valid folder structure, processing file...\")\n",
					"        success, new_path = process_file(file_path, date_folder, la_folder, file_name, storage_account)\n",
					"        if success:\n",
					"            moved_files['data_success'].append(new_path)\n",
					"            print(f\"  Successfully processed to: {new_path}\")\n",
					"            return True\n",
					"        else:\n",
					"            moved_files['failed'].append(f\"{file_info['path']} (processing failed)\")\n",
					"            print(f\"  Failed to process file\")\n",
					"            return False\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error processing {file_path}: {str(e)}\")\n",
					"        print(f\"  Error processing {file_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"  Traceback: {traceback.format_exc()}\")\n",
					"        moved_files['failed'].append(f\"{file_path} (processing error)\")\n",
					"        return False"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def process_file_group(file_group, date_folder, la_folder, storage_account, moved_files):\n",
					"    # Modified function to ensure proper file copying\n",
					"    try:\n",
					"        print(f\"\\nProcessing file group: {len(file_group)} files for {date_folder}_{la_folder}\")\n",
					"        \n",
					"        # Ensure destination directory exists\n",
					"        dest_dir = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/files\"\n",
					"        if not mssparkutils.fs.exists(dest_dir):\n",
					"            print(f\"Creating destination directory: {dest_dir}\")\n",
					"            mssparkutils.fs.mkdirs(dest_dir)\n",
					"        \n",
					"        successful_files = 0\n",
					"        \n",
					"        # Process each file individually\n",
					"        for idx, info in enumerate(file_group):\n",
					"            file_path = info[\"path\"]\n",
					"            file_name = info[\"file_name\"]\n",
					"            file_type = info[\"file_type\"]\n",
					"            \n",
					"            # Skip if file doesn't exist\n",
					"            if not mssparkutils.fs.exists(file_path):\n",
					"                print(f\"  File doesn't exist, skipping: {file_path}\")\n",
					"                continue\n",
					"            \n",
					"            # Create a unique destination filename \n",
					"            unique_suffix = f\"_{idx}\" if idx > 0 else \"\"\n",
					"            unique_folder_key = f\"{date_folder}_{la_folder}{unique_suffix}\"\n",
					"            \n",
					"            # Use Parquet as the destination format for consistency\n",
					"            dest_path = f\"{dest_dir}/{unique_folder_key}.parquet\"\n",
					"            \n",
					"            try:\n",
					"                # Process the file to Parquet using the process_file function\n",
					"                success, new_path = process_file(file_path, date_folder, la_folder, file_name, storage_account)\n",
					"                \n",
					"                if success:\n",
					"                    print(f\"  Successfully processed file to: {new_path}\")\n",
					"                    moved_files['data_success'].append(new_path)\n",
					"                    successful_files += 1\n",
					"                else:\n",
					"                    # Fallback to direct copy if processing fails\n",
					"                    print(f\"  Processing failed, falling back to direct copy\")\n",
					"                    fallback_path = f\"{dest_dir}/{unique_folder_key}.{file_type}\"\n",
					"                    mssparkutils.fs.cp(file_path, fallback_path, True)\n",
					"                    \n",
					"                    if mssparkutils.fs.exists(fallback_path):\n",
					"                        print(f\"  Successfully copied file with fallback method\")\n",
					"                        moved_files['data_success'].append(fallback_path)\n",
					"                        successful_files += 1\n",
					"                    else:\n",
					"                        print(f\"  ERROR: Fallback copy failed\")\n",
					"                        moved_files['failed'].append(f\"{file_path} (copy failed)\")\n",
					"            except Exception as proc_error:\n",
					"                print(f\"  ERROR processing file: {str(proc_error)}\")\n",
					"                moved_files['failed'].append(f\"{file_path} (processing error)\")\n",
					"        \n",
					"        if successful_files > 0:\n",
					"            return True, dest_dir\n",
					"        else:\n",
					"            return False, None\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"Error in file group processing: {str(e)}\")\n",
					"        return False, None"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def print_detailed_summary(moved_files):\n",
					"    \"\"\"Print detailed summary of moved files\"\"\"\n",
					"    print(\"\\nSummary of Moved Files:\")\n",
					"    print(f\"Data Files Successfully Moved: {len(moved_files['data_success'])}\")\n",
					"    for file in moved_files['data_success']:\n",
					"        print(f\"  - {file}\")\n",
					"    print(f\"Metadata Files Successfully Moved: {len(moved_files['metadata_success'])}\")\n",
					"    for file in moved_files['metadata_success']:\n",
					"        print(f\"  - {file}\")\n",
					"    # Add overseas files summary\n",
					"    overseas_count = len(moved_files.get('overseas', []))\n",
					"    print(f\"Overseas Files Moved: {overseas_count}\")\n",
					"    for file in moved_files.get('overseas', []):\n",
					"        print(f\"  - {file}\")\n",
					"    print(f\"Failed Files: {len(moved_files['failed'])}\")\n",
					"    for file in moved_files['failed']:\n",
					"        print(f\"  - {file}\")\n",
					"    print(f\"Quarantined Files: {len(moved_files['quarantine'])}\")\n",
					"    for file in moved_files['quarantine']:\n",
					"        print(f\"  - {file}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def normalize_file_path(path):\n",
					"    \"\"\"Normalize file path to ensure consistent comparison\"\"\"\n",
					"    if path is None:\n",
					"        return \"\"\n",
					"    # Remove any trailing slashes\n",
					"    path = path.rstrip('/')\n",
					"    # Convert to lowercase for case-insensitive comparison\n",
					"    return path.lower()\n",
					"\n",
					"def verify_file_processed(file_path, moved_files):\n",
					"    \"\"\"Verify if a file was successfully processed\"\"\"\n",
					"    # Check if the file path or a derivative is in the success lists\n",
					"    file_name = os.path.basename(file_path)\n",
					"    \n",
					"    # Check in data_success\n",
					"    for success_path in moved_files.get('data_success', []):\n",
					"        if file_name in success_path:\n",
					"            return True\n",
					"    \n",
					"    # Check in other success categories\n",
					"    for category in ['metadata_success', 'empty', 'quarantine', 'overseas']:\n",
					"        for success_path in moved_files.get(category, []):\n",
					"            if file_name in success_path:\n",
					"                return True\n",
					"    \n",
					"    return False\n",
					"\n",
					"def main():\n",
					"    storage_account = \"baubaisadfsaprod\"\n",
					"    source_path = f\"abfss://juror-ingestion@{storage_account}.dfs.core.windows.net\"\n",
					"    index_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/index/file_index.json\"\n",
					"    log_file_path = f\"abfss://juror-ingestion@{storage_account}.dfs.core.windows.net/processed_files_log.json\"\n",
					"    \n",
					"    # Add debug mode to process all files and ignore log\n",
					"    debug_mode = False  # Set to False to only process new files\n",
					"    \n",
					"    # Add a detailed log of all operations\n",
					"    operations_log = []\n",
					"    def log_operation(operation, file_path, status, details=None):\n",
					"        entry = {\n",
					"            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
					"            \"operation\": operation,\n",
					"            \"file_path\": file_path,\n",
					"            \"status\": status,\n",
					"            \"details\": details or {}\n",
					"        }\n",
					"        operations_log.append(entry)\n",
					"        print(f\"LOG: {entry['timestamp']} | {operation} | {file_path} | {status}\")\n",
					"    \n",
					"    moved_files = {\n",
					"        'data_success': [],\n",
					"        'metadata_success': [],\n",
					"        'empty': [],\n",
					"        'overseas': [],\n",
					"        'failed': [],\n",
					"        'quarantine': []\n",
					"    }\n",
					"    \n",
					"    # Verify source path exists with better error handling\n",
					"    try:\n",
					"        if not mssparkutils.fs.exists(source_path):\n",
					"            logging.error(f\"Source path does not exist: {source_path}\")\n",
					"            print(f\"ERROR: Source path does not exist: {source_path}\")\n",
					"            return\n",
					"        else:\n",
					"            logging.info(f\"Source path exists: {source_path}\")\n",
					"            print(f\"Source path exists: {source_path}\")\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error checking source path: {str(e)}\")\n",
					"        print(f\"ERROR checking source path: {str(e)}\")\n",
					"        return\n",
					"    \n",
					"    # Ensure destination directories exist with better error handling\n",
					"    dest_dirs = [\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/files\",\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/metadata\",\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/quarantine\",\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/empty\",\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/index\",\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/logs\",\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/overseas\" \n",
					"    ]\n",
					"    \n",
					"    for dir_path in dest_dirs:\n",
					"        try:\n",
					"            if not mssparkutils.fs.exists(dir_path):\n",
					"                print(f\"Creating destination directory: {dir_path}\")\n",
					"                mssparkutils.fs.mkdirs(dir_path)\n",
					"                log_operation(\"create_directory\", dir_path, \"success\")\n",
					"        except Exception as e:\n",
					"            print(f\"ERROR creating directory {dir_path}: {str(e)}\")\n",
					"            log_operation(\"create_directory\", dir_path, \"error\", {\"error\": str(e)})\n",
					"    \n",
					"    # Create/update the file index\n",
					"    file_index = {\"files\": []}\n",
					"    try: \n",
					"        print(\"Scanning for files to process...\")\n",
					"        all_files = process_folder(source_path, storage_account, moved_files)\n",
					"        print(f\"Found {len(all_files)} total files\")\n",
					"        \n",
					"        # Create a simple file index structure\n",
					"        file_index = {\n",
					"            \"created_at\": time.strftime(\"%Y%m%d%H%M%S\"),\n",
					"            \"files\": []\n",
					"        }\n",
					"        \n",
					"        # Process each file to get its metadata with better error handling\n",
					"        for file_path in all_files:\n",
					"            try:\n",
					"                file_name = os.path.basename(file_path)\n",
					"                date_folder, la_folder, confidence = get_folder_parts(file_path)\n",
					"                \n",
					"                file_info = {\n",
					"                    \"path\": file_path,\n",
					"                    \"file_name\": file_name,\n",
					"                    \"date_folder\": date_folder,\n",
					"                    \"la_folder\": la_folder,\n",
					"                    \"confidence\": confidence,\n",
					"                    \"file_type\": get_file_type(file_path),\n",
					"                    \"is_empty\": is_empty_file(file_path),\n",
					"                    \"is_metadata\": is_metadata_file(file_path),\n",
					"                    \"should_process\": should_process_file(file_path) and 'File Structure Definitions.xlsx' not in file_path,\n",
					"                    \"processed\": False\n",
					"                }\n",
					"                file_index[\"files\"].append(file_info)\n",
					"                log_operation(\"index_file\", file_path, \"success\", {\"file_info\": file_info})\n",
					"            except Exception as e:\n",
					"                print(f\"Error indexing file {file_path}: {str(e)}\")\n",
					"                log_operation(\"index_file\", file_path, \"error\", {\"error\": str(e)})\n",
					"                # Still add the file with basic info\n",
					"                file_index[\"files\"].append({\n",
					"                    \"path\": file_path,\n",
					"                    \"file_name\": os.path.basename(file_path),\n",
					"                    \"date_folder\": \"unknown\",\n",
					"                    \"la_folder\": \"unknown\",\n",
					"                    \"confidence\": \"very_low\",\n",
					"                    \"file_type\": None,\n",
					"                    \"is_empty\": False,\n",
					"                    \"is_metadata\": False,\n",
					"                    \"should_process\": True,\n",
					"                    \"processed\": False\n",
					"                })\n",
					"    except Exception as e:\n",
					"        print(f\"Error scanning files: {str(e)}\")\n",
					"        print(f\"Traceback: {traceback.format_exc()}\")\n",
					"        log_operation(\"scan_files\", source_path, \"error\", {\"error\": str(e)})\n",
					"    \n",
					"    print(f\"Working with index containing {len(file_index['files'])} files\")\n",
					"    \n",
					"    # Read the log of processed files with better error handling\n",
					"    processed_files_log = []\n",
					"    if not debug_mode:  # Skip reading log in debug mode\n",
					"        try:\n",
					"            if mssparkutils.fs.exists(log_file_path):\n",
					"                try:\n",
					"                    log_df = spark.read.json(log_file_path)\n",
					"                    # Check if \"file\" column exists in the log\n",
					"                    if \"file\" in log_df.columns:\n",
					"                        processed_files_log = log_df.select(\"file\").rdd.flatMap(lambda x: x).collect()\n",
					"                        # Normalize all paths in the log\n",
					"                        processed_files_log = [normalize_file_path(path) for path in processed_files_log]\n",
					"                        logging.info(f\"Read processed files log with {len(processed_files_log)} entries\")\n",
					"                        print(f\"\\nProcessed files log contains {len(processed_files_log)} entries\")\n",
					"                        log_operation(\"read_log\", log_file_path, \"success\", {\"count\": len(processed_files_log)})\n",
					"                    else:\n",
					"                        logging.warning(\"Log file exists but has unexpected structure. Will backup and recreate.\")\n",
					"                        print(f\"Log file has unexpected structure. Backing up and creating new log.\")\n",
					"                        # Backup the old log file\n",
					"                        backup_path = f\"{log_file_path}.bak.{time.strftime('%Y%m%d%H%M%S')}\"\n",
					"                        mssparkutils.fs.cp(log_file_path, backup_path, True)\n",
					"                        log_operation(\"backup_log\", log_file_path, \"success\", {\"backup_path\": backup_path})\n",
					"                        # Start with empty log\n",
					"                        processed_files_log = []\n",
					"                except Exception as parse_error:\n",
					"                    logging.warning(f\"Error parsing log file: {str(parse_error)}. Backing up and starting fresh.\")\n",
					"                    print(f\"Error parsing log file. Backing up and creating new log.\")\n",
					"                    # Backup the old log file\n",
					"                    backup_path = f\"{log_file_path}.corrupt.{time.strftime('%Y%m%d%H%M%S')}\"\n",
					"                    mssparkutils.fs.cp(log_file_path, backup_path, True)\n",
					"                    log_operation(\"backup_corrupted_log\", log_file_path, \"success\", {\"backup_path\": backup_path})\n",
					"                    # Start with empty log\n",
					"                    processed_files_log = []\n",
					"                \n",
					"                # Print first few log entries for debugging\n",
					"                if processed_files_log:\n",
					"                    print(\"First 3 processed file entries in log:\")\n",
					"                    for i, path in enumerate(processed_files_log[:3]):\n",
					"                        print(f\"  {i+1}. {path}\")\n",
					"            else:\n",
					"                print(f\"\\nProcessed files log does not exist at {log_file_path}, will create new log\")\n",
					"                log_operation(\"read_log\", log_file_path, \"info\", {\"message\": \"Log file does not exist yet\"})\n",
					"        except Exception as e:\n",
					"            logging.warning(f\"Could not read processed files log: {str(e)}\")\n",
					"            print(f\"\\nCould not read processed files log: {str(e)}\")\n",
					"            processed_files_log = []\n",
					"            log_operation(\"read_log\", log_file_path, \"error\", {\"error\": str(e)})\n",
					"    \n",
					"    # Filter out files that have already been processed\n",
					"    new_files = []\n",
					"    skipped_files = []\n",
					"    \n",
					"    for file_info in file_index[\"files\"]:\n",
					"        normalized_path = normalize_file_path(file_info[\"path\"])\n",
					"        if debug_mode or normalized_path not in processed_files_log:\n",
					"            # Check if the file still exists before adding it to process\n",
					"            if mssparkutils.fs.exists(file_info[\"path\"]):\n",
					"                new_files.append(file_info)\n",
					"                if not debug_mode and len(processed_files_log) > 0:\n",
					"                    print(f\"New file to process: {file_info['path']}\")\n",
					"            else:\n",
					"                print(f\"Skipping file that no longer exists: {file_info['path']}\")\n",
					"                skipped_files.append(file_info[\"path\"])\n",
					"        else:\n",
					"            print(f\"Skipping already processed file: {file_info['path']}\")\n",
					"            skipped_files.append(file_info[\"path\"])\n",
					"    \n",
					"    print(f\"\\nNew files to process: {len(new_files)}\")\n",
					"    print(f\"Skipped files: {len(skipped_files)}\")\n",
					"    log_operation(\"filter_files\", \"\", \"success\", {\n",
					"        \"total\": len(file_index[\"files\"]), \n",
					"        \"new\": len(new_files),\n",
					"        \"skipped\": len(skipped_files)\n",
					"    })\n",
					"    \n",
					"    # Group files by date and LA folder with better validation\n",
					"    file_groups = {}\n",
					"    individual_files = []\n",
					"    \n",
					"    for file_info in new_files:\n",
					"        # Skip files we can't process for some reason\n",
					"        try:\n",
					"            # Check if the file still exists\n",
					"            if not mssparkutils.fs.exists(file_info[\"path\"]):\n",
					"                print(f\"File no longer exists, skipping: {file_info['path']}\")\n",
					"                log_operation(\"check_file\", file_info[\"path\"], \"skip\", {\"reason\": \"not_exists\"})\n",
					"                continue\n",
					"                \n",
					"            # Files that should be processed individually\n",
					"            if file_info[\"is_empty\"] or file_info[\"is_metadata\"] or file_info[\"confidence\"] == \"very_low\" or not file_info[\"should_process\"]:\n",
					"                individual_files.append(file_info)\n",
					"                continue\n",
					"                \n",
					"            # Group by date and LA folder if valid\n",
					"            if file_info[\"date_folder\"] and file_info[\"la_folder\"]:\n",
					"                group_key = (file_info[\"date_folder\"], file_info[\"la_folder\"])\n",
					"                if group_key not in file_groups:\n",
					"                    file_groups[group_key] = []\n",
					"                file_groups[group_key].append(file_info)\n",
					"            else:\n",
					"                # If missing date or LA folder, process individually\n",
					"                individual_files.append(file_info)\n",
					"        except Exception as e:\n",
					"            # If any error, add to individual processing\n",
					"            print(f\"Error preparing file {file_info['path']}: {str(e)}\")\n",
					"            individual_files.append(file_info)\n",
					"    \n",
					"    print(f\"Grouped into {len(file_groups)} file groups and {len(individual_files)} individual files\")\n",
					"    log_operation(\"group_files\", \"\", \"success\", {\"groups\": len(file_groups), \"individual\": len(individual_files)})\n",
					"    \n",
					"    # Track actual processed files\n",
					"    successfully_processed_files = []\n",
					"    \n",
					"    # Process individual files first with better error handling\n",
					"    print(\"\\n====== PROCESSING INDIVIDUAL FILES ======\")\n",
					"    for i, file_info in enumerate(individual_files):\n",
					"        try:\n",
					"            print(f\"\\nProcessing individual file {i+1}/{len(individual_files)}\")\n",
					"            if not isinstance(file_info, dict):\n",
					"                print(f\"ERROR: file_info is not a dictionary: {type(file_info)}\")\n",
					"                log_operation(\"process_file\", \"unknown\", \"error\", {\"error\": \"invalid_file_info_type\"})\n",
					"                continue\n",
					"            \n",
					"            if \"path\" not in file_info:\n",
					"                print(f\"ERROR: file_info does not have a 'path' key: {file_info}\")\n",
					"                log_operation(\"process_file\", \"unknown\", \"error\", {\"error\": \"missing_path_key\"})\n",
					"                continue\n",
					"            \n",
					"            success = process_individual_file(file_info, storage_account, moved_files)\n",
					"            log_operation(\"process_individual\", file_info[\"path\"], \"success\" if success else \"error\")\n",
					"            \n",
					"            # Track if processing was successful for log update\n",
					"            if success and verify_file_processed(file_info[\"path\"], moved_files):\n",
					"                successfully_processed_files.append(file_info[\"path\"])\n",
					"        except Exception as e:\n",
					"            print(f\"Error processing individual file: {str(e)}\")\n",
					"            print(f\"File info: {file_info}\")\n",
					"            print(f\"Traceback: {traceback.format_exc()}\")\n",
					"            log_operation(\"process_individual\", file_info.get(\"path\", \"unknown\"), \"error\", {\"error\": str(e)})\n",
					"            if isinstance(file_info, dict) and \"path\" in file_info:\n",
					"                moved_files['failed'].append(f\"{file_info['path']} (processing error: {str(e)})\")\n",
					"            else:\n",
					"                moved_files['failed'].append(f\"Unknown file (processing error)\")\n",
					"    \n",
					"    # Process file groups with better error handling\n",
					"    print(\"\\n====== PROCESSING FILE GROUPS ======\")\n",
					"    group_idx = 0\n",
					"    total_groups = len(file_groups)\n",
					"    for (date_folder, la_folder), file_group in file_groups.items():\n",
					"        group_idx += 1\n",
					"        try:\n",
					"            # For clearer logs\n",
					"            group_id = f\"{date_folder}_{la_folder}\"\n",
					"            print(f\"\\nProcessing file group {group_idx}/{total_groups}: {group_id} with {len(file_group)} files\")\n",
					"            log_operation(\"process_group\", group_id, \"start\", {\"count\": len(file_group)})\n",
					"            \n",
					"            success, path = process_file_group(file_group, date_folder, la_folder, storage_account, moved_files)\n",
					"            log_operation(\"process_group\", group_id, \"success\" if success else \"error\")\n",
					"            \n",
					"            # If group processing successful, add all group files to successfully processed list\n",
					"            if success:\n",
					"                for file_info in file_group:\n",
					"                    successfully_processed_files.append(file_info[\"path\"])\n",
					"            else:\n",
					"                # If group processing failed, try individual processing\n",
					"                print(f\"Group processing failed, trying individual processing\")\n",
					"                log_operation(\"fallback_to_individual\", group_id, \"start\")\n",
					"                for file_info in file_group:\n",
					"                    try:\n",
					"                        individual_success = process_individual_file(file_info, storage_account, moved_files)\n",
					"                        if individual_success and verify_file_processed(file_info[\"path\"], moved_files):\n",
					"                            successfully_processed_files.append(file_info[\"path\"])\n",
					"                    except Exception as f_error:\n",
					"                        print(f\"Error in fallback individual processing: {str(f_error)}\")\n",
					"                        moved_files['failed'].append(f\"{file_info['path']} (fallback processing error)\")\n",
					"        except Exception as e:\n",
					"            group_id = f\"{date_folder}_{la_folder}\"\n",
					"            print(f\"Error processing file group {group_id}: {str(e)}\")\n",
					"            print(f\"Traceback: {traceback.format_exc()}\")\n",
					"            log_operation(\"process_group\", group_id, \"error\", {\"error\": str(e)})\n",
					"            \n",
					"            # Try individual processing even on group exception\n",
					"            print(f\"Group processing exception, trying individual processing\")\n",
					"            for file_info in file_group:\n",
					"                try:\n",
					"                    individual_success = process_individual_file(file_info, storage_account, moved_files)\n",
					"                    if individual_success and verify_file_processed(file_info[\"path\"], moved_files):\n",
					"                        successfully_processed_files.append(file_info[\"path\"])\n",
					"                except Exception as f_error:\n",
					"                    print(f\"Error in fallback individual processing: {str(f_error)}\")\n",
					"                    moved_files['failed'].append(f\"{file_info['path']} (fallback processing error)\")\n",
					"    \n",
					"    print_detailed_summary(moved_files)\n",
					"    \n",
					"    # Update the log of processed files\n",
					"    if not debug_mode:  # Don't update log in debug mode\n",
					"        try:\n",
					"            # Ensure the successfully_processed_files list is unique\n",
					"            unique_processed_files = list(set(successfully_processed_files))\n",
					"            \n",
					"            # Make sure we have a unique list of actually successfully processed files\n",
					"            print(f\"Updating log with {len(unique_processed_files)} successfully processed files\")\n",
					"            \n",
					"            # Make sure all paths are normalized\n",
					"            normalized_processed_files = [normalize_file_path(path) for path in unique_processed_files]\n",
					"            \n",
					"            # Create DataFrame for new processed files\n",
					"            new_processed_files_log = spark.createDataFrame([(file,) for file in normalized_processed_files], [\"file\"])\n",
					"            \n",
					"            if processed_files_log:\n",
					"                # Create DataFrame from existing log entries\n",
					"                existing_log_df = spark.createDataFrame([(file,) for file in processed_files_log], [\"file\"])\n",
					"                updated_log_df = existing_log_df.union(new_processed_files_log).distinct()\n",
					"            else:\n",
					"                updated_log_df = new_processed_files_log\n",
					"                \n",
					"            # Write the updated log\n",
					"            updated_log_df.write.mode(\"overwrite\").json(log_file_path)\n",
					"            \n",
					"            # Verify the log file was written\n",
					"            if mssparkutils.fs.exists(log_file_path):\n",
					"                log_size = mssparkutils.fs.size(log_file_path)\n",
					"                print(f\"Log file successfully updated, size: {log_size} bytes\")\n",
					"                print(f\"Added {len(unique_processed_files)} new entries to log\")\n",
					"                \n",
					"            logging.info(f\"Updated processed files log with {len(unique_processed_files)} entries\")\n",
					"            log_operation(\"update_log\", log_file_path, \"success\", {\"count\": len(unique_processed_files)})\n",
					"        except Exception as e:\n",
					"            print(f\"Error updating processed files log: {str(e)}\")\n",
					"            print(f\"Traceback: {traceback.format_exc()}\")\n",
					"            log_operation(\"update_log\", log_file_path, \"error\", {\"error\": str(e)})\n",
					"    \n",
					"    # Save the operations log\n",
					"    try:\n",
					"        # Convert operations log to DataFrame\n",
					"        if operations_log:\n",
					"            # Create a proper schema for the operations log\n",
					"            from pyspark.sql.types import StructType, StructField, StringType\n",
					"            schema = StructType([\n",
					"                StructField(\"timestamp\", StringType(), True),\n",
					"                StructField(\"operation\", StringType(), True),\n",
					"                StructField(\"file_path\", StringType(), True),\n",
					"                StructField(\"status\", StringType(), True)\n",
					"            ])\n",
					"            from pyspark.sql import Row\n",
					"\n",
					"            log_rows = [\n",
					"                Row(\n",
					"                    timestamp=log[\"timestamp\"],\n",
					"                    operation=log[\"operation\"],\n",
					"                    file_path=log[\"file_path\"],\n",
					"                    status=log[\"status\"]\n",
					"                )\n",
					"                for log in operations_log\n",
					"            ]\n",
					"\n",
					"            operations_log_df = spark.createDataFrame(log_rows, schema)\n",
					"            log_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/logs/operations_{time.strftime('%Y%m%d%H%M%S')}.parquet\"\n",
					"            operations_log_df.write.mode(\"overwrite\").parquet(log_path)\n",
					"            print(f\"Saved detailed operations log to: {log_path}\")\n",
					"    except Exception as e:\n",
					"        print(f\"Error saving operations log: {str(e)}\")\n",
					"        print(f\"Traceback: {traceback.format_exc()}\")\n",
					"\n",
					"    print(\"\\nSummary totals:\")\n",
					"    print(f\"Files processed: {len(new_files)}\")\n",
					"    print(f\"Files successfully processed: {len(successfully_processed_files)}\")\n",
					"    print(f\"Data files successfully moved: {len(moved_files['data_success'])}\")\n",
					"    print(f\"Metadata files successfully moved: {len(moved_files['metadata_success'])}\")\n",
					"    print(f\"Empty files moved: {len(moved_files.get('empty', []))}\")\n",
					"    print(f\"Overseas files moved: {len(moved_files.get('overseas', []))}\")\n",
					"    print(f\"Files quarantined: {len(moved_files['quarantine'])}\")\n",
					"    print(f\"Files failed: {len(moved_files['failed'])}\")"
				],
				"execution_count": null
			}
		]
	}
}