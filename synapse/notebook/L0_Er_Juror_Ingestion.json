{
	"name": "L0_Er_Juror_Ingestion",
	"properties": {
		"folder": {
			"name": "crime"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkbaubais",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "afe40a76-8da7-40b1-8cd6-cd80fa6298f7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/74dacd4f-a248-45bb-a2f0-af700dc4cf68/resourceGroups/baubais-data-factory-rg-stg/providers/Microsoft.Synapse/workspaces/baubais-synapse-stg/bigDataPools/sparkbaubais",
				"name": "sparkbaubais",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-stg.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkbaubais",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# L0 ER Juror Ingestion Pipeline\n",
					"\n",
					"## Overview\n",
					"This notebook implements an ETL pipeline for processing juror-related files stored in Azure Data Lake Storage (ADLS). It handles various file types (CSV, Excel, ZIP), moves files to appropriate folders (e.g., quarantine, metadata, overseas), and converts files to Parquet format for efficient storage and querying.\n",
					"\n",
					"## Features\n",
					"- **File Type Support**: Processes CSV, Excel, and ZIP files.\n",
					"- **Error Handling**: Gracefully handles errors and moves problematic files to quarantine.\n",
					"- **Distributed Processing**: Uses Apache Spark for scalable file processing.\n",
					"- **Azure Data Lake Integration**: Reads and writes files to ADLS.\n",
					"- **Metadata and Logging**: Adds metadata columns to processed files and maintains detailed logs.\n",
					"\n",
					"## Workflow\n",
					"1. **Initialization**:\n",
					"   - Configures logging and initializes a Spark session.\n",
					"2. **File Identification**:\n",
					"   - Identifies file types (CSV/Excel) and checks for metadata or empty files.\n",
					"3. **File Movement**:\n",
					"   - Moves files to appropriate folders (e.g., quarantine, metadata, overseas).\n",
					"4. **File Processing**:\n",
					"   - Converts CSV/Excel files to Parquet format and merges files by date and LA folder.\n",
					"5. **Folder Scanning**:\n",
					"   - Recursively scans directories for files to process.\n",
					"6. **Logging**:\n",
					"   - Tracks operations and saves logs to ADLS.\n",
					"\n",
					"## Key Functions\n",
					"`is_metadata_file(file_path)`\n",
					"Checks if a file is a metadata file based on its name.\n",
					"\n",
					"`should_process_file(file_path)`\n",
					"Determines if a file should be processed (CSV or Excel).\n",
					"\n",
					"`is_empty_file(file_path)`\n",
					"Checks if a file is empty or contains only a header.\n",
					"\n",
					"`unzip_file(file_path, extract_to, storage_account)`\n",
					"Extracts a ZIP file and uploads its contents to ADLS.\n",
					"\n",
					"`process_file(source_path, date_folder, la_folder, file_name, storage_account)`\n",
					"Processes a CSV or Excel file and converts it to Parquet format.\n",
					"\n",
					"`move_to_quarantine(file_path, storage_account, moved_files, reason=None)`\n",
					"Moves a file to the quarantine folder for manual inspection.\n",
					"\n",
					"`create_file_index(source_path, storage_account)`\n",
					"Creates a JSON index of all files in the source path.\n",
					"\n",
					"`main()`\n",
					"The main function that orchestrates the ETL workflow.\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## imports"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import logging\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"import time\n",
					"from notebookutils import mssparkutils\n",
					"import os\n",
					"import zipfile\n",
					"import shutil\n",
					"import tempfile\n",
					"import re\n",
					"import json  # Add this line\n",
					"from collections import defaultdict\n",
					"import logging\n",
					"import traceback"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Paramters"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": [
						"parameters"
					]
				},
				"source": [
					"env = \"stg\"\n",
					"storage_account = f\"baubaisadfsa{env}\"\n",
					"    # CONFIGURATION\n",
					"debug_mode = True  # Process only new files\n",
					"# year_filter = \"2025\"  # Only process 2025 files\n",
					"# source_container = \"juror-ingestion\"\n",
					"year_filter = \"2025\"  # Only process 2025 files\n",
					"source_container = \"juror-raw\"\n",
					"processing_container = \"qz-test\"\n",
					"ingestion_container = \"juror-raw\" #\"juror-ingestion\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# Configure logging\n",
					"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"\n",
					"# Initialize Spark session with the required packages\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"JurorIngestion\") \\\n",
					"    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.5,org.apache.xmlbeans:xmlbeans:3.1.0,org.apache.poi:poi-ooxml-schemas:4.1.2,org.apache.poi:poi-ooxml:4.1.2,org.apache.poi:poi:4.1.2\") \\\n",
					"    .getOrCreate()\n",
					"\n",
					"# Global counter to track file sequences by folder structure\n",
					"file_counters = defaultdict(int)\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def is_metadata_file(file_path):\n",
					"    \"\"\"Check if file is metadata related - enhanced version\"\"\"\n",
					"    metadata_indicators = [\n",
					"        'metadata.txt',\n",
					"        'readme.txt',\n",
					"        'readme',\n",
					"        'metadata',\n",
					"        'notes',\n",
					"        'information',\n",
					"        'instructions',\n",
					"        'guide'\n",
					"    ]\n",
					"    \n",
					"    # Case-insensitive check for indicators in filename\n",
					"    file_name_lower = os.path.basename(file_path).lower()\n",
					"    if any(indicator in file_name_lower for indicator in metadata_indicators):\n",
					"        return True\n",
					"        \n",
					"    # Check for files in metadata-like folders\n",
					"    path_lower = file_path.lower()\n",
					"    if 'metadata' in path_lower or 'docs' in path_lower or 'documentation' in path_lower:\n",
					"        return True\n",
					"        \n",
					"    return False"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def should_process_file(file_path):\n",
					"    \"\"\"Check if file should be processed (CSV or Excel) - case insensitive\"\"\"\n",
					"    extensions = ['.csv', '.xlsx', '.xls']\n",
					"    return any(file_path.lower().endswith(ext) for ext in extensions)\n",
					"\n",
					"def get_file_type(file_path):\n",
					"    \"\"\"Determine if file is CSV or Excel\"\"\"\n",
					"    if file_path.lower().endswith(('.xlsx', '.xls')):\n",
					"        return 'excel'\n",
					"    elif file_path.lower().endswith('.csv'):\n",
					"        return 'csv'\n",
					"    return None\n",
					"\n",
					"def is_zip_file(file_path):\n",
					"    \"\"\"Check if file is a zip file\"\"\"\n",
					"    return file_path.lower().endswith('.zip')"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def is_file_in_year(file_path, year_filter=\"2025\", prefix=\"\"):\n",
					"    \"\"\"\n",
					"    Check if file is in FIRST LEVEL date folder starting with specified year\n",
					"    \n",
					"    Structure: juror-ingestion/YYYYMMDD/LA_Data/...\n",
					"                               ^^^^^^^^ Must start with year_filter\n",
					"    \n",
					"    Args:\n",
					"        file_path: Full path to file\n",
					"        year_filter: Year to filter (e.g., \"2025\")\n",
					"    \n",
					"    Returns:\n",
					"        bool: True if in correct year folder, False otherwise\n",
					"    \n",
					"    Example:\n",
					"        \"juror-ingestion/20251014/LA_Data/file.csv\" → True (2025)\n",
					"        \"juror-ingestion/20241015/LA_Data/file.csv\" → False (2024)\n",
					"    \"\"\"\n",
					"    import re\n",
					"    \n",
					"    # Split path into parts\n",
					"    parts = file_path.split('/')\n",
					"    \n",
					"    # Find juror-ingestion container index\n",
					"    file_start_index = -1\n",
					"    for i, part in enumerate(parts):\n",
					"        if len(prefix) > 0 and prefix.lower() in part.lower():\n",
					"            file_start_index = i\n",
					"            break\n",
					"        elif source_container in part.lower():\n",
					"            file_start_index = i\n",
					"            \n",
					"    \n",
					"    # If found, check the NEXT part (first level folder after container)\n",
					"    if file_start_index >= 0 and len(parts) > file_start_index + 1:\n",
					"        first_level_folder = parts[file_start_index + 1]\n",
					"        \n",
					"        # Check if it's a date folder (8 digits: YYYYMMDD)\n",
					"        if re.match(r'^\\d{8}$', first_level_folder):\n",
					"            # Check if it starts with the year filter\n",
					"            if first_level_folder.startswith(year_filter):\n",
					"                print(f\"   ✅ {first_level_folder} starts with {year_filter}\")\n",
					"                return True  # Correct year\n",
					"            else:\n",
					"                print(f\"   ⏭️  {first_level_folder} does NOT start with {year_filter}\")\n",
					"                return False  # Wrong year\n",
					"        else:\n",
					"            print(f\"   ⚠️  First level '{first_level_folder}' is not a date folder\")\n",
					"            return False\n",
					"    \n",
					"    # No proper structure found\n",
					"    print(f\"   ⚠️  Could not find juror-ingestion container in path\")\n",
					"    return False\n",
					"\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def is_empty_file(file_path):\n",
					"    \"\"\"\n",
					"    Check if a file is empty and return specific error reason\n",
					"    \n",
					"    Returns:\n",
					"        tuple: (is_empty: bool, error_reason: str or None)\n",
					"    \"\"\"\n",
					"    try:\n",
					"        file_ext = os.path.splitext(file_path)[1].lower()\n",
					"        \n",
					"        if file_ext == '.csv':\n",
					"            # For CSV files\n",
					"            try:\n",
					"                df = spark.read.option(\"header\", \"true\").csv(file_path).limit(5)\n",
					"                row_count = df.count()\n",
					"                \n",
					"                if row_count == 0:\n",
					"                    return True, \"file_empty_header_only\"  # Has header but no data\n",
					"                return False, None\n",
					"            except:\n",
					"                return True, \"file_unreadable\"  # Can't read the CSV\n",
					"                \n",
					"        elif file_ext in ['.xlsx', '.xls']:\n",
					"            # For Excel files\n",
					"            try:\n",
					"                df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
					"                    .option(\"header\", \"true\") \\\n",
					"                    .load(file_path) \\\n",
					"                    .limit(5)\n",
					"                row_count = df.count()\n",
					"                \n",
					"                if row_count == 0:\n",
					"                    return True, \"file_empty_header_only\"\n",
					"                return False, None\n",
					"            except:\n",
					"                return True, \"file_unreadable\"\n",
					"                \n",
					"        # Default approach for other file types\n",
					"        try:\n",
					"            test_df = spark.read.text(file_path).limit(5)\n",
					"            if test_df.count() == 0:\n",
					"                return True, \"file_empty\"\n",
					"            return False, None\n",
					"        except Exception as e:\n",
					"            print(f\"Error checking if file is empty: {str(e)}\")\n",
					"            return True, \"file_unreadable\"\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"Error in is_empty_file check: {str(e)}\")\n",
					"        return True, \"file_corrupted\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Unzipfile"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def unzip_file(file_path:str, extract_to:str, storage_account:str, depth:int=0, max_depth:int=5, moved_files:dict=None):\n",
					"    \"\"\"\n",
					"    Unzip a file to a folder with the same name as the ZIP file\n",
					"    \n",
					"    Example:\n",
					"        Input:  .../262_SouthGloucesteshire/data_2024.zip\n",
					"        Output: .../262_SouthGloucesteshire/data_2024/\n",
					"                    ├── file1.csv\n",
					"                    ├── file2.xlsx\n",
					"                    └── nested.zip → data_2024/nested/ (if extracted)\n",
					"    \n",
					"    Returns:\n",
					"        tuple: (success: bool, error_reason: str or None)\n",
					"        \n",
					"    Error codes:\n",
					"        - \"password_protected\": ZIP requires password\n",
					"        - \"zip_corrupted\": ZIP file is corrupted/malformed\n",
					"        - \"max_recursion_depth\": Too many nested ZIP levels\n",
					"        - \"file_unreadable\": General extraction error\n",
					"    \"\"\"\n",
					"    indent = \"  \" * depth\n",
					"    \n",
					"    try:\n",
					"        print(f\"{indent}Attempting to unzip: {file_path}\")\n",
					"        \n",
					"        # Prevent infinite recursion\n",
					"        if depth >= max_depth:\n",
					"            print(f\"{indent}Max recursion depth reached\")\n",
					"            return False, \"max_recursion_depth\"\n",
					"        \n",
					"        # Get the directory where the ZIP file is located\n",
					"        zip_dir = os.path.dirname(file_path)\n",
					"        \n",
					"        # Get ZIP filename without extension (this will be the folder name)\n",
					"        zip_filename = os.path.basename(file_path)\n",
					"        folder_name = os.path.splitext(zip_filename)[0]  # Remove .zip extension\n",
					"        \n",
					"        # Create the extraction folder path\n",
					"        extraction_folder = f\"{zip_dir}/{extract_to}/\"\n",
					"        \n",
					"        print(f\"{indent}Will extract to folder: {extraction_folder}\")\n",
					"        \n",
					"        # Create a temporary local directory\n",
					"        with tempfile.TemporaryDirectory() as tmpdirname:\n",
					"            local_zip_path = os.path.join(tmpdirname, zip_filename)\n",
					"            \n",
					"            # Download the zip file to the local temporary directory\n",
					"            print(f\"{indent}Downloading zip file from {file_path} to {local_zip_path}\")\n",
					"            mssparkutils.fs.cp(file_path, f\"file://{local_zip_path}\")\n",
					"            \n",
					"            # Check if zip file is password protected before trying to extract\n",
					"            try:\n",
					"                zip_file = zipfile.ZipFile(local_zip_path)\n",
					"                \n",
					"                # Try to read the first file to see if it's password protected\n",
					"                for zip_info in zip_file.infolist():\n",
					"                    if zip_info.flag_bits & 0x1:\n",
					"                        print(f\"{indent}ZIP file is encrypted/password protected\")\n",
					"                        return False, \"password_protected\"\n",
					"                    \n",
					"                    # Try to read a little bit of data to check for password\n",
					"                    try:\n",
					"                        zip_file.open(zip_info.filename).read(1)\n",
					"                    except RuntimeError as e:\n",
					"                        if 'password required' in str(e).lower() or 'bad password' in str(e).lower():\n",
					"                            print(f\"{indent}Password required for ZIP file\")\n",
					"                            return False, \"password_protected\"\n",
					"                        raise\n",
					"                \n",
					"                # If we got here, try to extract\n",
					"                print(f\"{indent}Extracting ZIP file to {tmpdirname}\")\n",
					"                zip_file.extractall(path=tmpdirname)\n",
					"                zip_file.close()\n",
					"                \n",
					"            except zipfile.BadZipFile as e:\n",
					"                print(f\"{indent}Corrupted ZIP file: {str(e)}\")\n",
					"                return False, \"zip_corrupted\"\n",
					"            except RuntimeError as e:\n",
					"                error_msg = str(e).lower()\n",
					"                if 'password required' in error_msg or 'encrypted' in error_msg or 'bad password' in error_msg:\n",
					"                    print(f\"{indent}Password protected ZIP file detected\")\n",
					"                    return False, \"password_protected\"\n",
					"                raise\n",
					"            \n",
					"            # Create the extraction folder in ADLS\n",
					"            print(f\"{indent}Creating extraction folder in ADLS: {extraction_folder}\")\n",
					"            if not mssparkutils.fs.exists(extraction_folder):\n",
					"                mssparkutils.fs.mkdirs(extraction_folder)\n",
					"            \n",
					"            # Upload extracted files to ADLS in the extraction folder\n",
					"            print(f\"{indent}Uploading extracted files to Azure Data Lake\")\n",
					"            \n",
					"            for root, dirs, files in os.walk(tmpdirname):\n",
					"                for file in files:\n",
					"                    local_file_path = os.path.join(root, file)\n",
					"                    \n",
					"                    # Skip the original ZIP file itself\n",
					"                    if local_file_path == local_zip_path:\n",
					"                        continue\n",
					"                    \n",
					"                    # Get relative path from temp directory\n",
					"                    relative_path = os.path.relpath(local_file_path, tmpdirname)\n",
					"                    \n",
					"                    # Upload to extraction folder in ADLS\n",
					"                    remote_file_path = f\"{extraction_folder}/{relative_path}\"\n",
					"                    \n",
					"                    # Check if extracted file is also a ZIP (nested ZIP)\n",
					"                    if file.lower().endswith('.zip'):\n",
					"                        print(f\"{indent}Found nested ZIP file: {file}\")\n",
					"                        \n",
					"                        # Upload nested ZIP to ADLS first\n",
					"                        print(f\"{indent}Uploading nested ZIP to ADLS: {remote_file_path}\")\n",
					"                        mssparkutils.fs.cp(f\"file://{local_file_path}\", remote_file_path)\n",
					"                        \n",
					"                        # Recursively unzip the nested ZIP\n",
					"                        success, error_reason = unzip_file(\n",
					"                            remote_file_path, \n",
					"                            None,\n",
					"                            storage_account, \n",
					"                            depth + 1,\n",
					"                            max_depth,\n",
					"                            moved_files\n",
					"                        )\n",
					"                        \n",
					"                        if success:\n",
					"                            print(f\"{indent}Successfully extracted nested ZIP\")\n",
					"                        else:\n",
					"                            # Nested ZIP failed - move to quarantine\n",
					"                            print(f\"{indent}Failed to extract nested ZIP: {error_reason}\")\n",
					"                            if moved_files is not None:\n",
					"                                quarantine_reason = f\"nested_{error_reason}\"\n",
					"                                move_to_quarantine(\n",
					"                                    remote_file_path, \n",
					"                                    storage_account, \n",
					"                                    moved_files, \n",
					"                                    quarantine_reason\n",
					"                                )\n",
					"                    else:\n",
					"                        # Regular file - upload to extraction folder\n",
					"                        print(f\"{indent}Uploading: {relative_path}\")\n",
					"                        mssparkutils.fs.cp(f\"file://{local_file_path}\", remote_file_path)\n",
					"            \n",
					"            print(f\"{indent}✅ Successfully unzipped to folder: {extraction_folder}\")\n",
					"            return True, None\n",
					"            \n",
					"    except RuntimeError as e:\n",
					"        error_msg = str(e).lower()\n",
					"        if 'password required' in error_msg or 'encrypted' in error_msg or 'bad password' in error_msg:\n",
					"            print(f\"{indent}Password protected ZIP file detected\")\n",
					"            return False, \"password_protected\"\n",
					"        else:\n",
					"            print(f\"{indent}Runtime error unzipping file: {str(e)}\")\n",
					"            return False, \"file_unreadable\"\n",
					"    except Exception as e:\n",
					"        print(f\"{indent}Error unzipping file: {file_path} - {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"{indent}Traceback: {traceback.format_exc()}\")\n",
					"        return False, \"file_unreadable\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def check_for_overseas_columns(file_path, file_type):\n",
					"    \"\"\"Check if file contains overseas address columns and move to overseas container if found\"\"\"\n",
					"    try:\n",
					"        print(f\"  Checking for overseas address columns in: {file_path}\")\n",
					"        \n",
					"        # Read only the header row to check columns\n",
					"        if file_type == 'csv':\n",
					"            # For CSV files\n",
					"            df = spark.read.option(\"header\", \"true\").csv(file_path).limit(0)\n",
					"        elif file_type == 'excel':\n",
					"            # For Excel files\n",
					"            df = spark.read.format(\"com.crealytics.spark.excel\").option(\"header\", \"true\").load(file_path).limit(0)\n",
					"        else:\n",
					"            print(f\"  Unsupported file type for overseas check: {file_type}\")\n",
					"            return False, None\n",
					"            \n",
					"        # Get column names and check if overseas columns exist\n",
					"        columns = df.columns\n",
					"        has_overseas_columns = any(col in columns for col in ['Overseas Address','OverseasAddress1','QualifyingAddress1','UK Qualifying Address'])\n",
					"        \n",
					"        if has_overseas_columns:\n",
					"            print(f\"  Found overseas address columns in file: {file_path}\")\n",
					"            return True, columns\n",
					"        else:\n",
					"            print(f\"  No overseas address columns found in file: {file_path}\")\n",
					"            return False, None\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"  Error checking for overseas columns: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"  Traceback: {traceback.format_exc()}\")\n",
					"        return False, None\n",
					"\n",
					"def move_to_overseas(file_path, storage_account, moved_files):\n",
					"    \"\"\"Move a file to the overseas folder\"\"\"\n",
					"    try:\n",
					"        # Make sure overseas directory exists\n",
					"        overseas_dir = f\"abfss://{processing_container}@{storage_account}.dfs.core.windows.net/overseas\"\n",
					"        if not mssparkutils.fs.exists(overseas_dir):\n",
					"            print(f\"Creating overseas directory: {overseas_dir}\")\n",
					"            mssparkutils.fs.mkdirs(overseas_dir)\n",
					"        \n",
					"        # Create a unique filename to avoid conflicts\n",
					"        file_name = os.path.basename(file_path)\n",
					"        timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
					"        overseas_path = f\"{overseas_dir}/{timestamp}_{file_name}\"\n",
					"        \n",
					"        print(f\"Moving file to overseas: {file_path} -> {overseas_path}\")\n",
					"        \n",
					"        # Copy file to overseas directory\n",
					"        mssparkutils.fs.cp(file_path, overseas_path, True)\n",
					"        print(f\"Successfully copied file to overseas container\")\n",
					"        \n",
					"        # Update tracking dictionary (add 'overseas' category if it doesn't exist)\n",
					"        if 'overseas' not in moved_files:\n",
					"            moved_files['overseas'] = []\n",
					"        moved_files['overseas'].append(overseas_path)\n",
					"        \n",
					"        return True, overseas_path\n",
					"    except Exception as e:\n",
					"        print(f\"Error moving file to overseas container {file_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"Traceback: {traceback.format_exc()}\")\n",
					"        return False, None"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def move_to_quarantine(file_path, storage_account, moved_files, reason=None):\n",
					"    \"\"\"Enhanced function to move files to quarantine with date folder prefix instead of timestamp\"\"\"\n",
					"    try:\n",
					"        # Check if this file was already moved to quarantine\n",
					"        for existing_path in moved_files['quarantine']:\n",
					"            if os.path.basename(existing_path).endswith(os.path.basename(file_path)):\n",
					"                print(f\"File already in quarantine: {file_path}\")\n",
					"                return True, existing_path\n",
					"        \n",
					"        # Make sure quarantine directory exists\n",
					"        quarantine_dir = f\"abfss://{processing_container}@{storage_account}.dfs.core.windows.net/quarantine\"\n",
					"        if not mssparkutils.fs.exists(quarantine_dir):\n",
					"            print(f\"Creating quarantine directory: {quarantine_dir}\")\n",
					"            mssparkutils.fs.mkdirs(quarantine_dir)\n",
					"        \n",
					"        # Extract date_folder from the path\n",
					"        parts = file_path.split('/')\n",
					"        \n",
					"        # Try to find a date-formatted folder (8 digits for YYYYMMDD)\n",
					"        date_folder = None\n",
					"        for part in parts:\n",
					"            if re.match(r'^\\d{8}$', part):\n",
					"                date_folder = part\n",
					"                break\n",
					"        \n",
					"        # If not found, use current date\n",
					"        if not date_folder:\n",
					"            date_folder = time.strftime(\"%Y%m%d\")\n",
					"        \n",
					"        # Create a filename using date_folder as prefix\n",
					"        file_name = os.path.basename(file_path)\n",
					"        reason_tag = f\"_{reason}\" if reason else \"\"\n",
					"        quarantine_path = f\"{quarantine_dir}/{date_folder}{reason_tag}_{file_name}\"\n",
					"        \n",
					"        # Copy to quarantine\n",
					"        print(f\"Moving to quarantine: {file_path} -> {quarantine_path}\")\n",
					"        mssparkutils.fs.cp(file_path, quarantine_path, True)\n",
					"        \n",
					"        # Verify file was copied\n",
					"        if mssparkutils.fs.exists(quarantine_path):\n",
					"            print(f\"Successfully moved to quarantine\")\n",
					"            moved_files['quarantine'].append(quarantine_path)\n",
					"            return True, quarantine_path\n",
					"        else:\n",
					"            print(f\"Failed to move to quarantine\")\n",
					"            return False, None\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"Error in quarantine processing: {str(e)}\")\n",
					"        return False, None"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def process_metadata_file(file_path, file_name, storage_account):\n",
					"    f\"\"\"Process metadata file - corrected to use {processing_container} container\"\"\"\n",
					"    try:\n",
					"        # Debug info\n",
					"        print(f\"  process_metadata_file received: path={file_path}, name={file_name}\")\n",
					"        # Make sure metadata directory exists in the juror-etl container\n",
					"        metadata_dir = f\"abfss://{processing_container}@{storage_account}.dfs.core.windows.net/metadata\"\n",
					"        if not mssparkutils.fs.exists(metadata_dir):\n",
					"            print(f\"Creating metadata directory: {metadata_dir}\")\n",
					"            mssparkutils.fs.mkdirs(metadata_dir)\n",
					"        \n",
					"        # Create a unique name by including part of the original file path\n",
					"        # Remove the juror-ingestion container prefix\n",
					"        unique_name = file_path.replace(f\"abfss://juror-ingestion@{storage_account}.dfs.core.windows.net/\", \"\").replace(\"/\", \"_\")\n",
					"        destination_path = f\"{metadata_dir}/{unique_name}\"\n",
					"        \n",
					"        print(f\"Moving metadata file: {file_path} -> {destination_path}\")\n",
					"        \n",
					"        # Copy file to metadata directory in juror-etl container\n",
					"        mssparkutils.fs.cp(file_path, destination_path, True)\n",
					"        print(f\"Successfully copied metadata file\")\n",
					"        \n",
					"        print(f\"Moved metadata file to {destination_path}\")\n",
					"        return True, destination_path\n",
					"    except Exception as e:\n",
					"        print(f\"Error moving metadata file {file_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"Traceback: {traceback.format_exc()}\")\n",
					"        return False, None"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def process_empty_file(file_path, storage_account, moved_files):\n",
					"    \"\"\"Move an empty file to the empty folder\"\"\"\n",
					"    try:\n",
					"        # Make sure empty directory exists\n",
					"        empty_dir = f\"abfss://{processing_container}@{storage_account}.dfs.core.windows.net/empty\"\n",
					"        if not mssparkutils.fs.exists(empty_dir):\n",
					"            print(f\"Creating empty files directory: {empty_dir}\")\n",
					"            mssparkutils.fs.mkdirs(empty_dir)\n",
					"        \n",
					"        # Create a unique filename\n",
					"        file_name = os.path.basename(file_path)\n",
					"        timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
					"        empty_path = f\"{empty_dir}/{timestamp}_{file_name}\"\n",
					"        \n",
					"        print(f\"Moving empty file: {file_path} -> {empty_path}\")\n",
					"        \n",
					"        # Copy file to empty directory\n",
					"        mssparkutils.fs.cp(file_path, empty_path, True)\n",
					"        print(f\"Successfully copied empty file\")\n",
					"        \n",
					"        # Update tracking dictionary (add 'empty' category if it doesn't exist)\n",
					"        if 'empty' not in moved_files:\n",
					"            moved_files['empty'] = []\n",
					"        moved_files['empty'].append(empty_path)\n",
					"        \n",
					"        return True, empty_path\n",
					"    except Exception as e:\n",
					"        print(f\"Error moving empty file {file_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"Traceback: {traceback.format_exc()}\")\n",
					"        return False, None"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def explicit_file_copy(source_path, dest_path, storage_account):\n",
					"    \"\"\"Explicitly copy a file with better error handling and verification but no deletion\"\"\"\n",
					"    try:\n",
					"        # Debug info\n",
					"        print(f\"  Copying {source_path} to {dest_path}\")\n",
					"        \n",
					"        # Ensure destination directory exists\n",
					"        dest_dir = os.path.dirname(dest_path)\n",
					"        if not mssparkutils.fs.exists(dest_dir):\n",
					"            print(f\"  Creating destination directory: {dest_dir}\")\n",
					"            mssparkutils.fs.mkdirs(dest_dir)\n",
					"        \n",
					"        # Check if source exists\n",
					"        if not mssparkutils.fs.exists(source_path):\n",
					"            print(f\"  Source file does not exist: {source_path}\")\n",
					"            return False\n",
					"        \n",
					"        # Execute the copy\n",
					"        mssparkutils.fs.cp(source_path, dest_path, True)\n",
					"        \n",
					"        # Verify the file was copied\n",
					"        if mssparkutils.fs.exists(dest_path):\n",
					"            print(f\"  Verified file copied successfully\")\n",
					"            \n",
					"            # REMOVED THE DELETION CODE HERE\n",
					"            \n",
					"            return True\n",
					"        else:\n",
					"            print(f\"  ERROR: Copy failed, destination file not found: {dest_path}\")\n",
					"            return False\n",
					"    except Exception as e:\n",
					"        print(f\"  Error during file copy: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"  Traceback: {traceback.format_exc()}\")\n",
					"        return False"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def process_file(source_path, date_folder, la_folder, file_name, storage_account):\n",
					"    \"\"\"Process CSV or Excel file and convert to Parquet format\"\"\"\n",
					"    try:\n",
					"        # Create the folder name in the format YYYYMMDD_LA-Name\n",
					"        folder_key = f\"{date_folder}_{la_folder}\"\n",
					"        file_type = get_file_type(source_path)\n",
					"        \n",
					"        if not file_type:\n",
					"            print(f\"  Cannot determine file type for {source_path}\")\n",
					"            return False, None\n",
					"        \n",
					"        # Destination path for Parquet file (same regardless of source type)\n",
					"        dest_path = f\"abfss://{processing_container}@{storage_account}.dfs.core.windows.net/files/{folder_key}.parquet\"\n",
					"        \n",
					"        # Read the source file using different methods based on file type\n",
					"        try:\n",
					"            if file_type == 'excel':\n",
					"                print(f\"  Reading Excel file: {source_path}\")\n",
					"                \n",
					"                # Use pandas to read Excel\n",
					"                import pandas as pd\n",
					"                import tempfile\n",
					"                import os\n",
					"                \n",
					"                # Create a temporary file\n",
					"                temp_dir = tempfile.mkdtemp()\n",
					"                temp_file = os.path.join(temp_dir, \"temp_excel.xlsx\")\n",
					"                \n",
					"                # Download the file\n",
					"                mssparkutils.fs.cp(source_path, f\"file://{temp_file}\")\n",
					"                \n",
					"                # Read with pandas\n",
					"                excel_df = pd.read_excel(temp_file)\n",
					"                \n",
					"                # Convert to Spark DataFrame\n",
					"                df = spark.createDataFrame(excel_df)\n",
					"                \n",
					"                # Clean up\n",
					"                try:\n",
					"                    os.remove(temp_file)\n",
					"                    os.rmdir(temp_dir)\n",
					"                except:\n",
					"                    pass\n",
					"                \n",
					"            else:  # csv\n",
					"                print(f\"  Reading CSV file: {source_path}\")\n",
					"                df = spark.read \\\n",
					"                    .option(\"header\", \"true\") \\\n",
					"                    .option(\"inferSchema\", \"true\") \\\n",
					"                    .csv(source_path)\n",
					"            \n",
					"            # Add metadata columns for tracking\n",
					"            df = df.withColumn(\"source_file\", lit(source_path))\n",
					"            df = df.withColumn(\"source_type\", lit(file_type))\n",
					"            df = df.withColumn(\"ingestion_date\", lit(current_timestamp()))\n",
					"            df = df.withColumn(\"date_folder\", lit(date_folder))\n",
					"            df = df.withColumn(\"la_folder\", lit(la_folder))\n",
					"            \n",
					"            # Write to Parquet format\n",
					"            print(f\"  Writing to Parquet: {dest_path}\")\n",
					"            df.write.mode(\"overwrite\").parquet(dest_path)\n",
					"            \n",
					"            # Verify the Parquet file was created\n",
					"            if mssparkutils.fs.exists(dest_path):\n",
					"                print(f\"  Successfully converted to Parquet at: {dest_path}\")\n",
					"                return True, f\"parquet/{folder_key}.parquet\"\n",
					"            else:\n",
					"                print(f\"  ERROR: Parquet file not created at: {dest_path}\")\n",
					"                return False, None\n",
					"                \n",
					"        except Exception as read_error:\n",
					"            print(f\"  ERROR reading or converting file: {str(read_error)}\")\n",
					"            import traceback\n",
					"            print(f\"  Traceback: {traceback.format_exc()}\")\n",
					"            \n",
					"            # Fallback to simple copy without conversion if there's an error\n",
					"            print(f\"  Falling back to direct file copy without conversion\")\n",
					"            if file_type == 'excel':\n",
					"                fallback_path = f\"abfss://{processing_container}@{storage_account}.dfs.core.windows.net/files/{folder_key}.xlsx\"\n",
					"            else:  # csv\n",
					"                fallback_path = f\"abfss://{processing_container}@{storage_account}.dfs.core.windows.net/files/{folder_key}.csv\"\n",
					"            \n",
					"            mssparkutils.fs.cp(source_path, fallback_path, True)\n",
					"            \n",
					"            if mssparkutils.fs.exists(fallback_path):\n",
					"                print(f\"  Successfully copied file using fallback method\")\n",
					"                return True, f\"{file_type}/{folder_key}.{file_type}\"\n",
					"            else:\n",
					"                print(f\"  ERROR: Fallback copy failed\")\n",
					"                return False, None\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"  ERROR in process_file: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"  Traceback: {traceback.format_exc()}\")\n",
					"        return False, None"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def find_la_data_in_path(file_path):\n",
					"    \"\"\"Find LA_Data in path and extract date and LA folders with improved detection\"\"\"\n",
					"    parts = file_path.split('/')\n",
					"    print(f\"Path parts: {parts}\")\n",
					"    \n",
					"    # Try to find LA_Data in the path\n",
					"    la_index = -1\n",
					"    for i, part in enumerate(parts):\n",
					"        if part.lower() == 'la_data':\n",
					"            la_index = i\n",
					"            break\n",
					"    \n",
					"    if la_index == -1:\n",
					"        # Try more flexible matching (in case of case differences or formatting issues)\n",
					"        for i, part in enumerate(parts):\n",
					"            if 'la' in part.lower() and 'data' in part.lower():\n",
					"                print(f\"Found likely LA_Data folder with name: {part}\")\n",
					"                la_index = i\n",
					"                break\n",
					"    \n",
					"    if la_index == -1:\n",
					"        print(\"Could not find LA_Data or similar folder in path\")\n",
					"        return None, None\n",
					"        \n",
					"    print(f\"Found LA_Data at index {la_index}\")\n",
					"    \n",
					"    # Get date folder (before LA_Data)\n",
					"    date_folder = None\n",
					"    if la_index > 0:\n",
					"        date_folder = parts[la_index - 1]\n",
					"        print(f\"Date folder: {date_folder}\")\n",
					"    \n",
					"    # Get LA folder (after LA_Data)\n",
					"    la_folder = None\n",
					"    if len(parts) > la_index + 1:\n",
					"        la_folder = parts[la_index + 1]\n",
					"        print(f\"LA folder: {la_folder}\")\n",
					"    \n",
					"    return date_folder, la_folder"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def get_folder_parts(file_path):\n",
					"    \"\"\"Extract folder information with quarantine flagging for problematic patterns\"\"\"\n",
					"    parts = file_path.split('/')\n",
					"    print(f\"Path parts: {parts}\")\n",
					"    \n",
					"    # Initialize with defaults\n",
					"    date_folder = None\n",
					"    la_folder = None\n",
					"    confidence = \"low\"\n",
					"    \n",
					"    # Check for standard pattern with LA_Data\n",
					"    la_index = -1\n",
					"    for i, part in enumerate(parts):\n",
					"        if part.lower() == 'la_data':\n",
					"            la_index = i\n",
					"            break\n",
					"    \n",
					"    if la_index != -1:\n",
					"        # Get date folder (before LA_Data)\n",
					"        if la_index > 0:\n",
					"            date_folder = parts[la_index - 1]\n",
					"        \n",
					"        # Get LA folder (after LA_Data)\n",
					"        if len(parts) > la_index + 1:\n",
					"            la_folder = parts[la_index + 1]\n",
					"        \n",
					"        confidence = \"high\" if date_folder and la_folder else \"low\"\n",
					"    \n",
					"    # Check for problematic pattern like \"20231205/NT - White Ladies Aston.XLSX\"\n",
					"    filename = os.path.basename(file_path)\n",
					"    if not la_folder and \"-\" in filename:\n",
					"        # This is a non-standard pattern - flag for quarantine\n",
					"        confidence = \"very_low\"\n",
					"        \n",
					"        # Still extract something usable for the quarantine filename\n",
					"        date_pattern = re.findall(r'\\d{8}', '/'.join(parts))\n",
					"        if date_pattern:\n",
					"            date_folder = date_pattern[0]\n",
					"        else:\n",
					"            date_folder = time.strftime(\"%Y%m%d\")\n",
					"        \n",
					"        # Extract a clean identifier from the filename\n",
					"        name_part = os.path.splitext(filename)[0]\n",
					"        clean_name = re.sub(r'[^\\w\\-]', '_', name_part)\n",
					"        la_folder = f\"unknown_{clean_name}\"\n",
					"    \n",
					"    # Final fallbacks\n",
					"    if not date_folder:\n",
					"        date_folder = time.strftime(\"%Y%m%d\")\n",
					"        confidence = \"very_low\"\n",
					"    \n",
					"    if not la_folder:\n",
					"        name_part = os.path.splitext(filename)[0]\n",
					"        clean_name = re.sub(r'[^\\w\\-]', '_', name_part)\n",
					"        la_folder = f\"unknown_{clean_name}\"\n",
					"        confidence = \"very_low\"\n",
					"    \n",
					"    print(f\"Extracted: date_folder={date_folder}, la_folder={la_folder}, confidence={confidence}\")\n",
					"    return date_folder, la_folder, confidence"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Process folder method"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def process_folder(path, storage_account, moved_files, depth=0):\n",
					"    \"\"\"\n",
					"    Process folder recursively with ZIP extraction\n",
					"    \n",
					"    - Scans folders recursively\n",
					"    - When it finds a ZIP: extracts it to a named folder\n",
					"    - When extraction fails: moves ZIP to quarantine with error code\n",
					"    - Returns list of all processable files (CSV, Excel)\n",
					"    \"\"\"\n",
					"    all_files = []\n",
					"    indent = \"  \" * depth\n",
					"    \n",
					"    try:\n",
					"        print(f\"{indent}Scanning directory (depth {depth}): {path}\")\n",
					"        files = mssparkutils.fs.ls(path)\n",
					"        print(f\"{indent}Found {len(files)} items in {path}\")\n",
					"        \n",
					"        for file_info in files:\n",
					"            if file_info.isDir:\n",
					"                # Process subdirectories recursively\n",
					"                sub_files = process_folder(file_info.path, storage_account, moved_files, depth + 1)\n",
					"                all_files.extend(sub_files)\n",
					"                print(f\"{indent}Found {len(sub_files)} files in subdirectory\")\n",
					"            else:\n",
					"                # Check for ZIP files\n",
					"                if file_info.path.lower().endswith('.zip'):\n",
					"                    print(f\"{indent}Found ZIP file: {file_info.path}\")\n",
					"                    \n",
					"                    # Try to unzip the file\n",
					"                    success, error_reason = unzip_file(\n",
					"                        file_info.path,\n",
					"                        None,\n",
					"                        storage_account,\n",
					"                        depth=depth,  # Pass current depth (IMPORTANT!)\n",
					"                        max_depth=5,\n",
					"                        moved_files=moved_files\n",
					"                    )\n",
					"                    \n",
					"                    if success:\n",
					"                        print(f\"{indent}Successfully extracted ZIP file\")\n",
					"                        if 'zip_extracted' not in moved_files:\n",
					"                            moved_files['zip_extracted'] = []\n",
					"                        moved_files['zip_extracted'].append(file_info.path)\n",
					"                    else:\n",
					"                        # Failed to extract - move to quarantine with specific error reason\n",
					"                        print(f\"{indent}Moving ZIP to quarantine due to: {error_reason}\")\n",
					"                        \n",
					"                        success_q, quarantine_path = move_to_quarantine(\n",
					"                            file_info.path,\n",
					"                            storage_account,\n",
					"                            moved_files,\n",
					"                            error_reason  # password_protected, zip_corrupted, etc.\n",
					"                        )\n",
					"                        \n",
					"                        if success_q:\n",
					"                            print(f\"{indent}Successfully moved ZIP to quarantine: {quarantine_path}\")\n",
					"                        else:\n",
					"                            print(f\"{indent}Failed to move ZIP to quarantine\")\n",
					"                            if 'failed' not in moved_files:\n",
					"                                moved_files['failed'] = []\n",
					"                            moved_files['failed'].append(f\"{file_info.path} (quarantine failed)\")\n",
					"                \n",
					"                # Add non-ZIP files to processing list\n",
					"                elif 'File Structure Definitions.xlsx' not in file_info.path:\n",
					"                    print(f\"{indent}Found file: {file_info.path}\")\n",
					"                    all_files.append(file_info.path)\n",
					"                    \n",
					"    except Exception as e:\n",
					"        print(f\"{indent}Error accessing path {path}: {str(e)}\")\n",
					"        import traceback\n",
					"        print(f\"{indent}Traceback: {traceback.format_exc()}\")\n",
					"    \n",
					"    return all_files"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def process_individual_file(file_info, storage_account, moved_files):\n",
					"    \"\"\"\n",
					"    Process a single file with standardized error codes\n",
					"    \"\"\"\n",
					"    try:\n",
					"        file_path = file_info[\"path\"]\n",
					"        file_name = file_info[\"file_name\"]\n",
					"        date_folder = file_info[\"date_folder\"]\n",
					"        la_folder = file_info[\"la_folder\"]\n",
					"        confidence = file_info[\"confidence\"]\n",
					"        file_type = file_info[\"file_type\"]\n",
					"        \n",
					"        print(f\"\\nProcessing file: {file_path}\")\n",
					"        \n",
					"        # Check if file still exists\n",
					"        if not mssparkutils.fs.exists(file_path):\n",
					"            print(f\"  File no longer exists: {file_path}\")\n",
					"            return False\n",
					"        \n",
					"        # Check if file is empty - use new function that returns error reason\n",
					"        is_empty, empty_reason = is_empty_file(file_path)\n",
					"        if is_empty:\n",
					"            print(f\"  File is empty, moving to quarantine: {empty_reason}\")\n",
					"            success, _ = move_to_quarantine(file_path, storage_account, moved_files, empty_reason)\n",
					"            return success\n",
					"        \n",
					"        # Check for non-standard LA code pattern\n",
					"        if \"-\" in file_name and not (file_name.startswith(\"unknown_\") or \"la_data\" in file_path.lower()):\n",
					"            quarantine_reason = \"non_standard_la_code\"\n",
					"            print(f\"  Moving file to quarantine due to: {quarantine_reason}\")\n",
					"            success, _ = move_to_quarantine(file_path, storage_account, moved_files, quarantine_reason)\n",
					"            return success\n",
					"        \n",
					"        # Check for overseas columns\n",
					"        has_overseas, _ = check_for_overseas_columns(file_path, file_type)\n",
					"        if has_overseas:\n",
					"            success, _ = move_to_overseas(file_path, storage_account, moved_files)\n",
					"            return success\n",
					"        \n",
					"        # Check folder structure quality\n",
					"        should_quarantine = False\n",
					"        quarantine_reason = \"\"\n",
					"        \n",
					"        if not date_folder or not la_folder:\n",
					"            should_quarantine = True\n",
					"            quarantine_reason = \"missing_folder_parts\"\n",
					"        elif la_folder and \"la_data\" in la_folder.lower():\n",
					"            should_quarantine = True\n",
					"            quarantine_reason = \"invalid_la_code_la_data\"\n",
					"        elif confidence == \"very_low\":\n",
					"            should_quarantine = True\n",
					"            quarantine_reason = \"invalid_la_code_format\"\n",
					"        elif confidence == \"low\" and not re.match(r'^\\d{8}$', date_folder):\n",
					"            should_quarantine = True\n",
					"            quarantine_reason = \"low_confidence_structure\"\n",
					"        \n",
					"        if should_quarantine:\n",
					"            print(f\"  Moving file to quarantine due to: {quarantine_reason}\")\n",
					"            success, _ = move_to_quarantine(file_path, storage_account, moved_files, quarantine_reason)\n",
					"            return success\n",
					"        \n",
					"        # Process the file if it passed validation\n",
					"        print(f\"  Valid file structure, processing file...\")\n",
					"        success, new_path = process_file(file_path, date_folder, la_folder, file_name, storage_account)\n",
					"        \n",
					"        if success:\n",
					"            moved_files['data_success'].append(new_path)\n",
					"            print(f\"  Successfully processed to: {new_path}\")\n",
					"            return True\n",
					"        else:\n",
					"            # Processing failed - quarantine with specific reason\n",
					"            quarantine_reason = \"file_corrupted\"\n",
					"            print(f\"  Processing failed, moving to quarantine: {quarantine_reason}\")\n",
					"            success, _ = move_to_quarantine(file_path, storage_account, moved_files, quarantine_reason)\n",
					"            return success\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"  ERROR in process_individual_file: {str(e)}\")\n",
					"        # General error - quarantine\n",
					"        quarantine_reason = \"file_unreadable\"\n",
					"        move_to_quarantine(file_path, storage_account, moved_files, quarantine_reason)\n",
					"        return False"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def print_quarantine_summary(moved_files):\n",
					"    \"\"\"\n",
					"    Print summary showing ONLY the 6 essential error types\n",
					"    \"\"\"\n",
					"    quarantined = moved_files.get('quarantine', [])\n",
					"    \n",
					"    if len(quarantined) == 0:\n",
					"        print(\"\\n✅ No files quarantined\")\n",
					"        return\n",
					"    \n",
					"    print(\"\\n\" + \"=\" * 70)\n",
					"    print(\"QUARANTINE SUMMARY - ESSENTIAL ERRORS ONLY\")\n",
					"    print(\"=\" * 70)\n",
					"    \n",
					"    # Count each error type\n",
					"    error_counts = {\n",
					"        'ZIP_CORRUPTED': [],\n",
					"        'ZIP_PASSWORD_PROTECTED': [],\n",
					"        'EXCEL_CORRUPTED': [],\n",
					"        'EXCEL_EMPTY': [],\n",
					"        'CSV_CORRUPTED': [],\n",
					"        'CSV_EMPTY': []\n",
					"    }\n",
					"    \n",
					"    for file_path in quarantined:\n",
					"        filename = os.path.basename(file_path)\n",
					"        \n",
					"        # Extract error code from filename\n",
					"        for error_code in error_counts.keys():\n",
					"            if error_code in filename:\n",
					"                error_counts[error_code].append(filename)\n",
					"                break\n",
					"    \n",
					"    # Print each category\n",
					"    if error_counts['ZIP_CORRUPTED']:\n",
					"        print(f\"\\n💥 CORRUPTED ZIP FILES ({len(error_counts['ZIP_CORRUPTED'])}):\")\n",
					"        for f in error_counts['ZIP_CORRUPTED'][:10]:\n",
					"            print(f\"   • {f}\")\n",
					"        if len(error_counts['ZIP_CORRUPTED']) > 10:\n",
					"            print(f\"   ... and {len(error_counts['ZIP_CORRUPTED']) - 10} more\")\n",
					"    \n",
					"    if error_counts['ZIP_PASSWORD_PROTECTED']:\n",
					"        print(f\"\\n🔒 PASSWORD PROTECTED ZIP FILES ({len(error_counts['ZIP_PASSWORD_PROTECTED'])}):\")\n",
					"        for f in error_counts['ZIP_PASSWORD_PROTECTED'][:10]:\n",
					"            print(f\"   • {f}\")\n",
					"        if len(error_counts['ZIP_PASSWORD_PROTECTED']) > 10:\n",
					"            print(f\"   ... and {len(error_counts['ZIP_PASSWORD_PROTECTED']) - 10} more\")\n",
					"    \n",
					"    if error_counts['EXCEL_CORRUPTED']:\n",
					"        print(f\"\\n💥 CORRUPTED EXCEL FILES ({len(error_counts['EXCEL_CORRUPTED'])}):\")\n",
					"        for f in error_counts['EXCEL_CORRUPTED'][:10]:\n",
					"            print(f\"   • {f}\")\n",
					"        if len(error_counts['EXCEL_CORRUPTED']) > 10:\n",
					"            print(f\"   ... and {len(error_counts['EXCEL_CORRUPTED']) - 10} more\")\n",
					"    \n",
					"    if error_counts['EXCEL_EMPTY']:\n",
					"        print(f\"\\n📭 EMPTY EXCEL FILES ({len(error_counts['EXCEL_EMPTY'])}):\")\n",
					"        for f in error_counts['EXCEL_EMPTY'][:10]:\n",
					"            print(f\"   • {f}\")\n",
					"        if len(error_counts['EXCEL_EMPTY']) > 10:\n",
					"            print(f\"   ... and {len(error_counts['EXCEL_EMPTY']) - 10} more\")\n",
					"    \n",
					"    if error_counts['CSV_CORRUPTED']:\n",
					"        print(f\"\\n💥 CORRUPTED CSV FILES ({len(error_counts['CSV_CORRUPTED'])}):\")\n",
					"        for f in error_counts['CSV_CORRUPTED'][:10]:\n",
					"            print(f\"   • {f}\")\n",
					"        if len(error_counts['CSV_CORRUPTED']) > 10:\n",
					"            print(f\"   ... and {len(error_counts['CSV_CORRUPTED']) - 10} more\")\n",
					"    \n",
					"    if error_counts['CSV_EMPTY']:\n",
					"        print(f\"\\n📭 EMPTY CSV FILES ({len(error_counts['CSV_EMPTY'])}):\")\n",
					"        for f in error_counts['CSV_EMPTY'][:10]:\n",
					"            print(f\"   • {f}\")\n",
					"        if len(error_counts['CSV_EMPTY']) > 10:\n",
					"            print(f\"   ... and {len(error_counts['CSV_EMPTY']) - 10} more\")\n",
					"    \n",
					"    print(\"\\n\" + \"=\" * 70)\n",
					"    print(f\"Total quarantined: {len(quarantined)} files\")\n",
					"    print(\"=\" * 70)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def normalize_file_path(file_path):\n",
					"    \"\"\"Normalize file paths for consistent comparison\"\"\"\n",
					"    if not file_path:\n",
					"        return \"\"\n",
					"    \n",
					"    normalized = file_path.lower()\n",
					"    normalized = normalized.replace(\"abfss://\", \"abfs://\")\n",
					"    normalized = normalized.replace(\"wasbs://\", \"wasb://\")\n",
					"    normalized = normalized.rstrip(\"/\")\n",
					"    \n",
					"    # Remove double slashes except after protocol\n",
					"    while \"//\" in normalized[10:]:\n",
					"        normalized = normalized[:10] + normalized[10:].replace(\"//\", \"/\")\n",
					"    \n",
					"    return normalized"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def verify_file_processed(file_path, moved_files):\n",
					"    \"\"\"Verify if a file was successfully processed\"\"\"\n",
					"    # Check if the file path or a derivative is in the success lists\n",
					"    file_name = os.path.basename(file_path)\n",
					"    \n",
					"    # Check in data_success\n",
					"    for success_path in moved_files.get('data_success', []):\n",
					"        if file_name in success_path:\n",
					"            return True\n",
					"    \n",
					"    # Check in other success categories\n",
					"    for category in ['metadata_success', 'empty', 'quarantine', 'overseas']:\n",
					"        for success_path in moved_files.get(category, []):\n",
					"            if file_name in success_path:\n",
					"                return True\n",
					"    \n",
					"    return False  # Use False"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def read_processed_files_log(storage_account):\n",
					"    \"\"\"Read the log of previously processed files\"\"\"\n",
					"    log_file_path = f\"abfss://{processing_container}@{storage_account}.dfs.core.windows.net/processed_files_log.json\"\n",
					"    \n",
					"    print(\"\\n\" + \"=\" * 70)\n",
					"    print(\"READING PROCESSED FILES LOG\")\n",
					"    print(\"=\" * 70)\n",
					"    \n",
					"    if not mssparkutils.fs.exists(log_file_path):\n",
					"        print(\"ℹ️  No existing processed files log found\")\n",
					"        print(\"   All files will be processed as new\")\n",
					"        return set()\n",
					"    \n",
					"    try:\n",
					"        # Read using Spark\n",
					"        text_df = spark.read.text(log_file_path)\n",
					"        log_content = \"\\n\".join([row.value for row in text_df.collect()])\n",
					"        \n",
					"        # Parse JSON - handle both list and object formats\n",
					"        log_data = json.loads(log_content)\n",
					"        \n",
					"        processed_files = set()\n",
					"        \n",
					"        # Handle different log formats\n",
					"        if isinstance(log_data, list):\n",
					"            # List format: [{\"file\": \"path\"}, ...]\n",
					"            for entry in log_data:\n",
					"                if isinstance(entry, dict) and \"file\" in entry:\n",
					"                    file_path = entry[\"file\"]\n",
					"                    processed_files.add(file_path.lower())\n",
					"                    processed_files.add(normalize_file_path(file_path))\n",
					"        elif isinstance(log_data, dict) and \"processed_files\" in log_data:\n",
					"            # Object format: {\"processed_files\": [...]}\n",
					"            for entry in log_data[\"processed_files\"]:\n",
					"                if isinstance(entry, dict):\n",
					"                    file_path = entry.get(\"file_path\", entry.get(\"file\", \"\"))\n",
					"                    if file_path:\n",
					"                        processed_files.add(file_path.lower())\n",
					"                        processed_files.add(normalize_file_path(file_path))\n",
					"        \n",
					"        print(f\"✅ Successfully loaded {len(processed_files)} processed file entries\")\n",
					"        \n",
					"        # Show sample\n",
					"        sample = list(processed_files)[:3]\n",
					"        if sample:\n",
					"            print(\"\\n📋 Sample entries:\")\n",
					"            for i, fp in enumerate(sample, 1):\n",
					"                print(f\"   {i}. {fp[:80]}...\")\n",
					"        \n",
					"        return processed_files\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"❌ ERROR reading log: {str(e)}\")\n",
					"        import traceback\n",
					"        print(traceback.format_exc())\n",
					"        return set()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def process_file_group(file_group, date_folder, la_folder, storage_account, moved_files):\n",
					"    # Modified function to ensure proper file copying\n",
					"    try:\n",
					"        print(f\"\\nProcessing file group: {len(file_group)} files for {date_folder}_{la_folder}\")\n",
					"        \n",
					"        # Ensure destination directory exists\n",
					"        dest_dir = f\"abfss://{processing_container}@{storage_account}.dfs.core.windows.net/files\"\n",
					"        if not mssparkutils.fs.exists(dest_dir):\n",
					"            print(f\"Creating destination directory: {dest_dir}\")\n",
					"            mssparkutils.fs.mkdirs(dest_dir)\n",
					"        \n",
					"        successful_files = 0\n",
					"        \n",
					"        # Process each file individually\n",
					"        for idx, info in enumerate(file_group):\n",
					"            file_path = info[\"path\"]\n",
					"            file_name = info[\"file_name\"]\n",
					"            file_type = info[\"file_type\"]\n",
					"            \n",
					"            # Skip if file doesn't exist\n",
					"            if not mssparkutils.fs.exists(file_path):\n",
					"                print(f\"  File doesn't exist, skipping: {file_path}\")\n",
					"                continue\n",
					"            \n",
					"            # Create a unique destination filename \n",
					"            unique_suffix = f\"_{idx}\" if idx > 0 else \"\"\n",
					"            unique_folder_key = f\"{date_folder}_{la_folder}{unique_suffix}\"\n",
					"            \n",
					"            # Use Parquet as the destination format for consistency\n",
					"            dest_path = f\"{dest_dir}/{unique_folder_key}.parquet\"\n",
					"            \n",
					"            try:\n",
					"                # Process the file to Parquet using the process_file function\n",
					"                success, new_path = process_file(file_path, date_folder, la_folder, file_name, storage_account)\n",
					"                \n",
					"                if success:\n",
					"                    print(f\"  Successfully processed file to: {new_path}\")\n",
					"                    moved_files['data_success'].append(new_path)\n",
					"                    successful_files += 1\n",
					"                else:\n",
					"                    # Fallback to direct copy if processing fails\n",
					"                    print(f\"  Processing failed, falling back to direct copy\")\n",
					"                    fallback_path = f\"{dest_dir}/{unique_folder_key}.{file_type}\"\n",
					"                    mssparkutils.fs.cp(file_path, fallback_path, True)\n",
					"                    \n",
					"                    if mssparkutils.fs.exists(fallback_path):\n",
					"                        print(f\"  Successfully copied file with fallback method\")\n",
					"                        moved_files['data_success'].append(fallback_path)\n",
					"                        successful_files += 1\n",
					"                    else:\n",
					"                        print(f\"  ERROR: Fallback copy failed\")\n",
					"                        moved_files['failed'].append(f\"{file_path} (copy failed)\")\n",
					"            except Exception as proc_error:\n",
					"                print(f\"  ERROR processing file: {str(proc_error)}\")\n",
					"                moved_files['failed'].append(f\"{file_path} (processing error)\")\n",
					"        \n",
					"        if successful_files > 0:\n",
					"            return True, dest_dir\n",
					"        else:\n",
					"            return False, None  # Use False, not Fals\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"Error in file group processing: {str(e)}\")\n",
					"        return False, None  # Use False, not Fals"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def create_file_index_fast(all_files):\n",
					"    \"\"\"\n",
					"    Create file index WITHOUT checking if files are empty (faster)\n",
					"    Only check empty status during actual processing\n",
					"    \"\"\"\n",
					"    file_index = {\n",
					"        \"created_at\": time.strftime(\"%Y%m%d%H%M%S\"),\n",
					"        \"files\": []\n",
					"    }\n",
					"    \n",
					"    for file_path in all_files:\n",
					"        try:\n",
					"            file_name = os.path.basename(file_path)\n",
					"            date_folder, la_folder, confidence = get_folder_parts(file_path)\n",
					"            \n",
					"            file_info = {\n",
					"                \"path\": file_path,\n",
					"                \"file_name\": file_name,\n",
					"                \"date_folder\": date_folder,\n",
					"                \"la_folder\": la_folder,\n",
					"                \"confidence\": confidence,\n",
					"                \"file_type\": get_file_type(file_path),\n",
					"                \"is_empty\": False,  # Don't check yet - check during processing\n",
					"                \"is_metadata\": is_metadata_file(file_path),\n",
					"                \"should_process\": should_process_file(file_path) and 'File Structure Definitions.xlsx' not in file_path,\n",
					"                \"processed\": False\n",
					"            }\n",
					"            file_index[\"files\"].append(file_info)\n",
					"            \n",
					"        except Exception as e:\n",
					"            print(f\"Error indexing file {file_path}: {str(e)}\")\n",
					"            # Add with minimal info\n",
					"            file_index[\"files\"].append({\n",
					"                \"path\": file_path,\n",
					"                \"file_name\": os.path.basename(file_path),\n",
					"                \"date_folder\": \"unknown\",\n",
					"                \"la_folder\": \"unknown\",\n",
					"                \"confidence\": \"very_low\",\n",
					"                \"file_type\": None,\n",
					"                \"is_empty\": False,\n",
					"                \"is_metadata\": False,\n",
					"                \"should_process\": True,\n",
					"                \"processed\": False\n",
					"            })\n",
					"    \n",
					"    return file_index"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def update_processed_files_log(storage_account, log_file_path, successfully_processed_files):\n",
					"    \"\"\"Update the log with newly processed files - SIMPLIFIED\"\"\"\n",
					"    \n",
					"    if len(successfully_processed_files) == 0:\n",
					"        print(\"\\nℹ️  No new files to add to log\")\n",
					"        return True\n",
					"    \n",
					"    print(\"\\n\" + \"=\" * 70)\n",
					"    print(\"UPDATING PROCESSED FILES LOG\")\n",
					"    print(\"=\" * 70)\n",
					"    print(f\"Adding {len(successfully_processed_files)} files to log\")\n",
					"    \n",
					"    try:\n",
					"        # Read existing log\n",
					"        existing_entries = []\n",
					"        if mssparkutils.fs.exists(log_file_path):\n",
					"            try:\n",
					"                text_df = spark.read.text(log_file_path)\n",
					"                log_content = \"\\n\".join([row.value for row in text_df.collect()])\n",
					"                existing_entries = json.loads(log_content)\n",
					"                \n",
					"                if not isinstance(existing_entries, list):\n",
					"                    existing_entries = []\n",
					"                    \n",
					"                print(f\"   Read {len(existing_entries)} existing entries\")\n",
					"            except:\n",
					"                print(\"   Could not read existing log, starting fresh\")\n",
					"                existing_entries = []\n",
					"        \n",
					"        # Create new entries (simple format)\n",
					"        new_entries = [{\"file\": fp} for fp in successfully_processed_files]\n",
					"        \n",
					"        # Combine\n",
					"        all_entries = existing_entries + new_entries\n",
					"        \n",
					"        # Remove duplicates based on file path\n",
					"        seen = set()\n",
					"        unique_entries = []\n",
					"        for entry in all_entries:\n",
					"            file_path = entry.get(\"file\", \"\").lower()\n",
					"            if file_path and file_path not in seen:\n",
					"                seen.add(file_path)\n",
					"                unique_entries.append(entry)\n",
					"        \n",
					"        print(f\"   Total unique entries: {len(unique_entries)}\")\n",
					"        print(f\"   New entries added: {len(new_entries)}\")\n",
					"        \n",
					"        # Write to JSON (simple format)\n",
					"        log_json = json.dumps(unique_entries, indent=2)\n",
					"        \n",
					"        # Write using mssparkutils\n",
					"        temp_path = log_file_path + \".tmp\"\n",
					"        \n",
					"        # Create temp file locally\n",
					"        with open(\"/tmp/processed_log.json\", \"w\") as f:\n",
					"            f.write(log_json)\n",
					"        \n",
					"        # Upload to ADLS\n",
					"        mssparkutils.fs.put(temp_path, log_json, overwrite=True)\n",
					"        \n",
					"        # Move to final location\n",
					"        if mssparkutils.fs.exists(log_file_path):\n",
					"            mssparkutils.fs.rm(log_file_path)\n",
					"        mssparkutils.fs.mv(temp_path, log_file_path)\n",
					"        \n",
					"        print(\"✅ Successfully updated processed files log\")\n",
					"        \n",
					"        return True\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"❌ ERROR updating log: {str(e)}\")\n",
					"        import traceback\n",
					"        print(traceback.format_exc())\n",
					"        return False"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Main method"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def print_hearders():\n",
					"    print(\"\\n\" + \"=\" * 70)\n",
					"    print(\"L0 JUROR INGESTION PIPELINE\")\n",
					"    print(\"=\" * 70)\n",
					"    print(f\"Mode: {'DEBUG (all files)' if debug_mode else 'PRODUCTION (new files only)'}\")\n",
					"    print(f\"Year filter: {year_filter}\")\n",
					"    print(\"=\" * 70)\n",
					"\n",
					"def print_step1():\n",
					"    print(\"\\n\" + \"=\" * 70)\n",
					"    print(\"STEP 1: SCANNING & EXTRACTING ZIPS\")\n",
					"    print(\"=\" * 70)\n",
					"    print(\"🔍 Scanning folders and extracting ZIP files...\")\n",
					"    print(\"   (Corrupted/password-protected ZIPs → quarantine)\")\n",
					"\n",
					"def print_step1_results(_all_files, _moved_files):\n",
					"    print(f\"\\n📊 Scan results:\")\n",
					"    print(f\"   Total files found: {len(_all_files)}\")\n",
					"    print(f\"   ZIPs extracted: {len(_moved_files.get('zip_extracted', []))}\")\n",
					"    print(f\"   ZIPs quarantined: {len([f for f in _moved_files['quarantine'] if f.lower().endswith('.zip')])}\")\n",
					"\n",
					"def print_step2():\n",
					"    print(\"\\n\" + \"=\" * 70)\n",
					"    print(\"STEP 2: CHECKING FILES FOR EMPTY/CORRUPTED\")\n",
					"    print(\"=\" * 70)\n",
					"    print(\"🔍 Checking CSV and Excel files...\")\n",
					"    print(\"   (Empty/corrupted files → quarantine)\")\n",
					"\n",
					"def print_step2_results(_valid_files, _moved_files):\n",
					"    print(f\"\\n📊 File check results:\")\n",
					"    print(f\"   Valid files: {len(_valid_files)}\")\n",
					"    print(f\"   Files quarantined: {len(_moved_files['quarantine']) - len([f for f in _moved_files['quarantine'] if f.lower().endswith('.zip')])}\")\n",
					"    "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def process_raw_zip(path, storage_account, sink_path, moved_files):\n",
					"    files = mssparkutils.fs.ls(path)\n",
					"    indent = \"\"\n",
					"    print(f\"{indent}Found {len(files)} items in {path}\")\n",
					"        \n",
					"    for file_info in files:\n",
					"        # We are expecting only zip files on this levle\n",
					"        if file_info.path.lower().endswith('.zip'):\n",
					"            print(f\"{indent}Found ZIP file: {file_info.path}\")\n",
					"            success, error_reason = unzip_file(\n",
					"                        file_info.path,\n",
					"                        sink_path,\n",
					"                        storage_account,\n",
					"                        depth=0,  # Pass current depth (IMPORTANT!)\n",
					"                        max_depth=5,\n",
					"                        moved_files=moved_files\n",
					"                    )\n",
					"                    \n",
					"            if success:\n",
					"                print(f\"{indent}Successfully extracted ZIP file\")\n",
					"                if 'zip_extracted' not in moved_files:\n",
					"                    moved_files['zip_extracted'] = []\n",
					"                moved_files['zip_extracted'].append(file_info.path)\n",
					"            else:\n",
					"                # Failed to extract - move to quarantine with specific error reason\n",
					"                print(f\"{indent}Moving ZIP to quarantine due to: {error_reason}\")\n",
					"                \n",
					"                success_q, quarantine_path = move_to_quarantine(\n",
					"                    file_info.path,\n",
					"                    storage_account,\n",
					"                    moved_files,\n",
					"                    error_reason  # password_protected, zip_corrupted, etc.\n",
					"                )\n",
					"                \n",
					"                if success_q:\n",
					"                    print(f\"{indent}Successfully moved ZIP to quarantine: {quarantine_path}\")\n",
					"                else:\n",
					"                    print(f\"{indent}Failed to move ZIP to quarantine\")\n",
					"                    if 'failed' not in moved_files:\n",
					"                        moved_files['failed'] = []\n",
					"                    moved_files['failed'].append(f\"{file_info.path} (quarantine failed)\")\n",
					"        else:\n",
					"            success_q, quarantine_path = move_to_quarantine(\n",
					"                    file_info.path,\n",
					"                    storage_account,\n",
					"                    moved_files,\n",
					"                    error_reason  # password_protected, zip_corrupted, etc.\n",
					"                )\n",
					"                \n",
					"            if success_q:\n",
					"                print(f\"{indent}Successfully moved ZIP to quarantine: {quarantine_path}\")\n",
					"            else:\n",
					"                print(f\"{indent}Failed to move ZIP to quarantine\")\n",
					"                if 'failed' not in moved_files:\n",
					"                    moved_files['failed'] = []\n",
					"                moved_files['failed'].append(f\"{file_info.path} (quarantine failed)\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def main():\n",
					"    source_path = f\"abfss://{source_container}@{storage_account}.dfs.core.windows.net\"\n",
					"    log_file_path = f\"abfss://{processing_container}@{storage_account}.dfs.core.windows.net/processed_files_log.json\"\n",
					"    print_hearders()\n",
					"    \n",
					"    # Initialize tracking\n",
					"    moved_files = {\n",
					"        'data_success': [],\n",
					"        'metadata_success': [],\n",
					"        'empty': [],\n",
					"        'overseas': [],\n",
					"        'failed': [],\n",
					"        'zip_extracted': [],\n",
					"        'zip_skipped': [],\n",
					"        'quarantine': []\n",
					"    }\n",
					"    \n",
					"    sink_path = \"extracted\"\n",
					"\n",
					"    process_raw_zip(source_path, storage_account, sink_path, moved_files)\n",
					"\n",
					"    \n",
					"    # ========================================================================\n",
					"    # STEP 1: Scan for all files AND extract ZIPs\n",
					"    # ========================================================================\n",
					"        # Verify source path exists\n",
					"    if not mssparkutils.fs.exists(source_path):\n",
					"        print(f\"ERROR: Source path does not exist: {source_path}\")\n",
					"        return\n",
					"    source_path = f\"{source_path}/{sink_path}\"\n",
					"    print_step1()\n",
					"    all_files = process_folder(source_path, storage_account, moved_files)\n",
					"    \n",
					"    print_step1_results(all_files, moved_files)\n",
					"    # ========================================================================\n",
					"    # STEP 2: Check each file for empty/corrupted (quarantine bad ones)\n",
					"    # ========================================================================\n",
					"    print_step2()\n",
					"    \n",
					"    valid_files = []\n",
					"    \n",
					"    for file_path in all_files:\n",
					"        file_name = os.path.basename(file_path)\n",
					"        file_ext = os.path.splitext(file_path)[1].lower()\n",
					"        \n",
					"        # Only check CSV and Excel files\n",
					"        if file_ext in ['.csv', '.xlsx', '.xls']:\n",
					"            # Check if empty or corrupted\n",
					"            has_error, error_code = is_empty_file(file_path)\n",
					"            \n",
					"            if has_error and error_code:\n",
					"                # File is empty or corrupted - QUARANTINE\n",
					"                print(f\"   ❌ {file_name}: {error_code}\")\n",
					"                move_to_quarantine(file_path, storage_account, moved_files, error_code)\n",
					"            else:\n",
					"                # File is good - add to valid files\n",
					"                print(f\"   ✅ {file_name}: OK\")\n",
					"                valid_files.append(file_path)\n",
					"        else:\n",
					"            # Not CSV/Excel - just add to valid files\n",
					"            valid_files.append(file_path)\n",
					"    \n",
					"    print_step2_results(valid_files, moved_files)\n",
					"    # ========================================================================\n",
					"    # STEP 3: Create file index (only for valid, non-quarantined files)\n",
					"    # ========================================================================\n",
					"    print(\"\\n\" + \"=\" * 70)\n",
					"    print(\"STEP 3: CREATING FILE INDEX\")\n",
					"    print(\"=\" * 70)\n",
					"    print(\"📋 Indexing valid files...\")\n",
					"    \n",
					"    file_index = {\n",
					"        \"created_at\": time.strftime(\"%Y%m%d%H%M%S\"),\n",
					"        \"files\": []\n",
					"    }\n",
					"    \n",
					"    for file_path in valid_files:\n",
					"        try:\n",
					"            file_name = os.path.basename(file_path)\n",
					"            date_folder, la_folder, confidence = get_folder_parts(file_path)\n",
					"            \n",
					"            file_info = {\n",
					"                \"path\": file_path,\n",
					"                \"file_name\": file_name,\n",
					"                \"date_folder\": date_folder,\n",
					"                \"la_folder\": la_folder,\n",
					"                \"confidence\": confidence,\n",
					"                \"file_type\": get_file_type(file_path),\n",
					"                \"is_empty\": False,  # Already checked\n",
					"                \"is_metadata\": is_metadata_file(file_path),\n",
					"                \"should_process\": should_process_file(file_path),\n",
					"                \"processed\": False\n",
					"            }\n",
					"            file_index[\"files\"].append(file_info)\n",
					"        except Exception as e:\n",
					"            print(f\"   ⚠️  Error indexing {file_name}: {str(e)}\")\n",
					"    \n",
					"    print(f\"   Indexed {len(file_index['files'])} files\")\n",
					"    \n",
					"    # ========================================================================\n",
					"    # STEP 4: Filter by year (2025)\n",
					"    # ========================================================================\n",
					"    print(\"\\n\" + \"=\" * 70)\n",
					"    print(f\"STEP 4: FILTERING BY YEAR ({year_filter})\")\n",
					"    print(\"=\" * 70)\n",
					"    \n",
					"    files_in_year = []\n",
					"    files_wrong_year = []\n",
					"    \n",
					"    for file_info in file_index[\"files\"]:\n",
					"        if is_file_in_year(file_info[\"path\"], year_filter, prefix=sink_path):\n",
					"            files_in_year.append(file_info)\n",
					"        else:\n",
					"            files_wrong_year.append(file_info[\"path\"])\n",
					"            print(f\"   ⏭️  Skipping (not {year_filter}): {file_info['file_name']}\")\n",
					"    \n",
					"    print(f\"\\n📊 Year filter results:\")\n",
					"    print(f\"   Files in {year_filter}: {len(files_in_year)}\")\n",
					"    print(f\"   Files skipped (wrong year): {len(files_wrong_year)}\")\n",
					"    \n",
					"    # ========================================================================\n",
					"    # STEP 5: Filter by processed log (new files only)\n",
					"    # ========================================================================\n",
					"    print(\"\\n\" + \"=\" * 70)\n",
					"    print(\"STEP 5: FILTERING NEW FILES (NOT ALREADY PROCESSED)\")\n",
					"    print(\"=\" * 70)\n",
					"    \n",
					"    # Read processed files log\n",
					"    processed_files_set = read_processed_files_log(storage_account)\n",
					"    \n",
					"    new_files = []\n",
					"    already_processed = []\n",
					"    \n",
					"    for file_info in files_in_year:\n",
					"        file_path = file_info[\"path\"]\n",
					"        norm_path = normalize_file_path(file_path)\n",
					"        \n",
					"        if debug_mode:\n",
					"            # Debug mode: process all\n",
					"            new_files.append(file_info)\n",
					"        elif file_path.lower() in processed_files_set or norm_path in processed_files_set:\n",
					"            # Already processed - skip\n",
					"            already_processed.append(file_path)\n",
					"            print(f\"   ⏭️  Already processed: {file_info['file_name']}\")\n",
					"        else:\n",
					"            # New file - process it\n",
					"            new_files.append(file_info)\n",
					"            print(f\"   ✨ NEW: {file_info['file_name']}\")\n",
					"    \n",
					"    print(f\"\\n📊 New file filter results:\")\n",
					"    print(f\"   New files to process: {len(new_files)}\")\n",
					"    print(f\"   Already processed (skipped): {len(already_processed)}\")\n",
					"    \n",
					"    if len(new_files) == 0:\n",
					"        print(\"\\n✅ No new files to process\")\n",
					"        print_quarantine_summary(moved_files)\n",
					"        return\n",
					"    \n",
					"    # ========================================================================\n",
					"    # STEP 6: Group files for processing\n",
					"    # ========================================================================\n",
					"    print(\"\\n\" + \"=\" * 70)\n",
					"    print(\"STEP 6: GROUPING FILES FOR PROCESSING\")\n",
					"    print(\"=\" * 70)\n",
					"    \n",
					"    file_groups = {}\n",
					"    individual_files = []\n",
					"    \n",
					"    for file_info in new_files:\n",
					"        # Files to process individually\n",
					"        if (file_info[\"is_metadata\"] or \n",
					"            file_info[\"confidence\"] == \"very_low\" or \n",
					"            not file_info[\"should_process\"]):\n",
					"            individual_files.append(file_info)\n",
					"        # Files to group by date/LA\n",
					"        elif file_info[\"date_folder\"] and file_info[\"la_folder\"]:\n",
					"            group_key = (file_info[\"date_folder\"], file_info[\"la_folder\"])\n",
					"            if group_key not in file_groups:\n",
					"                file_groups[group_key] = []\n",
					"            file_groups[group_key].append(file_info)\n",
					"        else:\n",
					"            individual_files.append(file_info)\n",
					"    \n",
					"    print(f\"   File groups: {len(file_groups)}\")\n",
					"    print(f\"   Individual files: {len(individual_files)}\")\n",
					"    \n",
					"    # ========================================================================\n",
					"    # STEP 7: Process files\n",
					"    # ========================================================================\n",
					"    print(\"\\n\" + \"=\" * 70)\n",
					"    print(\"STEP 7: PROCESSING FILES\")\n",
					"    print(\"=\" * 70)\n",
					"    \n",
					"    successfully_processed_files = []\n",
					"    \n",
					"    # Process individual files\n",
					"    if individual_files:\n",
					"        print(f\"\\n⚙️  Processing {len(individual_files)} individual files...\")\n",
					"        for i, file_info in enumerate(individual_files, 1):\n",
					"            try:\n",
					"                print(f\"\\n[{i}/{len(individual_files)}] {file_info['file_name']}\")\n",
					"                success = process_individual_file(file_info, storage_account, moved_files)\n",
					"                \n",
					"                if success and verify_file_processed(file_info[\"path\"], moved_files):\n",
					"                    successfully_processed_files.append(file_info[\"path\"])\n",
					"            except Exception as e:\n",
					"                print(f\"   ❌ Error: {str(e)}\")\n",
					"                moved_files['failed'].append(file_info[\"path\"])\n",
					"    \n",
					"    # Process file groups\n",
					"    if file_groups:\n",
					"        print(f\"\\n⚙️  Processing {len(file_groups)} file groups...\")\n",
					"        for idx, ((date_folder, la_folder), file_group) in enumerate(file_groups.items(), 1):\n",
					"            try:\n",
					"                group_id = f\"{date_folder}_{la_folder}\"\n",
					"                print(f\"\\n[{idx}/{len(file_groups)}] {group_id} ({len(file_group)} files)\")\n",
					"                \n",
					"                success, path = process_file_group(file_group, date_folder, la_folder, storage_account, moved_files)\n",
					"                \n",
					"                if success:\n",
					"                    for file_info in file_group:\n",
					"                        successfully_processed_files.append(file_info[\"path\"])\n",
					"                else:\n",
					"                    # Fallback to individual\n",
					"                    for file_info in file_group:\n",
					"                        try:\n",
					"                            success = process_individual_file(file_info, storage_account, moved_files)\n",
					"                            if success and verify_file_processed(file_info[\"path\"], moved_files):\n",
					"                                successfully_processed_files.append(file_info[\"path\"])\n",
					"                        except:\n",
					"                            moved_files['failed'].append(file_info[\"path\"])\n",
					"            except Exception as e:\n",
					"                print(f\"   ❌ Error: {str(e)}\")\n",
					"    \n",
					"    # ========================================================================\n",
					"    # STEP 8: Update processed files log\n",
					"    # ========================================================================\n",
					"    if not debug_mode and len(successfully_processed_files) > 0:\n",
					"        print(\"\\n\" + \"=\" * 70)\n",
					"        print(\"STEP 8: UPDATING PROCESSED FILES LOG\")\n",
					"        print(\"=\" * 70)\n",
					"        update_processed_files_log(storage_account, log_file_path, successfully_processed_files)\n",
					"    \n",
					"    # ========================================================================\n",
					"    # FINAL SUMMARY\n",
					"    # ========================================================================\n",
					"    print(\"\\n\" + \"=\" * 70)\n",
					"    print(\"PROCESSING COMPLETE\")\n",
					"    print(\"=\" * 70)\n",
					"    print(f\"📊 Files scanned: {len(all_files)}\")\n",
					"    print(f\"✅ Files processed: {len(successfully_processed_files)}\")\n",
					"    print(f\"📦 ZIPs extracted: {len(moved_files.get('zip_extracted', []))}\")\n",
					"    print(f\"❌ Files quarantined: {len(moved_files['quarantine'])}\")\n",
					"    print(f\"⏭️  Files skipped (wrong year): {len(files_wrong_year)}\")\n",
					"    print(f\"⏭️  Files skipped (already processed): {len(already_processed)}\")\n",
					"    print(\"=\" * 70)\n",
					"    \n",
					"    # Show quarantine details\n",
					"    print_quarantine_summary(moved_files)\n",
					"\n",
					"\n",
					"\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if __name__ == \"__main__\":\n",
					"    main()"
				],
				"execution_count": null
			}
		]
	}
}