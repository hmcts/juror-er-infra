{
	"name": "L1_Er_Juror_Ingestion_Files",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b695ffd8-941d-4104-a95a-119fbb2d3e6e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import logging\n",
					"import os\n",
					"import json\n",
					"from datetime import datetime\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import collect_list, concat_ws\n",
					"from pyspark.sql.types import StringType, StructType, StructField\n",
					"from notebookutils import mssparkutils"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"\n",
					"# Configure logging\n",
					"logging.basicConfig(\n",
					"    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
					")\n",
					"\n",
					"def create_spark_session():\n",
					"    \"\"\"Create and return a Spark session\"\"\"\n",
					"    return SparkSession.builder.appName(\"Electoral Data Ingestion\").getOrCreate()\n",
					"\n",
					"def ensure_folder_exists(folder_path):\n",
					"    \"\"\"Create folder if it doesn't exist\"\"\"\n",
					"    try:\n",
					"        if not mssparkutils.fs.exists(folder_path):\n",
					"            logging.info(f\"Creating folder: {folder_path}\")\n",
					"            mssparkutils.fs.mkdirs(folder_path)\n",
					"            return True\n",
					"        return True\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error creating folder {folder_path}: {str(e)}\")\n",
					"        return False\n",
					"\n",
					"def list_files(folder_path, extensions):\n",
					"    \"\"\"List all files with specific extensions in a folder\"\"\"\n",
					"    try:\n",
					"        # Check if folder exists\n",
					"        if not mssparkutils.fs.exists(folder_path):\n",
					"            logging.warning(f\"Folder does not exist: {folder_path}\")\n",
					"            return []\n",
					"\n",
					"        files = mssparkutils.fs.ls(folder_path)\n",
					"        return [\n",
					"            f.path\n",
					"            for f in files\n",
					"            if any(f.name.lower().endswith(ext) for ext in extensions)\n",
					"        ]\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error listing files in {folder_path}: {str(e)}\")\n",
					"        return []\n",
					"\n",
					"def read_processed_files_log(spark, log_file_path):\n",
					"    \"\"\"Read the log of processed files with error handling\"\"\"\n",
					"    try:\n",
					"        if not mssparkutils.fs.exists(log_file_path):\n",
					"            logging.info(f\"Processed files log does not exist yet: {log_file_path}\")\n",
					"            return []\n",
					"\n",
					"        processed_files_log = (\n",
					"            spark.read.json(log_file_path)\n",
					"            .select(\"file\")\n",
					"            .rdd.flatMap(lambda x: x)\n",
					"            .collect()\n",
					"        )\n",
					"        logging.info(\n",
					"            f\"Read processed files log with {len(processed_files_log)} entries\"\n",
					"        )\n",
					"        return processed_files_log\n",
					"    except Exception as e:\n",
					"        logging.warning(f\"Could not read processed files log: {str(e)}\")\n",
					"        return []\n",
					"\n",
					"def update_processed_files_log(spark, log_file_path, processed_files_log, new_files):\n",
					"    \"\"\"Update the log of processed files\"\"\"\n",
					"    try:\n",
					"        # Create destination folder if it doesn't exist\n",
					"        log_folder = os.path.dirname(log_file_path)\n",
					"        if not mssparkutils.fs.exists(log_folder):\n",
					"            mssparkutils.fs.mkdirs(log_folder)\n",
					"            logging.info(f\"Created log folder: {log_folder}\")\n",
					"\n",
					"        # Create DataFrame for new files\n",
					"        new_processed_files_log = spark.createDataFrame(\n",
					"            [(file,) for file in new_files], [\"file\"]\n",
					"        )\n",
					"\n",
					"        # Merge with existing log if it exists\n",
					"        if processed_files_log:\n",
					"            existing_log_df = spark.createDataFrame(\n",
					"                [(file,) for file in processed_files_log], [\"file\"]\n",
					"            )\n",
					"            updated_log_df = existing_log_df.union(new_processed_files_log).distinct()\n",
					"        else:\n",
					"            updated_log_df = new_processed_files_log\n",
					"\n",
					"        # Write updated log\n",
					"        updated_log_df.write.mode(\"overwrite\").json(log_file_path)\n",
					"        logging.info(f\"Updated processed files log with {len(new_files)} new entries\")\n",
					"        return True\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error updating processed files log: {str(e)}\")\n",
					"        return False\n",
					"\n",
					"def read_column_mapping(spark, config_path):\n",
					"    \"\"\"Read and parse the column mapping configuration\"\"\"\n",
					"    try:\n",
					"        # Check if file exists\n",
					"        if not mssparkutils.fs.exists(config_path):\n",
					"            raise FileNotFoundError(f\"Column mapping file not found: {config_path}\")\n",
					"\n",
					"        try:\n",
					"            # Method 1: Using text file reader\n",
					"            mapping_df = spark.read.text(config_path)\n",
					"            json_str = mapping_df.agg(concat_ws(\"\", collect_list(\"value\"))).collect()[\n",
					"                0\n",
					"            ][0]\n",
					"            mapping_dict = json.loads(json_str)\n",
					"            return mapping_dict[\"mappings\"]\n",
					"        except Exception as e:\n",
					"            logging.warning(f\"Error with method 1: {str(e)}\")\n",
					"\n",
					"            # Method 2: Using wholeTextFiles\n",
					"            mapping_rdd = spark.sparkContext.wholeTextFiles(config_path)\n",
					"            json_str = mapping_rdd.values().first()\n",
					"            mapping_dict = json.loads(json_str)\n",
					"            return mapping_dict[\"mappings\"]\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error reading mapping file: {str(e)}\")\n",
					"        # Return a default mapping to prevent complete failure\n",
					"        return {}\n",
					"\n",
					"def ingest_files():\n",
					"    \"\"\"Main function for the ingestion process\"\"\"\n",
					"    # Initialize Spark session\n",
					"    spark = create_spark_session()\n",
					"\n",
					"    # Define paths\n",
					"    storage_account = \"baubaisadfsastg\"\n",
					"    files_folder = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/files\"\n",
					"    config_folder = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/config/schema\"\n",
					"    transform_folder = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/transformation\"\n",
					"    error_folder = f\"{transform_folder}/errors\"\n",
					"    staging_folder = f\"{transform_folder}/staging\"\n",
					"    log_file_path = f\"{transform_folder}/processed_files_log.json\"\n",
					"\n",
					"    # Check for inconsistent state (log exists but folder doesn't)\n",
					"    transform_folder_exists = mssparkutils.fs.exists(transform_folder)\n",
					"    log_file_exists = mssparkutils.fs.exists(log_file_path)\n",
					"    \n",
					"    if not transform_folder_exists and log_file_exists:\n",
					"        logging.warning(\"Found inconsistent state: transformation folder missing but log exists\")\n",
					"        try:\n",
					"            mssparkutils.fs.rm(log_file_path)\n",
					"            logging.info(\"Successfully removed stale log file\")\n",
					"        except Exception as e:\n",
					"            logging.error(f\"Failed to remove stale log file: {str(e)}\")\n",
					"\n",
					"    # Ensure required folders exist with validation\n",
					"    folder_status = {}\n",
					"    for folder in [files_folder, transform_folder, error_folder, staging_folder]:\n",
					"        result = ensure_folder_exists(folder)\n",
					"        folder_status[folder] = result\n",
					"        if not result:\n",
					"            logging.error(f\"Failed to create or verify folder: {folder}\")\n",
					"            print(f\"ERROR: Cannot access or create folder: {folder}\")\n",
					"            return []  # Exit if critical folders can't be created\n",
					"    \n",
					"    logging.info(f\"Folder status: {folder_status}\")\n",
					"    \n",
					"    # Check source folder content\n",
					"    try:\n",
					"        files_content = mssparkutils.fs.ls(files_folder)\n",
					"        logging.info(f\"Found {len(files_content)} items in source folder\")\n",
					"        if len(files_content) > 0:\n",
					"            logging.info(f\"Sample items: {[f.name for f in files_content[:3]]}\")\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error inspecting source folder: {str(e)}\")\n",
					"\n",
					"    # Read column mapping\n",
					"    mapping_file = f\"{config_folder}/col_mapping.json\"\n",
					"    try:\n",
					"        # Verify mapping file exists\n",
					"        if not mssparkutils.fs.exists(mapping_file):\n",
					"            logging.warning(f\"Mapping file not found at {mapping_file}\")\n",
					"            print(f\"WARNING: Mapping file not found at {mapping_file}\")\n",
					"            \n",
					"        column_mapping = read_column_mapping(spark, mapping_file)\n",
					"        if column_mapping:\n",
					"            logging.info(f\"Successfully read column mapping with {len(column_mapping)} mappings\")\n",
					"        else:\n",
					"            logging.warning(\"Column mapping is empty - check your configuration file\")\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error reading column mapping: {str(e)}\")\n",
					"        \n",
					"    # Read the log of processed files with validation\n",
					"    try:\n",
					"        processed_files_log = read_processed_files_log(spark, log_file_path)\n",
					"        logging.info(f\"Successfully read log with {len(processed_files_log)} entries\")\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error reading log, will start fresh: {str(e)}\")\n",
					"        processed_files_log = []\n",
					"\n",
					"    # List all files in the source folder with better error handling\n",
					"    try:\n",
					"        all_files = list_files(files_folder, [\".csv\", \".xlsx\", \".xls\"])\n",
					"        logging.info(f\"Found {len(all_files)} files to process\")\n",
					"        if len(all_files) == 0:\n",
					"            logging.warning(\"No files found in source directory - nothing to process\")\n",
					"            print(\"No files found in source directory.\")\n",
					"            return []\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error listing files: {str(e)}\")\n",
					"        print(f\"Error listing files: {str(e)}\")\n",
					"        return []\n",
					"\n",
					"    # Filter out files that have already been processed\n",
					"    new_files = [file for file in all_files if file not in processed_files_log]\n",
					"    logging.info(f\"New files to process: {len(new_files)}. Total files: {len(all_files)}\")\n",
					"    \n",
					"    if not new_files:\n",
					"        logging.info(\"No new files found - all files already processed\")\n",
					"        print(\"No new files to process - all files have been processed previously.\")\n",
					"        return []\n",
					"    \n",
					"    # Create a list of files to be processed in the next stage\n",
					"    result = {\n",
					"        \"new_files\": new_files,\n",
					"        \"processed_log\": processed_files_log,\n",
					"        \"column_mapping\": column_mapping,\n",
					"        \"paths\": {\n",
					"            \"files_folder\": files_folder,\n",
					"            \"transform_folder\": transform_folder, \n",
					"            \"staging_folder\": staging_folder,\n",
					"            \"error_folder\": error_folder,\n",
					"            \"log_file_path\": log_file_path\n",
					"        }\n",
					"    }\n",
					"    \n",
					"    # Save the file list for the next stage\n",
					"    staging_file = f\"{staging_folder}/ingestion_output.json\"\n",
					"    \n",
					"    # Convert data structure to DataFrame\n",
					"    result_df = spark.createDataFrame([(\n",
					"        json.dumps(result[\"new_files\"]),\n",
					"        json.dumps(result[\"processed_log\"]),\n",
					"        json.dumps(result[\"column_mapping\"]),\n",
					"        json.dumps(result[\"paths\"])\n",
					"    )], [\"new_files\", \"processed_log\", \"column_mapping\", \"paths\"])\n",
					"    \n",
					"    # Write to staging area\n",
					"    result_df.write.mode(\"overwrite\").json(staging_file)\n",
					"    \n",
					"    logging.info(f\"Saved ingestion results to {staging_file}\")\n",
					"    print(f\"Ingestion complete. {len(new_files)} new files ready for transformation.\")\n",
					"    \n",
					"    return new_files\n",
					"\n",
					"# Execute the function\n",
					"if __name__ == \"__main__\":\n",
					"    print(\"Starting ingestion process...\")\n",
					"    files = ingest_files()\n",
					"    print(f\"Ingestion process completed. Files identified: {len(files)}\")"
				],
				"execution_count": null
			}
		]
	}
}