{
	"name": "L1_MappingSchema",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkstg",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "f9897fbf-aa80-4ecd-a668-453b14994809"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/74dacd4f-a248-45bb-a2f0-af700dc4cf68/resourceGroups/baubais-data-factory-rg-stg/providers/Microsoft.Synapse/workspaces/baubais-synapse-stg/bigDataPools/sparkstg",
				"name": "sparkstg",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-stg.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkstg",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# L1 Mapping Schema - Electoral Data ETL\n",
					"\n",
					"## Purpose\n",
					"This notebook is part of an ETL pipeline for processing electoral data. It ensures data consistency, quality, and integration with Azure Data Lake Storage (ADLS) using Apache Spark.\n",
					"\n",
					"## Key Features\n",
					"- **Data Standardization**: Ensures consistent column names and data formats.\n",
					"- **Error Handling**: Logs detailed messages and handles errors gracefully.\n",
					"- **Distributed Processing**: Leverages Apache Spark for scalability.\n",
					"- **ADLS Integration**: Reads and writes files to Azure Data Lake Storage.\n",
					"- **Data Validation**: Validates critical fields to maintain data quality.\n",
					"\n",
					"## Workflow Overview\n",
					"1. **Initialization**: Sets up logging and Spark session.\n",
					"2. **File Management**: Ensures required folders exist and lists source files.\n",
					"3. **Column Mapping**: Reads JSON configuration for column mappings and applies transformations.\n",
					"4. **Data Transformation**: Cleans, standardizes, and validates data.\n",
					"5. **Output**: Writes transformed data to Parquet format and logs processed files.\n",
					"\n",
					"## Key Functions\n",
					"- `create_spark_session()`: Initializes a Spark session.\n",
					"- `ensure_folder_exists(folder_path)`: Ensures specified folders exist in ADLS.\n",
					"- `read_column_mapping(spark, config_path)`: Reads column mapping configuration.\n",
					"- `get_reverse_mapping(column_mapping)`: Creates a reverse mapping for standardizing column names.\n",
					"- `standardize_column_names(df, reverse_mapping)`: Standardizes column names in the DataFrame.\n",
					"- `ensure_required_fields(df)`: Ensures all required fields exist in the DataFrame with proper data types.\n",
					"- `combine_name_components(df)`: Combines initials and middle names into the `Elector Forename` column.\n",
					"- `enhanced_split_elector_name(df)`: Splits the `Elector Name` column into `Elector Surname`, `Elector Forename`, and `Elector Middlename`.\n",
					"- `clean_special_characters(df)`: Removes non-ASCII characters from name, address, and postcode fields.\n",
					"- `fix_numeric_fields(df)`: Ensures numeric fields are correctly formatted and not interpreted as dates.\n",
					"- `filter_records_with_empty_fields(df)`: Filters out records with empty critical fields or invalid characters.\n",
					"- `transform_data(df, creation_date, la_code)`: Cleans, transforms, and validates the data.\n",
					"- `filter_to_required_columns(df, column_mapping, keep_unmapped)`: Filters the DataFrame to include only required columns.\n",
					"- `process_file(spark, source_path, reverse_mapping, transform_folder, error_folder, column_mapping)`: Processes individual files and applies transformations.\n",
					"- `read_processed_files_log(spark, log_file_path)`: Reads the log of previously processed files.\n",
					"- `update_processed_files_log(spark, log_file_path, processed_files_log, new_files)`: Updates the log of processed files.\n",
					"- `print_summary(results)`: Prints a summary of the processing results.\n",
					"- `main()`: Orchestrates the ETL workflow.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import logging\n",
					"import os\n",
					"import json\n",
					"from datetime import datetime\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import (\n",
					"    col, lit, when, upper, trim, split, element_at, size, \n",
					"    concat, concat_ws, regexp_replace, regexp_extract, length, \n",
					"    collect_list, greatest, expr\n",
					")\n",
					"from pyspark.sql.types import StringType, StructType, StructField\n",
					"from notebookutils import mssparkutils"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"# Configure logging\n",
					"logging.basicConfig(\n",
					"    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
					")"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"def create_spark_session():\n",
					"    \"\"\"Create and return a Spark session with Excel support\"\"\"\n",
					"    return SparkSession.builder \\\n",
					"        .appName(\"Electoral Data ETL\") \\\n",
					"        .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.7\") \\\n",
					"        .config(\"spark.executor.memory\", \"4g\") \\\n",
					"        .config(\"spark.driver.memory\", \"4g\") \\\n",
					"        .getOrCreate()"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"def ensure_folder_exists(folder_path):\n",
					"    \"\"\"Create folder if it doesn't exist\"\"\"\n",
					"    try:\n",
					"        if not mssparkutils.fs.exists(folder_path):\n",
					"            logging.info(f\"Creating folder: {folder_path}\")\n",
					"            mssparkutils.fs.mkdirs(folder_path)\n",
					"            return True\n",
					"        return True\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error creating folder {folder_path}: {str(e)}\")\n",
					"        return False"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"source": [
					"def read_column_mapping(spark, config_path):\n",
					"    \"\"\"Read and parse the column mapping configuration\"\"\"\n",
					"    try:\n",
					"        # Check if file exists\n",
					"        if not mssparkutils.fs.exists(config_path):\n",
					"            raise FileNotFoundError(f\"Column mapping file not found: {config_path}\")\n",
					"\n",
					"        try:\n",
					"            # Method 1: Using text file reader\n",
					"            mapping_df = spark.read.text(config_path)\n",
					"            json_str = mapping_df.agg(concat_ws(\"\", collect_list(\"value\"))).collect()[\n",
					"                0\n",
					"            ][0]\n",
					"            mapping_dict = json.loads(json_str)\n",
					"            return mapping_dict[\"mappings\"]\n",
					"        except Exception as e:\n",
					"            logging.warning(f\"Error with method 1: {str(e)}\")\n",
					"\n",
					"            # Method 2: Using wholeTextFiles\n",
					"            mapping_rdd = spark.sparkContext.wholeTextFiles(config_path)\n",
					"            json_str = mapping_rdd.values().first()\n",
					"            mapping_dict = json.loads(json_str)\n",
					"            return mapping_dict[\"mappings\"]\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error reading mapping file: {str(e)}\")\n",
					"        # Return a default mapping to prevent complete failure\n",
					"        return {}"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"def get_reverse_mapping(column_mapping):\n",
					"    \"\"\"Create a reverse mapping for easy lookup\"\"\"\n",
					"    reverse_map = {}\n",
					"\n",
					"    for standard_name, variations in column_mapping.items():\n",
					"        # Add the standard name itself (case-insensitive mapping)\n",
					"        reverse_map[standard_name.lower()] = standard_name\n",
					"\n",
					"        # Handle different mapping formats\n",
					"        if isinstance(variations, dict) and \"aliases\" in variations:\n",
					"            # Format with aliases key\n",
					"            for alias in variations[\"aliases\"]:\n",
					"                reverse_map[alias.lower()] = standard_name\n",
					"        elif isinstance(variations, list):\n",
					"            # Direct array format (like in col_mapping.json)\n",
					"            for variation in variations:\n",
					"                reverse_map[variation.lower()] = standard_name\n",
					"\n",
					"    return reverse_map"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"source": [
					"def list_files(folder_path, extensions=None):\n",
					"    \"\"\"List all Parquet and Excel files in a folder\"\"\"\n",
					"    try:\n",
					"        if extensions is None:\n",
					"           extensions = ['.parquet', '.xlsx', '.xls']  # Include Excel files\n",
					"            \n",
					"        files = mssparkutils.fs.ls(folder_path)\n",
					"        return [f.path for f in files if any(f.name.lower().endswith(ext) for ext in extensions)]\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error listing files: {str(e)}\")\n",
					"        return []"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"source": [
					"def read_excel_file(spark, file_path):\n",
					"    \"\"\"Read an Excel file using pandas and convert to Spark DataFrame\"\"\"\n",
					"    import pandas as pd\n",
					"    import tempfile\n",
					"    import os\n",
					"    \n",
					"    # Create a temporary directory\n",
					"    with tempfile.TemporaryDirectory() as temp_dir:\n",
					"        # Create a temporary file path\n",
					"        temp_path = os.path.join(temp_dir, \"temp_file.xlsx\")\n",
					"        \n",
					"        # Download the file from ADLS\n",
					"        mssparkutils.fs.cp(file_path, f\"file://{temp_path}\")\n",
					"        \n",
					"        # Read with pandas\n",
					"        logging.info(f\"Reading Excel file using pandas: {file_path}\")\n",
					"        excel_df = pd.read_excel(temp_path)\n",
					"        \n",
					"        # Convert datetime columns to string to avoid Arrow conversion issues\n",
					"        for col in excel_df.select_dtypes(include=['datetime64']).columns:\n",
					"            excel_df[col] = excel_df[col].astype(str)\n",
					"        \n",
					"        # Convert to Spark DataFrame\n",
					"        return spark.createDataFrame(excel_df)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"source": [
					"def read_file(spark, file_path):\n",
					"    \"\"\"Read a file based on its extension\"\"\"\n",
					"    file_ext = os.path.splitext(file_path.lower())[1]\n",
					"    \n",
					"    if file_ext == '.parquet':\n",
					"        logging.info(f\"Reading Parquet file: {file_path}\")\n",
					"        return spark.read.parquet(file_path)\n",
					"    elif file_ext in ['.xlsx', '.xls']:\n",
					"        logging.info(f\"Reading Excel file with pandas: {file_path}\")\n",
					"        return read_excel_file(spark, file_path)  # Use the function from above\n",
					"    else:\n",
					"        raise ValueError(f\"Unsupported file type: {file_ext}\")"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"source": [
					"def standardize_column_names(df, reverse_mapping):\n",
					"    \"\"\"Standardize column names using the mapping\"\"\"\n",
					"    if not reverse_mapping:\n",
					"        logging.warning(\"Empty reverse mapping provided - cannot standardize columns\")\n",
					"        return df\n",
					"\n",
					"    renamed_columns = []\n",
					"\n",
					"    for col in df.columns:\n",
					"        std_name = reverse_mapping.get(col.lower())\n",
					"        if std_name:\n",
					"            df = df.withColumnRenamed(col, std_name)\n",
					"            renamed_columns.append(f\"{col} -> {std_name}\")\n",
					"\n",
					"    # Check if Flags/Markers exists after standardization, add if missing\n",
					"    if \"Flags/Markers\" not in df.columns:\n",
					"        logging.info(\"Adding missing 'Flags/Markers' column after standardization\")\n",
					"        df = df.withColumn(\"Flags/Markers\", lit(None).cast(StringType()))\n",
					"\n",
					"    if renamed_columns:\n",
					"        logging.info(\n",
					"            f\"Renamed {len(renamed_columns)} columns: {', '.join(renamed_columns[:5])}...\"\n",
					"        )\n",
					"    else:\n",
					"        logging.warning(\"No columns were renamed - check your mapping configuration\")\n",
					"\n",
					"    return df"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"source": [
					"def ensure_required_fields(df):\n",
					"    \"\"\"Ensure all required fields exist with proper data types\"\"\"\n",
					"    # Add Elector Title if it doesn't exist\n",
					"    if \"Elector Title\" not in df.columns and \"Title\" not in df.columns:\n",
					"        logging.info(\"Adding missing 'Elector Title' column\")\n",
					"        df = df.withColumn(\"Elector Title\", lit(None).cast(StringType()))\n",
					"    elif \"Title\" in df.columns and \"Elector Title\" not in df.columns:\n",
					"        logging.info(\"Renaming 'Title' to 'Elector Title'\")\n",
					"        df = df.withColumnRenamed(\"Title\", \"Elector Title\")\n",
					"\n",
					"    # Ensure other essential columns exist\n",
					"    required_columns = {\n",
					"        \"Elector Surname\": StringType(),\n",
					"        \"Elector Forename\": StringType(),\n",
					"        \"Elector DOB\": StringType(),\n",
					"        \"Address1\": StringType(),\n",
					"        \"PostCode\": StringType(),\n",
					"    }\n",
					"\n",
					"    for col_name, data_type in required_columns.items():\n",
					"        if col_name not in df.columns:\n",
					"            logging.info(f\"Adding missing '{col_name}' column\")\n",
					"            df = df.withColumn(col_name, lit(None).cast(data_type))\n",
					"\n",
					"    return df\n",
					""
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					"def combine_name_components(df):\n",
					"    \"\"\"Combine initials and middle names into Elector Forename\"\"\"\n",
					"    # Check if we have initials column\n",
					"    has_initials = False\n",
					"    initials_col = None\n",
					"\n",
					"    for col_name in [\"Initial\", \"Initials\"]:\n",
					"        if col_name in df.columns:\n",
					"            has_initials = True\n",
					"            initials_col = col_name\n",
					"            break\n",
					"\n",
					"    # Check if we have middle name column\n",
					"    has_middle_name = False\n",
					"    middle_name_col = None\n",
					"\n",
					"    for col_name in [\n",
					"        \"ElectorMiddleName\",\n",
					"        \"Elector Middlename\",\n",
					"        \"MiddleName\",\n",
					"        \"Middle Name\",\n",
					"    ]:\n",
					"        if col_name in df.columns:\n",
					"            has_middle_name = True\n",
					"            middle_name_col = col_name\n",
					"            break\n",
					"\n",
					"    # Combine components if needed\n",
					"    if \"Elector Forename\" in df.columns:\n",
					"        if has_initials:\n",
					"            # Add initials to forename (when both exist)\n",
					"            df = df.withColumn(\n",
					"                \"Elector Forename\",\n",
					"                when(\n",
					"                    (col(\"Elector Forename\").isNotNull())\n",
					"                    & (col(initials_col).isNotNull())\n",
					"                    & (length(trim(col(initials_col))) > 0),\n",
					"                    concat(col(\"Elector Forename\"), lit(\" \"), col(initials_col)),\n",
					"                ).otherwise(col(\"Elector Forename\")),\n",
					"            )\n",
					"\n",
					"            # Use initials as forename when forename is null\n",
					"            df = df.withColumn(\n",
					"                \"Elector Forename\",\n",
					"                when(\n",
					"                    (col(\"Elector Forename\").isNull())\n",
					"                    & (col(initials_col).isNotNull())\n",
					"                    & (length(trim(col(initials_col))) > 0),\n",
					"                    col(initials_col),\n",
					"                ).otherwise(col(\"Elector Forename\")),\n",
					"            )\n",
					"\n",
					"        if has_middle_name:\n",
					"            # Add middle name to forename (when both exist)\n",
					"            df = df.withColumn(\n",
					"                \"Elector Forename\",\n",
					"                when(\n",
					"                    (col(\"Elector Forename\").isNotNull())\n",
					"                    & (col(middle_name_col).isNotNull())\n",
					"                    & (length(trim(col(middle_name_col))) > 0),\n",
					"                    concat(col(\"Elector Forename\"), lit(\" \"), col(middle_name_col)),\n",
					"                ).otherwise(col(\"Elector Forename\")),\n",
					"            )\n",
					"\n",
					"            # Use middle name as forename when forename is null\n",
					"            df = df.withColumn(\n",
					"                \"Elector Forename\",\n",
					"                when(\n",
					"                    (col(\"Elector Forename\").isNull())\n",
					"                    & (col(middle_name_col).isNotNull())\n",
					"                    & (length(trim(col(middle_name_col))) > 0),\n",
					"                    col(middle_name_col),\n",
					"                ).otherwise(col(\"Elector Forename\")),\n",
					"            )\n",
					"\n",
					"    # Upper case and trim the final result\n",
					"    if \"Elector Forename\" in df.columns:\n",
					"        df = df.withColumn(\"Elector Forename\", upper(trim(col(\"Elector Forename\"))))\n",
					"\n",
					"    return df"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"source": [
					"def enhanced_split_elector_name(df):\n",
					"    \"\"\"Split elector name with improved handling of multiple forenames\"\"\"\n",
					"    if \"Elector Name\" in df.columns:\n",
					"        logging.info(\"Found 'Elector Name' column, performing enhanced name splitting\")\n",
					"\n",
					"        # First, create a backup of the original name\n",
					"        df = df.withColumn(\"Original_Name\", col(\"Elector Name\"))\n",
					"\n",
					"        # Split the name into parts\n",
					"        df = df.withColumn(\"name_parts\", split(col(\"Elector Name\"), \" \"))\n",
					"\n",
					"        # Extract surname (first part in UK electoral register format)\n",
					"        df = df.withColumn(\n",
					"            \"Elector Surname\",\n",
					"            when(\n",
					"                size(col(\"name_parts\")) > 0,\n",
					"                upper(trim(element_at(col(\"name_parts\"), lit(1)))),\n",
					"            ).otherwise(lit(None)),\n",
					"        )\n",
					"\n",
					"        # Extract primary forename (first name after surname)\n",
					"        df = df.withColumn(\n",
					"            \"Elector Forename\",\n",
					"            when(\n",
					"                size(col(\"name_parts\")) > 1,\n",
					"                upper(trim(element_at(col(\"name_parts\"), lit(2)))),\n",
					"            ).otherwise(lit(None)),\n",
					"        )\n",
					"\n",
					"        # Extract middle names (everything after the first forename)\n",
					"        df = df.withColumn(\n",
					"            \"Elector Middlename\",\n",
					"            when(\n",
					"                size(col(\"name_parts\")) > 2,\n",
					"                upper(\n",
					"                    trim(\n",
					"                        concat_ws(\n",
					"                            \" \",\n",
					"                            expr(f\"slice(name_parts, 3, greatest(1, size(name_parts) - 2))\")\n",
					"                        ),\n",
					"                    )\n",
					"                ),\n",
					"            ).otherwise(lit(None)),\n",
					"        )\n",
					"\n",
					"        # Drop temporary column\n",
					"        df = df.drop(\"name_parts\")\n",
					"\n",
					"        logging.info(\"Successfully performed enhanced name splitting\")\n",
					"\n",
					"    return df"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"source": [
					"def extract_numeric_la_code(df):\n",
					"    \"\"\"Extract only the numeric part from LA_Code\"\"\"\n",
					"    if \"LA_Code\" in df.columns:\n",
					"        logging.info(\"Extracting numeric part from LA_Code\")\n",
					"\n",
					"        # Extract only numeric characters from LA_Code\n",
					"        df = df.withColumn(\"LA_Code\", regexp_extract(col(\"LA_Code\"), \"([0-9]+)\", 1))\n",
					"\n",
					"        # If extraction results in empty string, set to null\n",
					"        df = df.withColumn(\n",
					"            \"LA_Code\", when(col(\"LA_Code\") == \"\", None).otherwise(col(\"LA_Code\"))\n",
					"        )\n",
					"\n",
					"        logging.info(\"LA_Code numeric extraction complete\")\n",
					"\n",
					"    return df"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"source": [
					"def clean_special_characters(df):\n",
					"    \"\"\"Clean special characters from name, address and postcode fields\"\"\"\n",
					"    logging.info(\"Cleaning special characters from name, address and postcode fields\")\n",
					"\n",
					"    # List of name columns to clean\n",
					"    name_cols = [\n",
					"        \"Elector Title\",\n",
					"        \"Elector Forename\",\n",
					"        \"Elector Surname\",\n",
					"        \"Elector Middlename\",\n",
					"        \"Suffix\",\n",
					"        \"Initials\",\n",
					"    ]\n",
					"\n",
					"    # Clean name fields first\n",
					"    for col_name in name_cols:\n",
					"        if col_name in df.columns:\n",
					"            # Replace non-ASCII characters and special characters like +\n",
					"            df = df.withColumn(\n",
					"                col_name,\n",
					"                regexp_replace(\n",
					"                    # First replace � and similar replacement characters\n",
					"                    regexp_replace(col(col_name), \"�\", \"\"),\n",
					"                    # Then replace other problematic characters including +\n",
					"                    r'[^a-zA-Z0-9\\-\\' ]',\n",
					"                    \"\"\n",
					"                )\n",
					"            )\n",
					"\n",
					"            # Trim any resulting double spaces\n",
					"            df = df.withColumn(col_name, trim(regexp_replace(col(col_name), \" +\", \" \")))\n",
					"\n",
					"    # List of address and postcode columns to clean\n",
					"    address_cols = [\n",
					"        \"Address1\",\n",
					"        \"Address2\",\n",
					"        \"Address3\",\n",
					"        \"Address4\",\n",
					"        \"Address5\",\n",
					"        \"Address6\",\n",
					"        \"PostCode\",\n",
					"    ]\n",
					"\n",
					"    for col_name in address_cols:\n",
					"        if col_name in df.columns:\n",
					"            # Replace non-ASCII characters with appropriate substitutions\n",
					"            df = df.withColumn(\n",
					"                col_name,\n",
					"                regexp_replace(\n",
					"                    # First replace � and similar replacement characters\n",
					"                    regexp_replace(col(col_name), \"�\", \"\"),\n",
					"                    # Then replace other problematic characters\n",
					"                    \"[^\\\\x00-\\\\x7f]\",\n",
					"                    \"\",\n",
					"                ),\n",
					"            )\n",
					"\n",
					"            # Trim any resulting double spaces\n",
					"            df = df.withColumn(col_name, trim(regexp_replace(col(col_name), \" +\", \" \")))\n",
					"\n",
					"            # For postcodes specifically, remove all non-alphanumeric characters except spaces\n",
					"            if col_name == \"PostCode\":\n",
					"                df = df.withColumn(\n",
					"                    col_name, regexp_replace(col(col_name), \"[^A-Z0-9 ]\", \"\")\n",
					"                )\n",
					"\n",
					"                # Ensure proper postcode format (if possible)\n",
					"                df = df.withColumn(\n",
					"                    col_name,\n",
					"                    when(\n",
					"                        # If it looks like a valid UK postcode pattern after cleaning\n",
					"                        col(col_name).rlike(\n",
					"                            \"^[A-Z]{1,2}[0-9][0-9A-Z]? ?[0-9][A-Z]{2}$\"\n",
					"                        ),\n",
					"                        # Ensure proper spacing (one space between outward and inward parts)\n",
					"                        regexp_replace(\n",
					"                            col(col_name),\n",
					"                            \"^([A-Z]{1,2}[0-9][0-9A-Z]?)[ ]*([0-9][A-Z]{2})$\",\n",
					"                            \"$1 $2\",\n",
					"                        ),\n",
					"                    ).otherwise(col(col_name)),\n",
					"                )\n",
					"\n",
					"    return df"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"source": [
					"def clean_character_data(df):\n",
					"    \"\"\"\n",
					"    Character cleaning function for L1 processing.\n",
					"    Cleans special characters and apostrophes in a single file before merging.\n",
					"    \n",
					"    Args:\n",
					"        df: Input DataFrame for a single file\n",
					"        \n",
					"    Returns:\n",
					"        DataFrame with cleaned character data\n",
					"    \"\"\"\n",
					"    from pyspark.sql.functions import col, when, regexp_replace, lit, trim, length\n",
					"    \n",
					"    # Track original counts for logging\n",
					"    original_count = df.count()\n",
					"    \n",
					"    # --- STEP 1: Clean apostrophes in names ---\n",
					"    if \"lname\" in df.columns:\n",
					"        df = df.withColumn(\n",
					"            \"lname\",\n",
					"            regexp_replace(col(\"lname\"), \"`\", \"'\")\n",
					"        )\n",
					"    \n",
					"    if \"fname\" in df.columns:\n",
					"        df = df.withColumn(\n",
					"            \"fname\",\n",
					"            regexp_replace(col(\"fname\"), \"`\", \"'\")\n",
					"        )\n",
					"    \n",
					"    # --- STEP 2: Clean control characters from address and name fields ---\n",
					"    address_name_cols = [\"lname\", \"fname\", \"address\", \"address2\", \"address3\", \n",
					"                        \"address4\", \"address5\", \"address6\"]\n",
					"    \n",
					"    for column in address_name_cols:\n",
					"        if column in df.columns:\n",
					"            # Remove newlines, carriage returns, and non-breaking spaces\n",
					"            df = df.withColumn(\n",
					"                column,\n",
					"                regexp_replace(\n",
					"                    regexp_replace(\n",
					"                        col(column), \n",
					"                        \"[\\r\\n]\", \"\"\n",
					"                    ),\n",
					"                    \"\\u00A0\", \" \"  # Replace non-breaking space\n",
					"                )\n",
					"            )\n",
					"            \n",
					"            # Remove other control characters\n",
					"            df = df.withColumn(\n",
					"                column,\n",
					"                regexp_replace(col(column), \"[^\\x20-\\x7E£]\", \"\")\n",
					"            )\n",
					"            \n",
					"            # Trim whitespace\n",
					"            df = df.withColumn(column, trim(col(column)))\n",
					"    \n",
					"    # --- STEP 3: Handle double quotes ---\n",
					"    for column in address_name_cols:\n",
					"        if column in df.columns and \"perm_disqual\" in df.columns:\n",
					"            df = df.withColumn(\n",
					"                \"perm_disqual\",\n",
					"                when(\n",
					"                    col(column).like(\"%\\\"%\") & col(\"perm_disqual\").isNull(),\n",
					"                    lit(\"Y\")\n",
					"                ).otherwise(col(\"perm_disqual\"))\n",
					"            )\n",
					"    \n",
					"    # --- STEP 4: Remove duplicate address text ---\n",
					"    if all(c in df.columns for c in [\"address4\", \"address5\"]):\n",
					"        df = df.withColumn(\n",
					"            \"address5\",\n",
					"            when(col(\"address4\") == col(\"address5\"), lit(None))\n",
					"            .otherwise(col(\"address5\"))\n",
					"        )\n",
					"    \n",
					"    # --- STEP 5: Handle names with numbers or invalid starting characters ---\n",
					"    for column in [\"lname\", \"fname\"]:\n",
					"        if column in df.columns and \"perm_disqual\" in df.columns:\n",
					"            # Mark records with numbers in names\n",
					"            df = df.withColumn(\n",
					"                \"perm_disqual\",\n",
					"                when(\n",
					"                    col(column).rlike(\"[0-9]\") & col(\"perm_disqual\").isNull(),\n",
					"                    lit(\"Y\")\n",
					"                ).otherwise(col(\"perm_disqual\"))\n",
					"            )\n",
					"            \n",
					"            # Mark records with invalid starting characters\n",
					"            invalid_starts = [\"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"-\"]\n",
					"            for char in invalid_starts:\n",
					"                df = df.withColumn(\n",
					"                    \"perm_disqual\",\n",
					"                    when(\n",
					"                        col(column).startswith(char) & col(\"perm_disqual\").isNull(),\n",
					"                        lit(\"Y\")\n",
					"                    ).otherwise(col(\"perm_disqual\"))\n",
					"                )\n",
					"    \n",
					"    # --- NEW STEP 6: Handle titles with numbers ---\n",
					"    if \"title\" in df.columns:\n",
					"        from pyspark.sql.functions import regexp_extract\n",
					"        \n",
					"        \n",
					"        if titles_with_numbers > 0:\n",
					"            print(f\"Found {titles_with_numbers} titles containing numbers - replacing with null\")\n",
					"            \n",
					"            # Replace titles containing numbers with null\n",
					"            df = df.withColumn(\n",
					"                \"title\",\n",
					"                when(\n",
					"                    col(\"title\").rlike(\"[0-9]\"),\n",
					"                    lit(None)\n",
					"                ).otherwise(col(\"title\"))\n",
					"            )\n",
					"        \n",
					"        #  Replace titles containing any invalid special characters with null\n",
					"        invalid_title_pattern = \"[@#$%&*+=<>?!~|\\\\\\\\/]\"\n",
					"        \n",
					"        for char in invalid_title_chars:\n",
					"            df = df.withColumn(\n",
					"                \"title\",\n",
					"                when(\n",
					"                    col(\"title\").rlike(invalid_title_pattern),\n",
					"                    lit(None)\n",
					"                ).otherwise(col(\"title\"))\n",
					"            )\n",
					"            \n",
					"        # Ensure title is trimmed\n",
					"        df = df.withColumn(\"title\", trim(col(\"title\")))\n",
					"    \n",
					"    return df"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"source": [
					"def fix_numeric_fields(df):\n",
					"    \"\"\"Fix numeric fields that might be interpreted as dates\"\"\"\n",
					"    numeric_columns = [\n",
					"        \"Elector Number\",\n",
					"        \"Poll number\",\n",
					"        \"poll_number\",\n",
					"        \"RollNo\",\n",
					"        \"Eno\",\n",
					"        \"ElectorNumber\",\n",
					"    ]\n",
					"\n",
					"    for col_name in numeric_columns:\n",
					"        if col_name in df.columns:\n",
					"            logging.info(f\"Fixing numeric format for column: {col_name}\")\n",
					"\n",
					"            # Check column data type first\n",
					"            col_type = df.schema[col_name].dataType\n",
					"            if isinstance(col_type, StringType):\n",
					"                # Only process string columns\n",
					"                df = df.withColumn(\n",
					"                    f\"{col_name}_temp\",\n",
					"                    when(\n",
					"                        # Check if value contains date separators\n",
					"                        regexp_extract(col(col_name), \"(-|/)\", 1) != \"\",\n",
					"                        # If it's a date format, extract only numeric part\n",
					"                        regexp_replace(col(col_name), \"[^0-9]\", \"\"),\n",
					"                    )\n",
					"                    .when(\n",
					"                        # For pure numeric strings, keep as is\n",
					"                        col(col_name).rlike(\"^[0-9]+$\"),\n",
					"                        col(col_name),\n",
					"                    )\n",
					"                    .otherwise(\n",
					"                        # For other formats, try to extract numeric part\n",
					"                        regexp_replace(col(col_name), \"[^0-9]\", \"\")\n",
					"                    ),\n",
					"                )\n",
					"\n",
					"                # Cast to integer if possible\n",
					"                df = df.withColumn(\n",
					"                    f\"{col_name}_temp\",\n",
					"                    when(\n",
					"                        col(f\"{col_name}_temp\").rlike(\"^[0-9]+$\"),\n",
					"                        col(f\"{col_name}_temp\").cast(\"int\"),\n",
					"                    ).otherwise(col(f\"{col_name}_temp\")),\n",
					"                )\n",
					"\n",
					"                # Replace original column with fixed version\n",
					"                df = df.drop(col_name).withColumnRenamed(f\"{col_name}_temp\", col_name)\n",
					"\n",
					"    return df"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"source": [
					"def process_name_components(df):\n",
					"    \"\"\"Process name components separately\"\"\"\n",
					"    name_columns = [\n",
					"        \"Elector Title\",\n",
					"        \"Elector Forename\",\n",
					"        \"Elector Surname\",\n",
					"        \"Elector Middlename\",\n",
					"        \"Suffix\",\n",
					"        \"Initials\",\n",
					"    ]\n",
					"\n",
					"    for col_name in name_columns:\n",
					"        if col_name in df.columns:\n",
					"            df = df.withColumn(\n",
					"                col_name,\n",
					"                when(col(col_name).isNotNull(), upper(trim(col(col_name)))).otherwise(\n",
					"                    None\n",
					"                ),\n",
					"            )\n",
					"\n",
					"    return df"
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"source": [
					"def get_folder_parts(file_path):\n",
					"    \"\"\"Extract folder information for naming with improved parsing\"\"\"\n",
					"    parts = file_path.split(\"/\")\n",
					"\n",
					"    try:\n",
					"        la_index = parts.index(\"LA_Data\")\n",
					"        if la_index > 0:\n",
					"            date_folder = parts[la_index - 1]\n",
					"            if len(parts) > la_index + 1:\n",
					"                # Get LA folder name (third folder)\n",
					"                third_folder = parts[la_index + 1]\n",
					"                return date_folder, third_folder\n",
					"    except ValueError:\n",
					"        pass\n",
					"\n",
					"    # Alternative parsing method if LA_Data not found\n",
					"    file_name = parts[-1]\n",
					"    if \"_\" in file_name:\n",
					"        parts = file_name.split(\"_\", 1)\n",
					"        if len(parts) == 2:\n",
					"            date_part = parts[0]\n",
					"            la_part = parts[1].split(\".\")[0]  # Remove extension\n",
					"            return date_part, la_part\n",
					"\n",
					"    # If all else fails, use current date and filename\n",
					"    current_date = datetime.now().strftime(\"%Y%m%d\")\n",
					"    filename_without_ext = os.path.splitext(os.path.basename(file_path))[0]\n",
					"    return current_date, filename_without_ext\n",
					""
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"source": [
					"def filter_records_with_empty_fields(df):\n",
					"    \"\"\"Filter out records where critical fields are empty or contain only special characters\"\"\"\n",
					"    logging.info(f\"Original record count: {df.count()}\")\n",
					"\n",
					"    # Define critical fields that should not be empty\n",
					"    critical_fields = [\n",
					"        \"Elector Number\",\n",
					"        \"Elector Surname\",\n",
					"        \"LA_Code\",\n",
					"    ]\n",
					"    \n",
					"    # Define address and postcode fields to check for special characters\n",
					"    address_fields = [\n",
					"        \"Elector Surname\",\n",
					"        \"PostCode\",\n",
					"        \"Address1\"\n",
					"    ]\n",
					"\n",
					"    # Filter out rows where critical fields are null or empty string\n",
					"    for field in critical_fields:\n",
					"        if field in df.columns:\n",
					"            initial_count = df.count()\n",
					"            df = df.filter((col(field).isNotNull()) & (length(trim(col(field))) > 0))\n",
					"            removed = initial_count - df.count()\n",
					"            logging.info(f\"Removed {removed} rows with empty {field}\")\n",
					"    \n",
					"    # Additional filtering for address fields to check for special characters only\n",
					"    for field in address_fields:\n",
					"        if field in df.columns:\n",
					"            initial_count = df.count()\n",
					"            \n",
					"            # Filter out records where the field contains only special characters\n",
					"            # This regex replaces all non-alphanumeric characters with empty string\n",
					"            # If the result is empty, the original string had only special characters\n",
					"            df = df.filter(\n",
					"                ~(col(field).isNotNull() & \n",
					"                  regexp_replace(col(field), \"[^a-zA-Z0-9]\", \"\").eqNullSafe(\"\"))\n",
					"            )\n",
					"            \n",
					"            removed = initial_count - df.count()\n",
					"            logging.info(f\"Removed {removed} rows with only special characters in {field}\")\n",
					"\n",
					"    logging.info(f\"Remaining record count after filtering: {df.count()}\")\n",
					"    return df"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"source": [
					"def transform_data(df, creation_date, la_code=None):\n",
					"    \"\"\"Transform data by cleaning text columns and handling names\"\"\"\n",
					"    # Clean text columns\n",
					"    for col_name in df.columns:\n",
					"        if df.schema[col_name].dataType == StringType():\n",
					"            df = df.withColumn(col_name, upper(trim(df[col_name])))\n",
					"\n",
					"    # Ensure all required fields exist\n",
					"    df = ensure_required_fields(df)\n",
					"\n",
					"    # Clean special characters in address and postcode fields\n",
					"    df = clean_special_characters(df)\n",
					"\n",
					"    # Fix numeric fields\n",
					"    df = fix_numeric_fields(df)\n",
					"\n",
					"    # Process name fields\n",
					"    df = enhanced_split_elector_name(df)\n",
					"    df = process_name_components(df)\n",
					"\n",
					"    # Combine name components (initials and middle names into forename)\n",
					"    df = combine_name_components(df)\n",
					"\n",
					"    # Add metadata\n",
					"    if creation_date:\n",
					"        df = df.withColumn(\"CreationDate\", lit(creation_date))\n",
					"\n",
					"    if la_code:\n",
					"        # Extract only numeric part from la_code\n",
					"        numeric_la_code = \"\".join(c for c in la_code if c.isdigit())\n",
					"        df = df.withColumn(\"LA_Code\", lit(numeric_la_code))\n",
					"\n",
					"    # Extract numeric part from LA_Code if it exists\n",
					"    df = extract_numeric_la_code(df)\n",
					"\n",
					"    # Filter out records with empty critical fields\n",
					"    df = filter_records_with_empty_fields(df)\n",
					"\n",
					"    return df"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"source": [
					"def filter_to_required_columns(df, column_mapping=None, keep_unmapped=False):\n",
					"    \"\"\"Filter columns based on mapping configuration\"\"\"\n",
					"    if not column_mapping:\n",
					"        logging.warning(\"No column mapping provided, returning original DataFrame\")\n",
					"        return df\n",
					"        \n",
					"    # Get all standard column names from the mapping\n",
					"    mapped_columns = list(column_mapping.keys())\n",
					"    \n",
					"    # Add essential metadata columns\n",
					"    essential_columns = [\"LA_Code\", \"CreationDate\"]\n",
					"    required_columns = mapped_columns + essential_columns\n",
					"    \n",
					"    if keep_unmapped:\n",
					"        # Keep all columns but reorder to put mapped columns first\n",
					"        existing_mapped = [col for col in required_columns if col in df.columns]\n",
					"        unmapped_columns = [col for col in df.columns if col not in required_columns]\n",
					"        ordered_columns = existing_mapped + unmapped_columns\n",
					"        return df.select(ordered_columns)\n",
					"    else:\n",
					"        # Keep only mapped columns\n",
					"        existing_columns = [col for col in required_columns if col in df.columns]\n",
					"        return df.select(existing_columns)\n",
					""
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"source": [
					"def process_file(spark, source_path, reverse_mapping, transform_folder, error_folder, column_mapping=None):\n",
					"    \"\"\"Process a source file and transform it to parquet format\"\"\"\n",
					"    # Print more detailed debugging info\n",
					"    print(f\"Debug - Starting to process file: {source_path}\")\n",
					"    print(f\"Debug - File exists check: {mssparkutils.fs.exists(source_path)}\")\n",
					"    \n",
					"    result = {\n",
					"        \"file\": source_path,\n",
					"        \"status\": \"error\",\n",
					"        \"message\": \"\",\n",
					"        \"output\": None,\n",
					"        \"timestamp\": datetime.now().isoformat(),\n",
					"    }\n",
					"\n",
					"    try:\n",
					"        # Create required folders if they don't exist\n",
					"        if not ensure_folder_exists(transform_folder):\n",
					"            result[\"message\"] = f\"Could not create transform folder: {transform_folder}\"\n",
					"            return result\n",
					"\n",
					"        if not ensure_folder_exists(error_folder):\n",
					"            result[\"message\"] = f\"Could not create error folder: {error_folder}\"\n",
					"            return result\n",
					"\n",
					"        # Get file details\n",
					"        file_name = os.path.basename(source_path)\n",
					"        date_folder, la_code = get_folder_parts(source_path)\n",
					"\n",
					"        logging.info(\n",
					"            f\"Processing file: {file_name}, Date: {date_folder}, LA: {la_code}\"\n",
					"        )\n",
					"\n",
					"        # Read the file\n",
					"        try:\n",
					"            df = read_file(spark, source_path)\n",
					"            logging.info(\n",
					"                f\"Successfully read file with {df.count()} rows and {len(df.columns)} columns\"\n",
					"            )\n",
					"        except Exception as e:\n",
					"            result[\"message\"] = f\"Error reading file: {str(e)}\"\n",
					"            return result\n",
					"\n",
					"        # Standardize column names using mapping\n",
					"        if reverse_mapping:\n",
					"            df = standardize_column_names(df, reverse_mapping)\n",
					"        else:\n",
					"            logging.warning(\n",
					"                \"No reverse mapping available - skipping column standardization\"\n",
					"            )\n",
					"\n",
					"        # Transform data\n",
					"        try:\n",
					"            # NEW: Add character cleaning step here\n",
					"            logging.info(\"Applying character cleaning...\")\n",
					"            df = clean_character_data(df)\n",
					"            logging.info(\"Character cleaning completed successfully\")\n",
					"            \n",
					"            # Continue with normal transformation\n",
					"            df = transform_data(df, date_folder, la_code)\n",
					"\n",
					"            # Validation checks\n",
					"            if \"Elector Number\" in df.columns:\n",
					"                numeric_count = df.filter(\n",
					"                    col(\"Elector Number\").cast(\"int\").isNotNull()\n",
					"                ).count()\n",
					"                total_count = df.count()\n",
					"                numeric_percentage = (\n",
					"                    (numeric_count / total_count) * 100 if total_count > 0 else 0\n",
					"                )\n",
					"                logging.info(\n",
					"                    f\"Elector Number field: {numeric_percentage:.2f}% are valid numbers ({numeric_count}/{total_count})\"\n",
					"                )\n",
					"\n",
					"            if \"LA_Code\" in df.columns:\n",
					"                logging.info(\n",
					"                    f\"LA_Code values sample: {[r.LA_Code for r in df.select('LA_Code').distinct().limit(5).collect()]}\"\n",
					"                )\n",
					"\n",
					"            logging.info(f\"Successfully transformed data with {df.count()} rows\")\n",
					"        except Exception as e:\n",
					"            result[\"message\"] = f\"Error transforming data: {str(e)}\"\n",
					"            return result\n",
					"\n",
					"        logging.info(\n",
					"            f\"Filtered to {len(df.columns)} required columns: {', '.join(df.columns)}\"\n",
					"        )\n",
					"        # Filter to required columns\n",
					"        df = filter_to_required_columns(df, column_mapping, keep_unmapped=True)\n",
					"\n",
					"        # Create a output name using date and LA code\n",
					"        output_name = f\"{date_folder}_{la_code}\"\n",
					"        output_path = f\"{transform_folder}/{os.path.splitext(file_name)[0]}.parquet\"\n",
					"        \n",
					"        # Write transformed data to parquet\n",
					"        try:\n",
					"            df.write.mode(\"overwrite\").parquet(output_path)\n",
					"            logging.info(f\"Successfully wrote parquet file to {output_path}\")\n",
					"\n",
					"            result[\"status\"] = \"success\"\n",
					"            result[\"message\"] = f\"Successfully processed file {file_name}\"\n",
					"            result[\"output\"] = output_path\n",
					"            return result\n",
					"        except Exception as e:\n",
					"            result[\"message\"] = f\"Error writing output file: {str(e)}\"\n",
					"            return result\n",
					"\n",
					"    except Exception as e:\n",
					"        result[\"message\"] = f\"Unexpected error: {str(e)}\"\n",
					"        return result"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"source": [
					"def read_processed_files_log(spark, log_file_path):\n",
					"    \"\"\"Read the log of processed files with error handling\"\"\"\n",
					"    try:\n",
					"        if not mssparkutils.fs.exists(log_file_path):\n",
					"            logging.info(f\"Processed files log does not exist yet: {log_file_path}\")\n",
					"            return []\n",
					"\n",
					"        processed_files_log = (\n",
					"            spark.read.json(log_file_path)\n",
					"            .select(\"file\")\n",
					"            .rdd.flatMap(lambda x: x)\n",
					"            .collect()\n",
					"        )\n",
					"        logging.info(\n",
					"            f\"Read processed files log with {len(processed_files_log)} entries\"\n",
					"        )\n",
					"        return processed_files_log\n",
					"    except Exception as e:\n",
					"        logging.warning(f\"Could not read processed files log: {str(e)}\")\n",
					"        return []"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"source": [
					"def update_processed_files_log(spark, log_file_path, processed_files_log, new_files):\n",
					"    \"\"\"Update the log of processed files\"\"\"\n",
					"    try:\n",
					"        # Create destination folder if it doesn't exist\n",
					"        log_folder = os.path.dirname(log_file_path)\n",
					"        if not mssparkutils.fs.exists(log_folder):\n",
					"            mssparkutils.fs.mkdirs(log_folder)\n",
					"            logging.info(f\"Created log folder: {log_folder}\")\n",
					"\n",
					"        # Create DataFrame for new files\n",
					"        new_processed_files_log = spark.createDataFrame(\n",
					"            [(file,) for file in new_files], [\"file\"]\n",
					"        )\n",
					"\n",
					"        # Merge with existing log if it exists\n",
					"        if processed_files_log:\n",
					"            existing_log_df = spark.createDataFrame(\n",
					"                [(file,) for file in processed_files_log], [\"file\"]\n",
					"            )\n",
					"            updated_log_df = existing_log_df.union(new_processed_files_log).distinct()\n",
					"        else:\n",
					"            updated_log_df = new_processed_files_log\n",
					"\n",
					"        # Write updated log\n",
					"        updated_log_df.write.mode(\"overwrite\").json(log_file_path)\n",
					"        logging.info(f\"Updated processed files log with {len(new_files)} new entries\")\n",
					"        return True\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error updating processed files log: {str(e)}\")\n",
					"        return False\n",
					""
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"source": [
					"def update_process_log(spark, new_file_paths, process_log_path):\n",
					"    \"\"\"Update the processing log with newly processed files\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    from pyspark.sql.types import StringType, TimestampType, StructType, StructField\n",
					"    \n",
					"    try:\n",
					"        # First, ensure the parent directory exists\n",
					"        parent_dir = \"/\".join(process_log_path.split(\"/\")[:-1])\n",
					"        if not mssparkutils.fs.exists(parent_dir):\n",
					"            print(f\"Creating log directory: {parent_dir}\")\n",
					"            mssparkutils.fs.mkdirs(parent_dir)\n",
					"            \n",
					"        # Create schema with explicit types\n",
					"        log_schema = StructType([\n",
					"            StructField(\"file_path\", StringType(), False),\n",
					"            StructField(\"process_timestamp\", StrigType(), False)\n",
					"        ])\n",
					"        \n",
					"        # Get current timestamp\n",
					"        from datetime import datetime\n",
					"        timestamp_str = datetime.now().isoformat()\n",
					"        \n",
					"        # Create new log entries\n",
					"        new_log_entries = [(file_path, timestamp_str) for file_path in new_file_paths]\n",
					"        \n",
					"        # Create DataFrame\n",
					"        new_log_df = spark.createDataFrame(new_log_entries, log_schema)\n",
					"        \n",
					"        # Update existing log if it exists\n",
					"        if mssparkutils.fs.exists(process_log_path):\n",
					"            try:\n",
					"                existing_log = spark.read.parquet(process_log_path)\n",
					"                updated_log = existing_log.union(new_log_df)\n",
					"            except Exception as e:\n",
					"                print(f\"Error reading existing log: {str(e)}. Creating new log.\")\n",
					"                updated_log = new_log_df\n",
					"        else:\n",
					"            updated_log = new_log_df\n",
					"            \n",
					"        # Write updated log\n",
					"        updated_log.write.mode(\"overwrite\").parquet(process_log_path)\n",
					"        return True\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"Error updating process log: {str(e)}\")\n",
					"        import traceback\n",
					"        print(traceback.format_exc())\n",
					"        return False"
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"source": [
					"def get_new_files(spark, input_path, process_log_path):\n",
					"    \"\"\"Identify which files are new and need processing\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    # Get all files in the input directory\n",
					"    all_files = []\n",
					"    \n",
					"    # List all items in the directory\n",
					"    try:\n",
					"        items = mssparkutils.fs.ls(input_path)\n",
					"        \n",
					"        # Log what we found for debugging\n",
					"        print(f\"Found {len(items)} items in {input_path}\")\n",
					"        \n",
					"        for item in items:\n",
					"            # Check if it's a file we should process\n",
					"            if (hasattr(item, 'path') and should_process_file(item.path)):\n",
					"                all_files.append(item.path)\n",
					"                print(f\"Found file for processing: {item.path}\")\n",
					"            elif (hasattr(item, 'path') and is_zip_file(item.path)):\n",
					"                all_files.append(item.path)\n",
					"                print(f\"Found ZIP file for processing: {item.path}\")\n",
					"    except Exception as e:\n",
					"        print(f\"Error listing files in {input_path}: {str(e)}\")\n",
					"    \n",
					"    # Read process log to get previously processed files\n",
					"    processed_files = []\n",
					"    if mssparkutils.fs.exists(process_log_path):\n",
					"        try:\n",
					"            processed_files = spark.read.parquet(process_log_path) \\\n",
					"                                .select(\"file_path\").rdd.flatMap(lambda x: x).collect()\n",
					"            print(f\"Found {len(processed_files)} previously processed files in log\")\n",
					"        except Exception as e:\n",
					"            print(f\"Error reading process log: {str(e)}\")\n",
					"            print(f\"Treating all files as new due to log error\")\n",
					"    \n",
					"    # Identify new files with more detailed logging\n",
					"    new_files = []\n",
					"    for f in all_files:\n",
					"        if f not in processed_files:\n",
					"            new_files.append(f)\n",
					"            print(f\"New file to process: {f}\")\n",
					"        else:\n",
					"            print(f\"Skipping already processed file: {f}\")\n",
					"    \n",
					"    print(f\"Found {len(new_files)} new files to process out of {len(all_files)} total files\")\n",
					"    return new_files, all_files"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"source": [
					"def print_summary(results):\n",
					"    \"\"\"Print a summary of processing results\"\"\"\n",
					"    successful = [r for r in results if r[\"status\"] == \"success\"]\n",
					"    failed = [r for r in results if r[\"status\"] == \"error\"]\n",
					"\n",
					"    print(\"\\n========== PROCESSING SUMMARY ==========\")\n",
					"    print(f\"Total files processed: {len(results)}\")\n",
					"    print(f\"Successfully processed: {len(successful)}\")\n",
					"    print(f\"Failed: {len(failed)}\")\n",
					"\n",
					"    if successful:\n",
					"        print(\"\\n----- Successfully Processed Files -----\")\n",
					"        for result in successful[:10]:  # Show first 10\n",
					"            print(f\"  - {result['file']} -> {result['output']}\")\n",
					"        if len(successful) > 10:\n",
					"            print(f\"  ... and {len(successful) - 10} more files\")\n",
					"\n",
					"    if failed:\n",
					"        print(\"\\n----- Failed Files -----\")\n",
					"        for result in failed[:10]:  # Show first 10\n",
					"            print(f\"  - {result['file']}: {result['message']}\")\n",
					"        if len(failed) > 10:\n",
					"            print(f\"  ... and {len(failed) - 10} more files\")\n",
					"\n",
					"    print(\"=======================================\\n\")"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"def main():\n",
					"    \"\"\"Main function for the transformation process\"\"\"\n",
					"    # Initialize Spark session\n",
					"    spark = create_spark_session()\n",
					"\n",
					"    # Define paths\n",
					"    storage_account = \"baubaisadfsastg\"\n",
					"    files_folder = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/files\"\n",
					"    config_folder = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/config/schema\"\n",
					"    transform_folder = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/transformation\"\n",
					"    error_folder = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/transformation_errors\"\n",
					"    log_file_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/L1_process_log/processed_files_log.json\"\n",
					"    summary_folder= f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/L1_process_log\"\n",
					"\n",
					"    # Check for inconsistent state (log exists but folder doesn't)\n",
					"    transform_folder_exists = mssparkutils.fs.exists(transform_folder)\n",
					"    log_file_exists = mssparkutils.fs.exists(log_file_path)\n",
					"    \n",
					"    if not transform_folder_exists and log_file_exists:\n",
					"        logging.warning(\"Found inconsistent state: transformation folder missing but log exists\")\n",
					"        try:\n",
					"            mssparkutils.fs.rm(log_file_path)\n",
					"            logging.info(\"Successfully removed stale log file\")\n",
					"        except Exception as e:\n",
					"            logging.error(f\"Failed to remove stale log file: {str(e)}\")\n",
					"\n",
					"    # Ensure required folders exist with validation\n",
					"    folder_status = {}\n",
					"    for folder in [files_folder, transform_folder, error_folder]:\n",
					"        result = ensure_folder_exists(folder)\n",
					"        folder_status[folder] = result\n",
					"        if not result:\n",
					"            logging.error(f\"Failed to create or verify folder: {folder}\")\n",
					"            print(f\"ERROR: Cannot access or create folder: {folder}\")\n",
					"            return []  # Exit if critical folders can't be created\n",
					"    \n",
					"    logging.info(f\"Folder status: {folder_status}\")\n",
					"    \n",
					"    # Check source folder content\n",
					"    try:\n",
					"        files_content = mssparkutils.fs.ls(files_folder)\n",
					"        logging.info(f\"Found {len(files_content)} items in source folder\")\n",
					"        if len(files_content) > 0:\n",
					"            logging.info(f\"Sample items: {[f.name for f in files_content[:3]]}\")\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error inspecting source folder: {str(e)}\")\n",
					"\n",
					"    # Read column mapping\n",
					"    mapping_file = f\"{config_folder}/col_mapping.json\"\n",
					"    try:\n",
					"        # Verify mapping file exists\n",
					"        if not mssparkutils.fs.exists(mapping_file):\n",
					"            logging.warning(f\"Mapping file not found at {mapping_file}\")\n",
					"            print(f\"WARNING: Mapping file not found at {mapping_file}\")\n",
					"            \n",
					"        column_mapping = read_column_mapping(spark, mapping_file)\n",
					"        if column_mapping:\n",
					"            logging.info(f\"Successfully read column mapping with {len(column_mapping)} mappings\")\n",
					"            reverse_mapping = get_reverse_mapping(column_mapping)\n",
					"            logging.info(f\"Created reverse mapping with {len(reverse_mapping)} entries\")\n",
					"        else:\n",
					"            logging.warning(\"Column mapping is empty - check your configuration file\")\n",
					"            reverse_mapping = {}\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error reading column mapping: {str(e)}\")\n",
					"        reverse_mapping = {}\n",
					"\n",
					"    # Read the log of processed files with validation\n",
					"    try:\n",
					"        processed_files_log = read_processed_files_log(spark, log_file_path)\n",
					"        logging.info(f\"Successfully read log with {len(processed_files_log)} entries\")\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error reading log, will start fresh: {str(e)}\")\n",
					"        processed_files_log = []\n",
					"\n",
					"    # List all files in the source folder with better error handling\n",
					"    try:\n",
					"        all_files = list_files(files_folder, ['.parquet', '.xlsx', '.xls'])\n",
					"        logging.info(f\"Found {len(all_files)} files to process\")\n",
					"        if len(all_files) == 0:\n",
					"            logging.warning(\"No files found in source directory - nothing to process\")\n",
					"            print(\"No files found in source directory.\")\n",
					"            \n",
					"            # Create empty summary to maintain consistency\n",
					"            empty_schema = StructType([\n",
					"                StructField(\"file\", StringType(), True),\n",
					"                StructField(\"status\", StringType(), True),\n",
					"                StructField(\"message\", StringType(), True),\n",
					"                StructField(\"output\", StringType(), True),\n",
					"                StructField(\"timestamp\", StringType(), True),\n",
					"            ])\n",
					"            empty_df = spark.createDataFrame([], empty_schema)\n",
					"            \n",
					"            # Ensure directory exists before writing\n",
					"            ensure_folder_exists(summary_folder)\n",
					"            empty_df.write.mode(\"overwrite\").json(f\"{summary_folder}/processing_summary.json\")\n",
					"            logging.info(\"Created empty processing summary\")\n",
					"            return []\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error listing files: {str(e)}\")\n",
					"        print(f\"Error listing files: {str(e)}\")\n",
					"        return []\n",
					"\n",
					"    # Filter out files that have already been processed\n",
					"    new_files = [file for file in all_files if file not in processed_files_log]\n",
					"    logging.info(f\"New files to process: {len(new_files)}\")\n",
					"    \n",
					"    if not new_files:\n",
					"        logging.info(\"No new files found - all files already processed\")\n",
					"        print(\"No new files to process - all files have been processed previously.\")\n",
					"        return []\n",
					"\n",
					"    # Process each new file with better progress reporting\n",
					"    results = []\n",
					"    total_files = len(new_files)\n",
					"    for i, file_path in enumerate(new_files):\n",
					"        file_num = i + 1\n",
					"        progress_pct = (file_num / total_files) * 100\n",
					"        logging.info(f\"Processing file {file_num}/{total_files} ({progress_pct:.1f}%): {file_path}\")\n",
					"        print(f\"Processing file {file_num}/{total_files} ({progress_pct:.1f}%): {os.path.basename(file_path)}\")\n",
					"        \n",
					"        # In the main function, where you call process_file\n",
					"        result = process_file(spark, file_path, reverse_mapping, transform_folder, error_folder, column_mapping)\n",
					"        results.append(result)\n",
					"        \n",
					"        status_emoji = \"✅\" if result[\"status\"] == \"success\" else \"❌\"\n",
					"        logging.info(f\"{status_emoji} Completed file {file_num}/{total_files}: {result['status']} - {result['message']}\")\n",
					"        print(f\"{status_emoji} Completed file {file_num}/{total_files}: {result['status']} - {result['message']}\")\n",
					"\n",
					"    # Create a summary DataFrame\n",
					"    if results:\n",
					"        try:\n",
					"            summary_df = spark.createDataFrame(results)\n",
					"            summary_df.write.mode(\"overwrite\").json(f\"{summary_folder}/processing_summary.json\")\n",
					"            logging.info(f\"Wrote processing summary to {summary_folder}/processing_summary.json\")\n",
					"        except Exception as e:\n",
					"            logging.error(f\"Error creating summary DataFrame: {str(e)}\")\n",
					"            print(f\"Error creating summary: {str(e)}\")\n",
					"    else:\n",
					"        # Handle case with no results\n",
					"        logging.warning(\"No files were processed - not creating summary\")\n",
					"        empty_schema = StructType([\n",
					"            StructField(\"file\", StringType(), True),\n",
					"            StructField(\"status\", StringType(), True),\n",
					"            StructField(\"message\", StringType(), True),\n",
					"            StructField(\"output\", StringType(), True),\n",
					"            StructField(\"timestamp\", StringType(), True),\n",
					"        ])\n",
					"        empty_df = spark.createDataFrame([], empty_schema)\n",
					"        empty_df.write.mode(\"overwrite\").json(f\"{summary_file_path}/processing_summary.json\")\n",
					"\n",
					"    # Print a summary of results\n",
					"    print_summary(results)\n",
					"\n",
					"    # Update the log of processed files\n",
					"    if new_files:\n",
					"        try:\n",
					"            update_result = update_processed_files_log(spark, log_file_path, processed_files_log, new_files)\n",
					"            if update_result:\n",
					"                logging.info(f\"Successfully updated processing log with {len(new_files)} new entries\")\n",
					"            else:\n",
					"                logging.warning(\"Failed to update processing log\")\n",
					"        except Exception as e:\n",
					"            logging.error(f\"Error updating log: {str(e)}\")\n",
					"\n",
					"    # Return the results for further processing\n",
					"    return results\n",
					"# Execute the main function with a clean run flag\n",
					"if __name__ == \"__main__\":\n",
					"    # Set to True to force reprocessing of all files (ignore log)\n",
					"    FORCE_CLEAN_RUN = True\n",
					"    \n",
					"    # Delete log file if forcing a clean run\n",
					"    if FORCE_CLEAN_RUN:\n",
					"        log_file_path = \"abfss://juror-etl@baubaisadfsastg.dfs.core.windows.net/L1_process_log/processed_files_log.json\"\n",
					"        if mssparkutils.fs.exists(log_file_path):\n",
					"            try:\n",
					"                mssparkutils.fs.rm(log_file_path, True)  # Add recursive=True parameter\n",
					"                print(\"Deleted existing log to force fresh processing\")\n",
					"            except Exception as e:\n",
					"                print(f\"Error deleting log file: {str(e)}\")\n",
					"    \n",
					"    # Run the main function\n",
					"    results = main()\n",
					"    print(\"\\nProcess completed.\")"
				],
				"execution_count": 29
			}
		]
	}
}