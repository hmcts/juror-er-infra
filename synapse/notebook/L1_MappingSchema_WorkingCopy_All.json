{
	"name": "L1_MappingSchema_WorkingCopy_All",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkstg",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "0ee8b56e-9147-4d6d-90aa-3e4f492c10a3"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/74dacd4f-a248-45bb-a2f0-af700dc4cf68/resourceGroups/baubais-data-factory-rg-stg/providers/Microsoft.Synapse/workspaces/baubais-synapse-stg/bigDataPools/sparkstg",
				"name": "sparkstg",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-stg.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkstg",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import logging\n",
					"import os\n",
					"import json\n",
					"from datetime import datetime\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import (\n",
					"    col, lit, when, upper, trim, split, element_at, size, \n",
					"    concat, concat_ws, regexp_replace, regexp_extract, length, \n",
					"    collect_list, greatest, expr\n",
					")\n",
					"from pyspark.sql.types import StringType, StructType, StructField\n",
					"from notebookutils import mssparkutils"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Configure logging\n",
					"logging.basicConfig(\n",
					"    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
					")\n",
					"\n",
					"\n",
					"def create_spark_session():\n",
					"    \"\"\"Create and return a Spark session\"\"\"\n",
					"    return SparkSession.builder.appName(\"Electoral Data ETL\").getOrCreate()\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"def ensure_folder_exists(folder_path):\n",
					"    \"\"\"Create folder if it doesn't exist\"\"\"\n",
					"    try:\n",
					"        if not mssparkutils.fs.exists(folder_path):\n",
					"            logging.info(f\"Creating folder: {folder_path}\")\n",
					"            mssparkutils.fs.mkdirs(folder_path)\n",
					"            return True\n",
					"        return True\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error creating folder {folder_path}: {str(e)}\")\n",
					"        return False"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def read_column_mapping(spark, config_path):\n",
					"    \"\"\"Read and parse the column mapping configuration\"\"\"\n",
					"    try:\n",
					"        # Check if file exists\n",
					"        if not mssparkutils.fs.exists(config_path):\n",
					"            raise FileNotFoundError(f\"Column mapping file not found: {config_path}\")\n",
					"\n",
					"        try:\n",
					"            # Method 1: Using text file reader\n",
					"            mapping_df = spark.read.text(config_path)\n",
					"            json_str = mapping_df.agg(concat_ws(\"\", collect_list(\"value\"))).collect()[\n",
					"                0\n",
					"            ][0]\n",
					"            mapping_dict = json.loads(json_str)\n",
					"            return mapping_dict[\"mappings\"]\n",
					"        except Exception as e:\n",
					"            logging.warning(f\"Error with method 1: {str(e)}\")\n",
					"\n",
					"            # Method 2: Using wholeTextFiles\n",
					"            mapping_rdd = spark.sparkContext.wholeTextFiles(config_path)\n",
					"            json_str = mapping_rdd.values().first()\n",
					"            mapping_dict = json.loads(json_str)\n",
					"            return mapping_dict[\"mappings\"]\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error reading mapping file: {str(e)}\")\n",
					"        # Return a default mapping to prevent complete failure\n",
					"        return {}"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def get_reverse_mapping(column_mapping):\n",
					"    \"\"\"Create a reverse mapping for easy lookup\"\"\"\n",
					"    reverse_map = {}\n",
					"\n",
					"    for standard_name, variations in column_mapping.items():\n",
					"        # Add the standard name itself (case-insensitive mapping)\n",
					"        reverse_map[standard_name.lower()] = standard_name\n",
					"\n",
					"        # Handle different mapping formats\n",
					"        if isinstance(variations, dict) and \"aliases\" in variations:\n",
					"            # Format with aliases key\n",
					"            for alias in variations[\"aliases\"]:\n",
					"                reverse_map[alias.lower()] = standard_name\n",
					"        elif isinstance(variations, list):\n",
					"            # Direct array format (like in col_mapping.json)\n",
					"            for variation in variations:\n",
					"                reverse_map[variation.lower()] = standard_name\n",
					"\n",
					"    return reverse_map"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def list_files(folder_path):\n",
					"    \"\"\"List all Parquet files in a folder\"\"\"\n",
					"    try:\n",
					"        files = mssparkutils.fs.ls(folder_path)\n",
					"        return [f.path for f in files if f.name.lower().endswith('.parquet')]\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error listing files: {str(e)}\")\n",
					"        return []"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def read_file(spark, file_path):\n",
					"    \"\"\"Read a Parquet file\"\"\"\n",
					"    logging.info(f\"Reading Parquet file: {file_path}\")\n",
					"    return spark.read.parquet(file_path)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def standardize_column_names(df, reverse_mapping):\n",
					"    \"\"\"Standardize column names using the mapping\"\"\"\n",
					"    if not reverse_mapping:\n",
					"        logging.warning(\"Empty reverse mapping provided - cannot standardize columns\")\n",
					"        return df\n",
					"\n",
					"    renamed_columns = []\n",
					"\n",
					"    for col in df.columns:\n",
					"        std_name = reverse_mapping.get(col.lower())\n",
					"        if std_name:\n",
					"            df = df.withColumnRenamed(col, std_name)\n",
					"            renamed_columns.append(f\"{col} -> {std_name}\")\n",
					"\n",
					"    # Check if Flags/Markers exists after standardization, add if missing\n",
					"    if \"Flags/Markers\" not in df.columns:\n",
					"        logging.info(\"Adding missing 'Flags/Markers' column after standardization\")\n",
					"        df = df.withColumn(\"Flags/Markers\", lit(None).cast(StringType()))\n",
					"\n",
					"    if renamed_columns:\n",
					"        logging.info(\n",
					"            f\"Renamed {len(renamed_columns)} columns: {', '.join(renamed_columns[:5])}...\"\n",
					"        )\n",
					"    else:\n",
					"        logging.warning(\"No columns were renamed - check your mapping configuration\")\n",
					"\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def ensure_required_fields(df):\n",
					"    \"\"\"Ensure all required fields exist with proper data types\"\"\"\n",
					"    # Add Elector Title if it doesn't exist\n",
					"    if \"Elector Title\" not in df.columns and \"Title\" not in df.columns:\n",
					"        logging.info(\"Adding missing 'Elector Title' column\")\n",
					"        df = df.withColumn(\"Elector Title\", lit(None).cast(StringType()))\n",
					"    elif \"Title\" in df.columns and \"Elector Title\" not in df.columns:\n",
					"        logging.info(\"Renaming 'Title' to 'Elector Title'\")\n",
					"        df = df.withColumnRenamed(\"Title\", \"Elector Title\")\n",
					"\n",
					"    # Ensure other essential columns exist\n",
					"    required_columns = {\n",
					"        \"Elector Surname\": StringType(),\n",
					"        \"Elector Forename\": StringType(),\n",
					"        \"Elector DOB\": StringType(),\n",
					"        \"Address1\": StringType(),\n",
					"        \"PostCode\": StringType(),\n",
					"    }\n",
					"\n",
					"    for col_name, data_type in required_columns.items():\n",
					"        if col_name not in df.columns:\n",
					"            logging.info(f\"Adding missing '{col_name}' column\")\n",
					"            df = df.withColumn(col_name, lit(None).cast(data_type))\n",
					"\n",
					"    return df\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def combine_name_components(df):\n",
					"    \"\"\"Combine initials and middle names into Elector Forename\"\"\"\n",
					"    # Check if we have initials column\n",
					"    has_initials = False\n",
					"    initials_col = None\n",
					"\n",
					"    for col_name in [\"Initial\", \"Initials\"]:\n",
					"        if col_name in df.columns:\n",
					"            has_initials = True\n",
					"            initials_col = col_name\n",
					"            break\n",
					"\n",
					"    # Check if we have middle name column\n",
					"    has_middle_name = False\n",
					"    middle_name_col = None\n",
					"\n",
					"    for col_name in [\n",
					"        \"ElectorMiddleName\",\n",
					"        \"Elector Middlename\",\n",
					"        \"MiddleName\",\n",
					"        \"Middle Name\",\n",
					"    ]:\n",
					"        if col_name in df.columns:\n",
					"            has_middle_name = True\n",
					"            middle_name_col = col_name\n",
					"            break\n",
					"\n",
					"    # Combine components if needed\n",
					"    if \"Elector Forename\" in df.columns:\n",
					"        if has_initials:\n",
					"            # Add initials to forename (when both exist)\n",
					"            df = df.withColumn(\n",
					"                \"Elector Forename\",\n",
					"                when(\n",
					"                    (col(\"Elector Forename\").isNotNull())\n",
					"                    & (col(initials_col).isNotNull())\n",
					"                    & (length(trim(col(initials_col))) > 0),\n",
					"                    concat(col(\"Elector Forename\"), lit(\" \"), col(initials_col)),\n",
					"                ).otherwise(col(\"Elector Forename\")),\n",
					"            )\n",
					"\n",
					"            # Use initials as forename when forename is null\n",
					"            df = df.withColumn(\n",
					"                \"Elector Forename\",\n",
					"                when(\n",
					"                    (col(\"Elector Forename\").isNull())\n",
					"                    & (col(initials_col).isNotNull())\n",
					"                    & (length(trim(col(initials_col))) > 0),\n",
					"                    col(initials_col),\n",
					"                ).otherwise(col(\"Elector Forename\")),\n",
					"            )\n",
					"\n",
					"        if has_middle_name:\n",
					"            # Add middle name to forename (when both exist)\n",
					"            df = df.withColumn(\n",
					"                \"Elector Forename\",\n",
					"                when(\n",
					"                    (col(\"Elector Forename\").isNotNull())\n",
					"                    & (col(middle_name_col).isNotNull())\n",
					"                    & (length(trim(col(middle_name_col))) > 0),\n",
					"                    concat(col(\"Elector Forename\"), lit(\" \"), col(middle_name_col)),\n",
					"                ).otherwise(col(\"Elector Forename\")),\n",
					"            )\n",
					"\n",
					"            # Use middle name as forename when forename is null\n",
					"            df = df.withColumn(\n",
					"                \"Elector Forename\",\n",
					"                when(\n",
					"                    (col(\"Elector Forename\").isNull())\n",
					"                    & (col(middle_name_col).isNotNull())\n",
					"                    & (length(trim(col(middle_name_col))) > 0),\n",
					"                    col(middle_name_col),\n",
					"                ).otherwise(col(\"Elector Forename\")),\n",
					"            )\n",
					"\n",
					"    # Upper case and trim the final result\n",
					"    if \"Elector Forename\" in df.columns:\n",
					"        df = df.withColumn(\"Elector Forename\", upper(trim(col(\"Elector Forename\"))))\n",
					"\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def enhanced_split_elector_name(df):\n",
					"    \"\"\"Split elector name with improved handling of multiple forenames\"\"\"\n",
					"    if \"Elector Name\" in df.columns:\n",
					"        logging.info(\"Found 'Elector Name' column, performing enhanced name splitting\")\n",
					"\n",
					"        # First, create a backup of the original name\n",
					"        df = df.withColumn(\"Original_Name\", col(\"Elector Name\"))\n",
					"\n",
					"        # Split the name into parts\n",
					"        df = df.withColumn(\"name_parts\", split(col(\"Elector Name\"), \" \"))\n",
					"\n",
					"        # Extract surname (first part in UK electoral register format)\n",
					"        df = df.withColumn(\n",
					"            \"Elector Surname\",\n",
					"            when(\n",
					"                size(col(\"name_parts\")) > 0,\n",
					"                upper(trim(element_at(col(\"name_parts\"), lit(1)))),\n",
					"            ).otherwise(lit(None)),\n",
					"        )\n",
					"\n",
					"        # Extract primary forename (first name after surname)\n",
					"        df = df.withColumn(\n",
					"            \"Elector Forename\",\n",
					"            when(\n",
					"                size(col(\"name_parts\")) > 1,\n",
					"                upper(trim(element_at(col(\"name_parts\"), lit(2)))),\n",
					"            ).otherwise(lit(None)),\n",
					"        )\n",
					"\n",
					"        # Extract middle names (everything after the first forename)\n",
					"        df = df.withColumn(\n",
					"            \"Elector Middlename\",\n",
					"            when(\n",
					"                size(col(\"name_parts\")) > 2,\n",
					"                upper(\n",
					"                    trim(\n",
					"                        concat_ws(\n",
					"                            \" \",\n",
					"                            expr(f\"slice(name_parts, 3, greatest(1, size(name_parts) - 2))\")\n",
					"                        ),\n",
					"                    )\n",
					"                ),\n",
					"            ).otherwise(lit(None)),\n",
					"        )\n",
					"\n",
					"        # Drop temporary column\n",
					"        df = df.drop(\"name_parts\")\n",
					"\n",
					"        logging.info(\"Successfully performed enhanced name splitting\")\n",
					"\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def extract_numeric_la_code(df):\n",
					"    \"\"\"Extract only the numeric part from LA_Code\"\"\"\n",
					"    if \"LA_Code\" in df.columns:\n",
					"        logging.info(\"Extracting numeric part from LA_Code\")\n",
					"\n",
					"        # Extract only numeric characters from LA_Code\n",
					"        df = df.withColumn(\"LA_Code\", regexp_extract(col(\"LA_Code\"), \"([0-9]+)\", 1))\n",
					"\n",
					"        # If extraction results in empty string, set to null\n",
					"        df = df.withColumn(\n",
					"            \"LA_Code\", when(col(\"LA_Code\") == \"\", None).otherwise(col(\"LA_Code\"))\n",
					"        )\n",
					"\n",
					"        logging.info(\"LA_Code numeric extraction complete\")\n",
					"\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def clean_special_characters(df):\n",
					"    \"\"\"Clean special characters from address and postcode fields\"\"\"\n",
					"    logging.info(\"Cleaning special characters from address and postcode fields\")\n",
					"\n",
					"    # List of address and postcode columns to clean\n",
					"    address_cols = [\n",
					"        \"Address1\",\n",
					"        \"Address2\",\n",
					"        \"Address3\",\n",
					"        \"Address4\",\n",
					"        \"Address5\",\n",
					"        \"Address6\",\n",
					"        \"PostCode\",\n",
					"    ]\n",
					"\n",
					"    for col_name in address_cols:\n",
					"        if col_name in df.columns:\n",
					"            # Replace non-ASCII characters with appropriate substitutions\n",
					"            df = df.withColumn(\n",
					"                col_name,\n",
					"                regexp_replace(\n",
					"                    # First replace � and similar replacement characters\n",
					"                    regexp_replace(col(col_name), \"�\", \"\"),\n",
					"                    # Then replace other problematic characters\n",
					"                    \"[^\\x00-\\x7f]\",\n",
					"                    \"\",\n",
					"                ),\n",
					"            )\n",
					"\n",
					"            # Trim any resulting double spaces\n",
					"            df = df.withColumn(col_name, trim(regexp_replace(col(col_name), \" +\", \" \")))\n",
					"\n",
					"            # For postcodes specifically, remove all non-alphanumeric characters except spaces\n",
					"            if col_name == \"PostCode\":\n",
					"                df = df.withColumn(\n",
					"                    col_name, regexp_replace(col(col_name), \"[^A-Z0-9 ]\", \"\")\n",
					"                )\n",
					"\n",
					"                # Ensure proper postcode format (if possible)\n",
					"                df = df.withColumn(\n",
					"                    col_name,\n",
					"                    when(\n",
					"                        # If it looks like a valid UK postcode pattern after cleaning\n",
					"                        col(col_name).rlike(\n",
					"                            \"^[A-Z]{1,2}[0-9][0-9A-Z]? ?[0-9][A-Z]{2}$\"\n",
					"                        ),\n",
					"                        # Ensure proper spacing (one space between outward and inward parts)\n",
					"                        regexp_replace(\n",
					"                            col(col_name),\n",
					"                            \"^([A-Z]{1,2}[0-9][0-9A-Z]?)[ ]*([0-9][A-Z]{2})$\",\n",
					"                            \"$1 $2\",\n",
					"                        ),\n",
					"                    ).otherwise(col(col_name)),\n",
					"                )\n",
					"\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def fix_numeric_fields(df):\n",
					"    \"\"\"Fix numeric fields that might be interpreted as dates\"\"\"\n",
					"    numeric_columns = [\n",
					"        \"Elector Number\",\n",
					"        \"Poll number\",\n",
					"        \"poll_number\",\n",
					"        \"RollNo\",\n",
					"        \"Eno\",\n",
					"        \"ElectorNumber\",\n",
					"    ]\n",
					"\n",
					"    for col_name in numeric_columns:\n",
					"        if col_name in df.columns:\n",
					"            logging.info(f\"Fixing numeric format for column: {col_name}\")\n",
					"\n",
					"            # Check column data type first\n",
					"            col_type = df.schema[col_name].dataType\n",
					"            if isinstance(col_type, StringType):\n",
					"                # Only process string columns\n",
					"                df = df.withColumn(\n",
					"                    f\"{col_name}_temp\",\n",
					"                    when(\n",
					"                        # Check if value contains date separators\n",
					"                        regexp_extract(col(col_name), \"(-|/)\", 1) != \"\",\n",
					"                        # If it's a date format, extract only numeric part\n",
					"                        regexp_replace(col(col_name), \"[^0-9]\", \"\"),\n",
					"                    )\n",
					"                    .when(\n",
					"                        # For pure numeric strings, keep as is\n",
					"                        col(col_name).rlike(\"^[0-9]+$\"),\n",
					"                        col(col_name),\n",
					"                    )\n",
					"                    .otherwise(\n",
					"                        # For other formats, try to extract numeric part\n",
					"                        regexp_replace(col(col_name), \"[^0-9]\", \"\")\n",
					"                    ),\n",
					"                )\n",
					"\n",
					"                # Cast to integer if possible\n",
					"                df = df.withColumn(\n",
					"                    f\"{col_name}_temp\",\n",
					"                    when(\n",
					"                        col(f\"{col_name}_temp\").rlike(\"^[0-9]+$\"),\n",
					"                        col(f\"{col_name}_temp\").cast(\"int\"),\n",
					"                    ).otherwise(col(f\"{col_name}_temp\")),\n",
					"                )\n",
					"\n",
					"                # Replace original column with fixed version\n",
					"                df = df.drop(col_name).withColumnRenamed(f\"{col_name}_temp\", col_name)\n",
					"\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def process_name_components(df):\n",
					"    \"\"\"Process name components separately\"\"\"\n",
					"    name_columns = [\n",
					"        \"Elector Title\",\n",
					"        \"Elector Forename\",\n",
					"        \"Elector Surname\",\n",
					"        \"Elector Middlename\",\n",
					"        \"Suffix\",\n",
					"        \"Initials\",\n",
					"    ]\n",
					"\n",
					"    for col_name in name_columns:\n",
					"        if col_name in df.columns:\n",
					"            df = df.withColumn(\n",
					"                col_name,\n",
					"                when(col(col_name).isNotNull(), upper(trim(col(col_name)))).otherwise(\n",
					"                    None\n",
					"                ),\n",
					"            )\n",
					"\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def get_folder_parts(file_path):\n",
					"    \"\"\"Extract folder information for naming with improved parsing\"\"\"\n",
					"    parts = file_path.split(\"/\")\n",
					"\n",
					"    try:\n",
					"        la_index = parts.index(\"LA_Data\")\n",
					"        if la_index > 0:\n",
					"            date_folder = parts[la_index - 1]\n",
					"            if len(parts) > la_index + 1:\n",
					"                # Get LA folder name (third folder)\n",
					"                third_folder = parts[la_index + 1]\n",
					"                return date_folder, third_folder\n",
					"    except ValueError:\n",
					"        pass\n",
					"\n",
					"    # Alternative parsing method if LA_Data not found\n",
					"    file_name = parts[-1]\n",
					"    if \"_\" in file_name:\n",
					"        parts = file_name.split(\"_\", 1)\n",
					"        if len(parts) == 2:\n",
					"            date_part = parts[0]\n",
					"            la_part = parts[1].split(\".\")[0]  # Remove extension\n",
					"            return date_part, la_part\n",
					"\n",
					"    # If all else fails, use current date and filename\n",
					"    current_date = datetime.now().strftime(\"%Y%m%d\")\n",
					"    filename_without_ext = os.path.splitext(os.path.basename(file_path))[0]\n",
					"    return current_date, filename_without_ext\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def filter_records_with_empty_fields(df):\n",
					"    \"\"\"Filter out records where critical fields are empty\"\"\"\n",
					"    logging.info(f\"Original record count: {df.count()}\")\n",
					"\n",
					"    # Define critical fields that should not be empty\n",
					"    critical_fields = [\n",
					"        \"Elector Number\",\n",
					"        \"Elector Surname\",\n",
					"        \"Elector Forename\",\n",
					"        \"LA_Code\",\n",
					"    ]\n",
					"\n",
					"    # Filter out rows where critical fields are null or empty string\n",
					"    for field in critical_fields:\n",
					"        if field in df.columns:\n",
					"            initial_count = df.count()\n",
					"            df = df.filter((col(field).isNotNull()) & (length(trim(col(field))) > 0))\n",
					"            removed = initial_count - df.count()\n",
					"            logging.info(f\"Removed {removed} rows with empty {field}\")\n",
					"\n",
					"    logging.info(f\"Remaining record count after filtering: {df.count()}\")\n",
					"    return df\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def transform_data(df, creation_date, la_code=None):\n",
					"    \"\"\"Transform data by cleaning text columns and handling names\"\"\"\n",
					"    # Clean text columns\n",
					"    for col_name in df.columns:\n",
					"        if df.schema[col_name].dataType == StringType():\n",
					"            df = df.withColumn(col_name, upper(trim(df[col_name])))\n",
					"\n",
					"    # Ensure all required fields exist\n",
					"    df = ensure_required_fields(df)\n",
					"\n",
					"    # Clean special characters in address and postcode fields\n",
					"    df = clean_special_characters(df)\n",
					"\n",
					"    # Fix numeric fields\n",
					"    df = fix_numeric_fields(df)\n",
					"\n",
					"    # Process name fields\n",
					"    df = enhanced_split_elector_name(df)\n",
					"    df = process_name_components(df)\n",
					"\n",
					"    # Combine name components (initials and middle names into forename)\n",
					"    df = combine_name_components(df)\n",
					"\n",
					"    # Add metadata\n",
					"    if creation_date:\n",
					"        df = df.withColumn(\"CreationDate\", lit(creation_date))\n",
					"\n",
					"    if la_code:\n",
					"        # Extract only numeric part from la_code\n",
					"        numeric_la_code = \"\".join(c for c in la_code if c.isdigit())\n",
					"        df = df.withColumn(\"LA_Code\", lit(numeric_la_code))\n",
					"\n",
					"    # Extract numeric part from LA_Code if it exists\n",
					"    df = extract_numeric_la_code(df)\n",
					"\n",
					"    # Filter out records with empty critical fields\n",
					"    df = filter_records_with_empty_fields(df)\n",
					"\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def filter_to_required_columns(df, column_mapping=None, keep_unmapped=False):\n",
					"    \"\"\"Filter columns based on mapping configuration\"\"\"\n",
					"    if not column_mapping:\n",
					"        logging.warning(\"No column mapping provided, returning original DataFrame\")\n",
					"        return df\n",
					"        \n",
					"    # Get all standard column names from the mapping\n",
					"    mapped_columns = list(column_mapping.keys())\n",
					"    \n",
					"    # Add essential metadata columns\n",
					"    essential_columns = [\"LA_Code\", \"CreationDate\"]\n",
					"    required_columns = mapped_columns + essential_columns\n",
					"    \n",
					"    if keep_unmapped:\n",
					"        # Keep all columns but reorder to put mapped columns first\n",
					"        existing_mapped = [col for col in required_columns if col in df.columns]\n",
					"        unmapped_columns = [col for col in df.columns if col not in required_columns]\n",
					"        ordered_columns = existing_mapped + unmapped_columns\n",
					"        return df.select(ordered_columns)\n",
					"    else:\n",
					"        # Keep only mapped columns\n",
					"        existing_columns = [col for col in required_columns if col in df.columns]\n",
					"        return df.select(existing_columns)\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def process_file(spark, source_path, reverse_mapping, transform_folder, error_folder, column_mapping=None):\n",
					"    \"\"\"Process a source file and transform it to parquet format\"\"\"\n",
					"    # Print more detailed debugging info\n",
					"    print(f\"Debug - Starting to process file: {source_path}\")\n",
					"    print(f\"Debug - File exists check: {mssparkutils.fs.exists(source_path)}\")\n",
					"    \n",
					"    result = {\n",
					"        \"file\": source_path,\n",
					"        \"status\": \"error\",\n",
					"        \"message\": \"\",\n",
					"        \"output\": None,\n",
					"        \"timestamp\": datetime.now().isoformat(),\n",
					"    }\n",
					"\n",
					"    try:\n",
					"        # Create required folders if they don't exist\n",
					"        if not ensure_folder_exists(transform_folder):\n",
					"            result[\"message\"] = f\"Could not create transform folder: {transform_folder}\"\n",
					"            return result\n",
					"\n",
					"        if not ensure_folder_exists(error_folder):\n",
					"            result[\"message\"] = f\"Could not create error folder: {error_folder}\"\n",
					"            return result\n",
					"\n",
					"        # Get file details\n",
					"        file_name = os.path.basename(source_path)\n",
					"        date_folder, la_code = get_folder_parts(source_path)\n",
					"\n",
					"        logging.info(\n",
					"            f\"Processing file: {file_name}, Date: {date_folder}, LA: {la_code}\"\n",
					"        )\n",
					"\n",
					"        # Read the file\n",
					"        try:\n",
					"            df = read_file(spark, source_path)\n",
					"            logging.info(\n",
					"                f\"Successfully read file with {df.count()} rows and {len(df.columns)} columns\"\n",
					"            )\n",
					"        except Exception as e:\n",
					"            result[\"message\"] = f\"Error reading file: {str(e)}\"\n",
					"            return result\n",
					"\n",
					"        # Standardize column names using mapping\n",
					"        if reverse_mapping:\n",
					"            df = standardize_column_names(df, reverse_mapping)\n",
					"        else:\n",
					"            logging.warning(\n",
					"                \"No reverse mapping available - skipping column standardization\"\n",
					"            )\n",
					"\n",
					"        # Transform data\n",
					"        try:\n",
					"            df = transform_data(df, date_folder, la_code)\n",
					"\n",
					"            # Validation checks\n",
					"            if \"Elector Number\" in df.columns:\n",
					"                numeric_count = df.filter(\n",
					"                    col(\"Elector Number\").cast(\"int\").isNotNull()\n",
					"                ).count()\n",
					"                total_count = df.count()\n",
					"                numeric_percentage = (\n",
					"                    (numeric_count / total_count) * 100 if total_count > 0 else 0\n",
					"                )\n",
					"                logging.info(\n",
					"                    f\"Elector Number field: {numeric_percentage:.2f}% are valid numbers ({numeric_count}/{total_count})\"\n",
					"                )\n",
					"\n",
					"            if \"LA_Code\" in df.columns:\n",
					"                logging.info(\n",
					"                    f\"LA_Code values sample: {[r.LA_Code for r in df.select('LA_Code').distinct().limit(5).collect()]}\"\n",
					"                )\n",
					"\n",
					"            logging.info(f\"Successfully transformed data with {df.count()} rows\")\n",
					"        except Exception as e:\n",
					"            result[\"message\"] = f\"Error transforming data: {str(e)}\"\n",
					"            return result\n",
					"\n",
					"        logging.info(\n",
					"            f\"Filtered to {len(df.columns)} required columns: {', '.join(df.columns)}\"\n",
					"        )\n",
					"        # Filter to required columns\n",
					"        df = filter_to_required_columns(df, column_mapping, keep_unmapped=True)\n",
					"\n",
					"        # Create a output name using date and LA code\n",
					"        output_name = f\"{date_folder}_{la_code}\"\n",
					"        output_path =f\"{transform_folder}/{os.path.splitext(file_name)[0]}.parquet\"\n",
					"        # Validate data before writing\n",
					"        validation_errors = []\n",
					"\n",
					"        # Check for null values in critical columns\n",
					"        critical_columns = [\"Elector Number\", \"LA_Code\"]\n",
					"        for col_name in critical_columns:\n",
					"            if col_name in df.columns:\n",
					"                null_count = df.filter(col(col_name).isNull()).count()\n",
					"                if null_count > 0:\n",
					"                    validation_errors.append(\n",
					"                        f\"Column '{col_name}' contains {null_count} null values\"\n",
					"                    )\n",
					"\n",
					"        # Check for invalid data types\n",
					"        if \"Elector Number\" in df.columns:\n",
					"            try:\n",
					"                numeric_test = df.withColumn(\"_test\", col(\"Elector Number\").cast(\"int\"))\n",
					"                invalid_count = numeric_test.filter(\n",
					"                    col(\"_test\").isNull() & col(\"Elector Number\").isNotNull()\n",
					"                ).count()\n",
					"                if invalid_count > 0:\n",
					"                    validation_errors.append(\n",
					"                        f\"Column 'Elector Number' contains {invalid_count} non-numeric values\"\n",
					"                    )\n",
					"            except Exception as e:\n",
					"                validation_errors.append(f\"Error validating 'Elector Number': {str(e)}\")\n",
					"        # Log validation results\n",
					"        if validation_errors:\n",
					"            logging.warning(f\"Data validation warnings: {'; '.join(validation_errors)}\")\n",
					"\n",
					"        # Write transformed data to parquet - continue even with warnings\n",
					"\n",
					"        try:\n",
					"            df.write.mode(\"overwrite\").parquet(output_path)\n",
					"            logging.info(f\"Successfully wrote parquet file to {output_path}\")\n",
					"\n",
					"            result[\"status\"] = \"success\"\n",
					"            result[\"message\"] = f\"Successfully processed file {file_name}\"\n",
					"            result[\"output\"] = output_path\n",
					"            return result\n",
					"        except Exception as e:\n",
					"            result[\"message\"] = f\"Error writing output file: {str(e)}\"\n",
					"            return result\n",
					"\n",
					"    except Exception as e:\n",
					"        result[\"message\"] = f\"Unexpected error: {str(e)}\"\n",
					"        return result"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def read_processed_files_log(spark, log_file_path):\n",
					"    \"\"\"Read the log of processed files with error handling\"\"\"\n",
					"    try:\n",
					"        if not mssparkutils.fs.exists(log_file_path):\n",
					"            logging.info(f\"Processed files log does not exist yet: {log_file_path}\")\n",
					"            return []\n",
					"\n",
					"        processed_files_log = (\n",
					"            spark.read.json(log_file_path)\n",
					"            .select(\"file\")\n",
					"            .rdd.flatMap(lambda x: x)\n",
					"            .collect()\n",
					"        )\n",
					"        logging.info(\n",
					"            f\"Read processed files log with {len(processed_files_log)} entries\"\n",
					"        )\n",
					"        return processed_files_log\n",
					"    except Exception as e:\n",
					"        logging.warning(f\"Could not read processed files log: {str(e)}\")\n",
					"        return []"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def update_processed_files_log(spark, log_file_path, processed_files_log, new_files):\n",
					"    \"\"\"Update the log of processed files\"\"\"\n",
					"    try:\n",
					"        # Create destination folder if it doesn't exist\n",
					"        log_folder = os.path.dirname(log_file_path)\n",
					"        if not mssparkutils.fs.exists(log_folder):\n",
					"            mssparkutils.fs.mkdirs(log_folder)\n",
					"            logging.info(f\"Created log folder: {log_folder}\")\n",
					"\n",
					"        # Create DataFrame for new files\n",
					"        new_processed_files_log = spark.createDataFrame(\n",
					"            [(file,) for file in new_files], [\"file\"]\n",
					"        )\n",
					"\n",
					"        # Merge with existing log if it exists\n",
					"        if processed_files_log:\n",
					"            existing_log_df = spark.createDataFrame(\n",
					"                [(file,) for file in processed_files_log], [\"file\"]\n",
					"            )\n",
					"            updated_log_df = existing_log_df.union(new_processed_files_log).distinct()\n",
					"        else:\n",
					"            updated_log_df = new_processed_files_log\n",
					"\n",
					"        # Write updated log\n",
					"        updated_log_df.write.mode(\"overwrite\").json(log_file_path)\n",
					"        logging.info(f\"Updated processed files log with {len(new_files)} new entries\")\n",
					"        return True\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error updating processed files log: {str(e)}\")\n",
					"        return False\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def print_summary(results):\n",
					"    \"\"\"Print a summary of processing results\"\"\"\n",
					"    successful = [r for r in results if r[\"status\"] == \"success\"]\n",
					"    failed = [r for r in results if r[\"status\"] == \"error\"]\n",
					"\n",
					"    print(\"\\n========== PROCESSING SUMMARY ==========\")\n",
					"    print(f\"Total files processed: {len(results)}\")\n",
					"    print(f\"Successfully processed: {len(successful)}\")\n",
					"    print(f\"Failed: {len(failed)}\")\n",
					"\n",
					"    if successful:\n",
					"        print(\"\\n----- Successfully Processed Files -----\")\n",
					"        for result in successful[:10]:  # Show first 10\n",
					"            print(f\"  - {result['file']} -> {result['output']}\")\n",
					"        if len(successful) > 10:\n",
					"            print(f\"  ... and {len(successful) - 10} more files\")\n",
					"\n",
					"    if failed:\n",
					"        print(\"\\n----- Failed Files -----\")\n",
					"        for result in failed[:10]:  # Show first 10\n",
					"            print(f\"  - {result['file']}: {result['message']}\")\n",
					"        if len(failed) > 10:\n",
					"            print(f\"  ... and {len(failed) - 10} more files\")\n",
					"\n",
					"    print(\"=======================================\\n\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"def main():\n",
					"    \"\"\"Main function for the transformation process\"\"\"\n",
					"    # Initialize Spark session\n",
					"    spark = create_spark_session()\n",
					"\n",
					"    # Define paths\n",
					"    storage_account = \"baubaisadfsastg\"\n",
					"    files_folder = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/files\"\n",
					"    config_folder = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/config/schema\"\n",
					"    transform_folder = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/transformation\"\n",
					"    error_folder = f\"{transform_folder}/errors\"\n",
					"    log_file_path = f\"{transform_folder}/processed_files_log.json\"\n",
					"\n",
					"    # Check for inconsistent state (log exists but folder doesn't)\n",
					"    transform_folder_exists = mssparkutils.fs.exists(transform_folder)\n",
					"    log_file_exists = mssparkutils.fs.exists(log_file_path)\n",
					"    \n",
					"    if not transform_folder_exists and log_file_exists:\n",
					"        logging.warning(\"Found inconsistent state: transformation folder missing but log exists\")\n",
					"        try:\n",
					"            mssparkutils.fs.rm(log_file_path)\n",
					"            logging.info(\"Successfully removed stale log file\")\n",
					"        except Exception as e:\n",
					"            logging.error(f\"Failed to remove stale log file: {str(e)}\")\n",
					"\n",
					"    # Ensure required folders exist with validation\n",
					"    folder_status = {}\n",
					"    for folder in [files_folder, transform_folder, error_folder]:\n",
					"        result = ensure_folder_exists(folder)\n",
					"        folder_status[folder] = result\n",
					"        if not result:\n",
					"            logging.error(f\"Failed to create or verify folder: {folder}\")\n",
					"            print(f\"ERROR: Cannot access or create folder: {folder}\")\n",
					"            return []  # Exit if critical folders can't be created\n",
					"    \n",
					"    logging.info(f\"Folder status: {folder_status}\")\n",
					"    \n",
					"    # Check source folder content\n",
					"    try:\n",
					"        files_content = mssparkutils.fs.ls(files_folder)\n",
					"        logging.info(f\"Found {len(files_content)} items in source folder\")\n",
					"        if len(files_content) > 0:\n",
					"            logging.info(f\"Sample items: {[f.name for f in files_content[:3]]}\")\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error inspecting source folder: {str(e)}\")\n",
					"\n",
					"    # Read column mapping\n",
					"    mapping_file = f\"{config_folder}/col_mapping.json\"\n",
					"    try:\n",
					"        # Verify mapping file exists\n",
					"        if not mssparkutils.fs.exists(mapping_file):\n",
					"            logging.warning(f\"Mapping file not found at {mapping_file}\")\n",
					"            print(f\"WARNING: Mapping file not found at {mapping_file}\")\n",
					"            \n",
					"        column_mapping = read_column_mapping(spark, mapping_file)\n",
					"        if column_mapping:\n",
					"            logging.info(f\"Successfully read column mapping with {len(column_mapping)} mappings\")\n",
					"            reverse_mapping = get_reverse_mapping(column_mapping)\n",
					"            logging.info(f\"Created reverse mapping with {len(reverse_mapping)} entries\")\n",
					"        else:\n",
					"            logging.warning(\"Column mapping is empty - check your configuration file\")\n",
					"            reverse_mapping = {}\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error reading column mapping: {str(e)}\")\n",
					"        reverse_mapping = {}\n",
					"\n",
					"    # Read the log of processed files with validation\n",
					"    try:\n",
					"        processed_files_log = read_processed_files_log(spark, log_file_path)\n",
					"        logging.info(f\"Successfully read log with {len(processed_files_log)} entries\")\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error reading log, will start fresh: {str(e)}\")\n",
					"        processed_files_log = []\n",
					"\n",
					"    # List all files in the source folder with better error handling\n",
					"    try:\n",
					"        all_files = list_files(files_folder, ['.parquet'])\n",
					"        logging.info(f\"Found {len(all_files)} files to process\")\n",
					"        if len(all_files) == 0:\n",
					"            logging.warning(\"No files found in source directory - nothing to process\")\n",
					"            print(\"No files found in source directory.\")\n",
					"            \n",
					"            # Create empty summary to maintain consistency\n",
					"            empty_schema = StructType([\n",
					"                StructField(\"file\", StringType(), True),\n",
					"                StructField(\"status\", StringType(), True),\n",
					"                StructField(\"message\", StringType(), True),\n",
					"                StructField(\"output\", StringType(), True),\n",
					"                StructField(\"timestamp\", StringType(), True),\n",
					"            ])\n",
					"            empty_df = spark.createDataFrame([], empty_schema)\n",
					"            \n",
					"            # Ensure directory exists before writing\n",
					"            ensure_folder_exists(transform_folder)\n",
					"            empty_df.write.mode(\"overwrite\").json(f\"{transform_folder}/processing_summary.json\")\n",
					"            logging.info(\"Created empty processing summary\")\n",
					"            return []\n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error listing files: {str(e)}\")\n",
					"        print(f\"Error listing files: {str(e)}\")\n",
					"        return []\n",
					"\n",
					"    # Filter out files that have already been processed\n",
					"    new_files = [file for file in all_files if file not in processed_files_log]\n",
					"    logging.info(f\"New files to process: {len(new_files)}\")\n",
					"    \n",
					"    if not new_files:\n",
					"        logging.info(\"No new files found - all files already processed\")\n",
					"        print(\"No new files to process - all files have been processed previously.\")\n",
					"        return []\n",
					"\n",
					"    # Process each new file with better progress reporting\n",
					"    results = []\n",
					"    total_files = len(new_files)\n",
					"    for i, file_path in enumerate(new_files):\n",
					"        file_num = i + 1\n",
					"        progress_pct = (file_num / total_files) * 100\n",
					"        logging.info(f\"Processing file {file_num}/{total_files} ({progress_pct:.1f}%): {file_path}\")\n",
					"        print(f\"Processing file {file_num}/{total_files} ({progress_pct:.1f}%): {os.path.basename(file_path)}\")\n",
					"        \n",
					"        # In the main function, where you call process_file\n",
					"        result = process_file(spark, file_path, reverse_mapping, transform_folder, error_folder, column_mapping)\n",
					"        results.append(result)\n",
					"        \n",
					"        status_emoji = \"✅\" if result[\"status\"] == \"success\" else \"❌\"\n",
					"        logging.info(f\"{status_emoji} Completed file {file_num}/{total_files}: {result['status']} - {result['message']}\")\n",
					"        print(f\"{status_emoji} Completed file {file_num}/{total_files}: {result['status']} - {result['message']}\")\n",
					"\n",
					"    # Create a summary DataFrame\n",
					"    if results:\n",
					"        try:\n",
					"            summary_df = spark.createDataFrame(results)\n",
					"            summary_df.write.mode(\"overwrite\").json(f\"{transform_folder}/processing_summary.json\")\n",
					"            logging.info(f\"Wrote processing summary to {transform_folder}/processing_summary.json\")\n",
					"        except Exception as e:\n",
					"            logging.error(f\"Error creating summary DataFrame: {str(e)}\")\n",
					"            print(f\"Error creating summary: {str(e)}\")\n",
					"    else:\n",
					"        # Handle case with no results\n",
					"        logging.warning(\"No files were processed - not creating summary\")\n",
					"        empty_schema = StructType([\n",
					"            StructField(\"file\", StringType(), True),\n",
					"            StructField(\"status\", StringType(), True),\n",
					"            StructField(\"message\", StringType(), True),\n",
					"            StructField(\"output\", StringType(), True),\n",
					"            StructField(\"timestamp\", StringType(), True),\n",
					"        ])\n",
					"        empty_df = spark.createDataFrame([], empty_schema)\n",
					"        empty_df.write.mode(\"overwrite\").json(f\"{transform_folder}/processing_summary.json\")\n",
					"\n",
					"    # Print a summary of results\n",
					"    print_summary(results)\n",
					"\n",
					"    # Update the log of processed files\n",
					"    if new_files:\n",
					"        try:\n",
					"            update_result = update_processed_files_log(spark, log_file_path, processed_files_log, new_files)\n",
					"            if update_result:\n",
					"                logging.info(f\"Successfully updated processing log with {len(new_files)} new entries\")\n",
					"            else:\n",
					"                logging.warning(\"Failed to update processing log\")\n",
					"        except Exception as e:\n",
					"            logging.error(f\"Error updating log: {str(e)}\")\n",
					"\n",
					"    # Return the results for further processing\n",
					"    return results\n",
					"# Execute the main function with a clean run flag\n",
					"if __name__ == \"__main__\":\n",
					"    # Set to True to force reprocessing of all files (ignore log)\n",
					"    FORCE_CLEAN_RUN = True\n",
					"    \n",
					"    # Delete log file if forcing a clean run\n",
					"    if FORCE_CLEAN_RUN:\n",
					"        log_file_path = \"abfss://juror-etl@baubaisadfsastg.dfs.core.windows.net/transformation/processed_files_log.json\"\n",
					"        if mssparkutils.fs.exists(log_file_path):\n",
					"            try:\n",
					"                mssparkutils.fs.rm(log_file_path)\n",
					"                print(\"Deleted existing log to force fresh processing\")\n",
					"            except Exception as e:\n",
					"                print(f\"Error deleting log file: {str(e)}\")\n",
					"    \n",
					"    # Run the main function\n",
					"    results = main()\n",
					"    print(\"\\nProcess completed.\")"
				],
				"execution_count": null
			}
		]
	}
}