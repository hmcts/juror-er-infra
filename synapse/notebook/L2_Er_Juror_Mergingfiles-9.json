{
	"name": "L2_Er_Juror_Mergingfiles-9",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkstg",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "fceca983-3f8b-4bc8-85c0-cc5abae94715"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/74dacd4f-a248-45bb-a2f0-af700dc4cf68/resourceGroups/baubais-data-factory-rg-stg/providers/Microsoft.Synapse/workspaces/baubais-synapse-stg/bigDataPools/sparkstg",
				"name": "sparkstg",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-stg.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkstg",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Electoral Roll Data ETL Process\n",
					"\n",
					"### **L2_Er_Juror_Mergingfiles: Electoral Roll Data ETL Process**\n",
					"\n",
					"This script implements an **ETL (Extract, Transform, Load)** pipeline for processing electoral roll data. It handles data ingestion, transformation, deduplication, hashing, and merging with existing data. The final output is written to Azure Data Lake Storage in a standardized format.\n",
					"\n",
					"---\n",
					"\n",
					"### **Overview**\n",
					"\n",
					"The script processes electoral roll data from multiple sources, standardizes column names, removes duplicates, hashes sensitive information, and prepares the data for storage in Azure Data Lake. It also maintains a process log to track processed files and ensures schema consistency.\n",
					"\n",
					"---\n",
					"\n",
					"### **Key Steps in the ETL Process**\n",
					"\n",
					"#### **1. Setup Functions**\n",
					"- **`create_spark_session()`**:\n",
					"  - Initializes a Spark session with necessary configurations.\n",
					"- **`configure_logging()`**:\n",
					"  - Sets up logging to track the ETL process and writes logs to a file (`electoral_etl.log`).\n",
					"- **`load_config()`**:\n",
					"  - Loads configuration details such as storage paths for input, output, and schema files.\n",
					"\n",
					"---\n",
					"\n",
					"#### **2. Configuration Processing**\n",
					"- **`read_column_mapping(spark, config_path)`**:\n",
					"  - Reads and parses a JSON configuration file containing column mappings.\n",
					"  - Standardizes column names based on the mapping.\n",
					"\n",
					"- **`standardize_columns(df, column_mapping)`**:\n",
					"  - Renames columns in the input DataFrame to match the standardized schema.\n",
					"\n",
					"- **`read_schema_config(spark, config_path)`**:\n",
					"  - Reads and parses the schema configuration to ensure the DataFrame matches the target schema.\n",
					"\n",
					"- **`ensure_consistent_schema(df, required_columns)`**:\n",
					"  - Ensures the DataFrame has all required columns with correct data types.\n",
					"\n",
					"---\n",
					"\n",
					"#### **3. Incremental Data Processing**\n",
					"- **`get_new_files(spark, input_path, process_log_path)`**:\n",
					"  - Identifies new files in the input directory that have not been processed yet.\n",
					"  - Compares the list of files in the input directory with the process log.\n",
					"\n",
					"- **`process_input_files(spark, file_paths, column_mapping)`**:\n",
					"  - Reads and standardizes columns from the identified new files.\n",
					"  - Adds metadata such as the source file path and processing timestamp.\n",
					"\n",
					"---\n",
					"\n",
					"#### **4. Data Transformation**\n",
					"- **`merge_dataframes(dfs)`**:\n",
					"  - Merges multiple DataFrames with standardized schemas into a single DataFrame.\n",
					"  - Handles schema variations using `unionByName`.\n",
					"\n",
					"- **`deduplicate_by_creation_date(df)`**:\n",
					"  - Removes duplicate records by keeping only the most recent record for each unique entity based on the `CreationDate` column.\n",
					"\n",
					"- **`optimise_deduplicate_data(df)`**:\n",
					"  - Performs conservative deduplication by removing exact duplicates across all columns.\n",
					"\n",
					"- **`improved_deduplicate_data(df)`**:\n",
					"  - Uses window functions to keep the most recent record for each unique entity based on identifying columns.\n",
					"\n",
					"- **`apply_hashing(df)`**:\n",
					"  - Hashes sensitive fields (e.g., names, addresses, dates of birth) to create a unique identifier (`hash_id`).\n",
					"\n",
					"- **`apply_hashing_to_voters(df)`**:\n",
					"  - Applies hashing specifically for the `voters_temp` table structure.\n",
					"\n",
					"- **`transform_to_target_schema(df, column_mapping)`**:\n",
					"  - Transforms the DataFrame to match the target schema for PostgreSQL.\n",
					"  - Applies validation rules to ensure data quality.\n",
					"\n",
					"---\n",
					"\n",
					"#### **5. Merging with Existing Data**\n",
					"- **`merge_with_existing_data(spark, new_data, output_path)`**:\n",
					"  - Merges new data with existing data in the output path.\n",
					"  - Uses SQL-based logic to handle updates and inserts:\n",
					"    - Keeps all new records that do not exist in the existing data.\n",
					"    - Updates existing records with newer values if available.\n",
					"\n",
					"---\n",
					"\n",
					"#### **6. Writing Output and Updating Process Log**\n",
					"- **`write_output_safely(df, output_path, description)`**:\n",
					"  - Writes the DataFrame to the specified output path in Azure Data Lake Storage.\n",
					"  - Ensures proper error handling and directory creation.\n",
					"\n",
					"- **`write_output_and_update_log(spark, merged_data, new_file_paths, process_log_path)`**:\n",
					"  - Writes the merged data to the output location and updates the process log with newly processed files.\n",
					"\n",
					"- **`update_process_log(spark, new_file_paths, process_log_path)`**:\n",
					"  - Updates the process log to track which files have been processed.\n",
					"\n",
					"---\n",
					"\n",
					"### **Main Pipeline Function**\n",
					"\n",
					"The `main()` function orchestrates the entire ETL process:\n",
					"\n",
					"1. **Initialization**:\n",
					"   - Configures logging and initializes the Spark session.\n",
					"   - Loads configuration and column mapping.\n",
					"\n",
					"2. **File Processing**:\n",
					"   - Identifies new files to process.\n",
					"   - Reads and standardizes the new files.\n",
					"\n",
					"3. **Data Transformation**:\n",
					"   - Merges new data into a single DataFrame.\n",
					"   - Deduplicates the data and applies hashing to sensitive fields.\n",
					"   - Transforms the data to match the target schema.\n",
					"\n",
					"4. **Merging with Existing Data**:\n",
					"   - Merges the transformed data with existing data in the output path.\n",
					"\n",
					"5. **Writing Output**:\n",
					"   - Writes the final data to Azure Data Lake Storage.\n",
					"   - Updates the process log.\n",
					"\n",
					"6. **Error Handling**:\n",
					"   - Logs errors and continues processing where possible.\n",
					"\n",
					"---\n",
					"\n",
					"### **Key Features**\n",
					"\n",
					"1. **Schema Standardization**:\n",
					"   - Ensures all input files conform to a consistent schema.\n",
					"\n",
					"2. **Deduplication**:\n",
					"   - Removes duplicate records based on creation date or identifying columns.\n",
					"\n",
					"3. **Hashing**:\n",
					"   - Hashes sensitive fields to protect privacy.\n",
					"\n",
					"4. **Incremental Processing**:\n",
					"   - Processes only new files that have not been processed before.\n",
					"\n",
					"5. **Merge Logic**:\n",
					"   - Combines new data with existing data, handling updates and inserts.\n",
					"\n",
					"6. **Error Handling**:\n",
					"   - Logs errors and continues processing where possible.\n",
					"\n",
					"---\n",
					"\n",
					"### **Output**\n",
					"\n",
					"1. **Transformed Data**:\n",
					"   - The final transformed data is written to the specified output path in Azure Data Lake Storage.\n",
					"\n",
					"2. **Process Log**:\n",
					"   - A log of processed files is maintained to avoid reprocessing.\n",
					"\n",
					"---\n",
					"\n",
					"### **Next Steps**\n",
					"\n",
					"1. **Run the Script**:\n",
					"   - Execute the `main()` function to process the electoral roll data.\n",
					"\n",
					"2. **Verify the Output**:\n",
					"   - Check the output path in Azure Data Lake Storage to ensure the data is written correctly.\n",
					"\n",
					"3. **Monitor Logs**:\n",
					"   - Review the `electoral_etl.log` file for any errors or warnings.\n",
					"\n",
					"4. **Integrate with Downstream Systems**:\n",
					"   - Use the transformed data for analytics or load it into a database (e.g., PostgreSQL).\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#import sys\n",
					"#sys.path.append('/Configuration')  # Add Configuration directory to Python path\n",
					"\n",
					"#try:\n",
					"    # Import from the Python modules\n",
					" #   from column_constraint import validate_and_transform_all_columns\n",
					"  #  print(\"Successfully imported validation modules\")\n",
					"#except ImportError as e:\n",
					" #   print(f\"Error importing validation modules: {e}\")"
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession, Window\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"import json\n",
					"import logging\n",
					"from datetime import datetime\n",
					"import time\n",
					"import os\n",
					"import re\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql.functions import col, when, upper, regexp_extract, regexp_replace, length, substring, lit"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"source": [
					"def configure_logging():\n",
					"    \"\"\"Set up proper logging with rotating file handler\"\"\"\n",
					"    log_format = '%(asctime)s [%(levelname)s] %(message)s'\n",
					"    logging.basicConfig(level=logging.INFO, format=log_format)\n",
					"    \n",
					"    # Add a handler that writes to a log file\n",
					"    file_handler = logging.FileHandler('electoral_etl.log')\n",
					"    file_handler.setFormatter(logging.Formatter(log_format))\n",
					"    \n",
					"    # Get the root logger and add the file handler\n",
					"    root_logger = logging.getLogger()\n",
					"    root_logger.addHandler(file_handler)\n",
					"    \n",
					"    return root_logger"
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"source": [
					"def create_spark_session():\n",
					"    \"\"\"Create and return a Spark session\"\"\"\n",
					"    return SparkSession.builder \\\n",
					"        .appName(\"Electoral Data ETL\") \\\n",
					"        .getOrCreate()"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"source": [
					"def normalize_storage_path(path):\n",
					"    \"\"\"Normalize storage path to ensure consistency\"\"\"\n",
					"    # Remove any trailing slashes\n",
					"    path = path.rstrip('/')\n",
					"    \n",
					"    # Check if path is missing container info\n",
					"    if not path.startswith(\"abfss://\"):\n",
					"        # Add default storage and container\n",
					"        path = f\"abfss://juror- etl@baubaisadfsastg.dfs.core.windows.net/{path}\"\n",
					"    \n",
					"    return path"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import StringType, IntegerType, TimestampType\n",
					"\n",
					"# Define global schema\n",
					"TARGET_SCHEMA = [\n",
					"    (\"part_no\", StringType()),\n",
					"    (\"register_lett\", IntegerType()),\n",
					"    (\"poll_number\", IntegerType()),\n",
					"    (\"new_marker\", StringType()),\n",
					"    (\"title\", StringType()),\n",
					"    (\"lname\", StringType()),\n",
					"    (\"fname\", StringType()),\n",
					"    (\"dob\", StringType()),\n",
					"    (\"flags\", StringType()),\n",
					"    (\"address\", StringType()),\n",
					"    (\"address2\", StringType()),\n",
					"    (\"address3\", StringType()),\n",
					"    (\"address4\", StringType()),\n",
					"    (\"address5\", StringType()),\n",
					"    (\"address6\", StringType()),\n",
					"    (\"zip\", StringType()),\n",
					"    (\"date_selected1\", TimestampType()),\n",
					"    (\"date_selected2\", TimestampType()),\n",
					"    (\"date_selected3\", TimestampType()),\n",
					"    (\"rec_num\", IntegerType()),\n",
					"    (\"perm_disqual\", StringType()),\n",
					"    (\"source_id\", StringType()),\n",
					"    (\"postcode_start\", StringType()),\n",
					"    (\"hash_id\", StringType()),\n",
					"    (\"creation_date\", StringType())\n",
					"]"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"source": [
					"def load_config():\n",
					"    \"\"\"Load configuration including storage paths\"\"\"\n",
					"    storage_account = \"baubaisadfsastg\"\n",
					"    config = {\n",
					"        \"storage_account\": storage_account,\n",
					"        \"input_path\": f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/transformation\",\n",
					"        \"config_path\": f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/config/schema\",\n",
					"        \"output_path\": f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_temp\"\n",
					"    }\n",
					"    \n",
					"    # Normalize all paths\n",
					"    config[\"output_path\"] = normalize_storage_path(config[\"output_path\"])\n",
					"    config[\"input_path\"] = normalize_storage_path(config[\"input_path\"])\n",
					"    config[\"config_path\"] = normalize_storage_path(config[\"config_path\"])\n",
					"    \n",
					"    return config"
				],
				"execution_count": 36
			},
			{
				"cell_type": "code",
				"source": [
					"def process_postcodes(df: DataFrame) -> DataFrame:\n",
					"    \"\"\"\n",
					"    Process postcodes to standardize format and handle invalid values\n",
					"    \"\"\"\n",
					"    # Get postcode settings from our loaded configuration\n",
					"    postcode_settings = config_dict.get(\"validation_rules\", {}).get(\"POSTCODE_SETTINGS\", {})\n",
					"    \n",
					"    # Check if we have the necessary postcode columns\n",
					"    possible_columns = postcode_settings.get(\"possible_columns\", [\"PostCode\", \"zip\"])\n",
					"    postcode_columns = [col_name for col_name in possible_columns if col_name in df.columns]\n",
					"    \n",
					"    if not postcode_columns:\n",
					"        print(\"No postcode columns found in DataFrame\")\n",
					"        return df\n",
					"        \n",
					"    # Get the main postcode column (usually \"zip\" in target schema)\n",
					"    main_postcode_col = \"zip\" if \"zip\" in df.columns else postcode_columns[0]\n",
					"    \n",
					"    # Get validation settings\n",
					"    validation_pattern = postcode_settings.get(\"validation_pattern\", r'^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$')\n",
					"    extraction_pattern = postcode_settings.get(\"extraction_pattern\", r'([A-Z0-9]{1,4})[ -]*([0-9][A-Z]{2})')\n",
					"    format_template = postcode_settings.get(\"format_template\", '$1 $2')\n",
					"    remove_invalid = postcode_settings.get(\"remove_invalid\", False)\n",
					"    \n",
					"    # Standardize format\n",
					"    df = df.withColumn(\n",
					"        main_postcode_col,\n",
					"        when(col(main_postcode_col).isNotNull(),\n",
					"             regexp_replace(upper(col(main_postcode_col)), extraction_pattern, format_template)\n",
					"        ).otherwise(col(main_postcode_col))\n",
					"    )\n",
					"    \n",
					"    # Filter invalid if requested\n",
					"    if remove_invalid:\n",
					"        df = df.filter(\n",
					"            col(main_postcode_col).isNull() | col(main_postcode_col).rlike(validation_pattern)\n",
					"        )\n",
					"    \n",
					"    # Extract postcode_start if needed\n",
					"    if \"postcode_start\" in df.columns:\n",
					"        postcode_start_pattern = postcode_settings.get(\"postcode_start_pattern\", r'^([A-Z]{1,2}[0-9][A-Z0-9]?)')\n",
					"        df = df.withColumn(\n",
					"            \"postcode_start\",\n",
					"            regexp_extract(col(main_postcode_col), postcode_start_pattern, 1)\n",
					"        )\n",
					"    \n",
					"    return df\n",
					"\n",
					"def enforce_null_columns(df: DataFrame) -> DataFrame:\n",
					"    \"\"\"\n",
					"    Set specific columns to NULL as defined in configuration\n",
					"    \"\"\"\n",
					"    null_columns = config_dict.get(\"validation_rules\", {}).get(\"NULL_COLUMNS\", [])\n",
					"    \n",
					"    for col_name in null_columns:\n",
					"        if col_name in df.columns:\n",
					"            df = df.withColumn(col_name, lit(None))\n",
					"    \n",
					"    return df\n",
					"\n",
					"def create_target_schema_df(df: DataFrame) -> DataFrame:\n",
					"    \"\"\"\n",
					"    Transform DataFrame to target schema\n",
					"    \"\"\"\n",
					"    # Get target schema from configuration\n",
					"    target_schema = config_dict.get(\"validation_rules\", {}).get(\"TARGET_SCHEMA\", [])\n",
					"    column_mappings = config_dict.get(\"validation_rules\", {}).get(\"COLUMN_MAPPINGS\", {})\n",
					"    \n",
					"    if not target_schema:\n",
					"        print(\"No target schema defined in configuration\")\n",
					"        return df\n",
					"    \n",
					"    # Get all target column names\n",
					"    target_columns = [col_def[\"name\"] for col_def in target_schema]\n",
					"    \n",
					"    # First, apply column mappings if needed\n",
					"    for source_col, target_col in column_mappings.items():\n",
					"        if source_col in df.columns and source_col != target_col:\n",
					"            # If target column doesn't exist, rename the source column\n",
					"            if target_col not in df.columns:\n",
					"                df = df.withColumnRenamed(source_col, target_col)\n",
					"            # If both exist, keep target and drop source\n",
					"            elif source_col != target_col:\n",
					"                df = df.drop(source_col)\n",
					"    \n",
					"    # Add any missing columns with NULL values\n",
					"    for col_def in target_schema:\n",
					"        col_name = col_def[\"name\"]\n",
					"        if col_name not in df.columns:\n",
					"            df = df.withColumn(col_name, lit(None))\n",
					"    \n",
					"    # Select only the columns in the target schema, in the correct order\n",
					"    df = df.select(target_columns)\n",
					"    \n",
					"    return df\n",
					"\n",
					"def enforce_column_lengths(df: DataFrame) -> DataFrame:\n",
					"    \"\"\"\n",
					"    Enforce maximum length constraints on string columns\n",
					"    \"\"\"\n",
					"    # Get string columns with length constraints from configuration\n",
					"    string_columns = config_dict.get(\"column_constraints\", {}).get(\"string_columns\", {})\n",
					"    \n",
					"    for col_name, constraints in string_columns.items():\n",
					"        if col_name in df.columns and \"max_length\" in constraints:\n",
					"            max_length = constraints[\"max_length\"]\n",
					"            df = df.withColumn(\n",
					"                col_name,\n",
					"                when(\n",
					"                    (col(col_name).isNotNull()) & (length(col(col_name)) > max_length),\n",
					"                    substring(col(col_name), 1, max_length)\n",
					"                ).otherwise(col(col_name))\n",
					"            )\n",
					"            \n",
					"            # Apply case transformation if specified\n",
					"            if \"case\" in constraints and constraints[\"case\"] == \"upper\":\n",
					"                df = df.withColumn(\n",
					"                    col_name,\n",
					"                    when(col(col_name).isNotNull(), upper(col(col_name)))\n",
					"                    .otherwise(col(col_name))\n",
					"                )\n",
					"    \n",
					"    return df\n",
					"\n",
					"def process_dataframe(df):\n",
					"    \"\"\"\n",
					"    Apply all validation and transformation rules to a DataFrame.\n",
					"    \"\"\"\n",
					"    logger = logging.getLogger()\n",
					"    logger.info(f\"Starting data processing with {df.count()} rows\")\n",
					"    \n",
					"    try:\n",
					"        # Step 1: Process postcodes (standardize and filter invalid)\n",
					"        df = process_postcodes(df)\n",
					"        \n",
					"        # Step 2: Enforce NULL columns\n",
					"        df = enforce_null_columns(df)\n",
					"        \n",
					"        # Step 3: Transform to target schema\n",
					"        df = create_target_schema_df(df)\n",
					"        \n",
					"        # Step 4: Enforce column length limits\n",
					"        df = enforce_column_lengths(df)\n",
					"        \n",
					"        logger.info(f\"Completed data processing with {df.count()} rows\")\n",
					"        return df\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error during DataFrame processing: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        \n",
					"        # Return the original DataFrame as fallback\n",
					"        logger.warning(\"Returning original DataFrame due to processing error\")\n",
					"        return df"
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import concat_ws, collect_list\n",
					"def read_json_config(storage_account, config_path):\n",
					"    \"\"\"Read JSON configuration from Azure Data Lake Storage using Spark's text reader\"\"\"\n",
					"    try:\n",
					"        from pyspark.sql import SparkSession\n",
					"        spark = SparkSession.builder.getOrCreate()\n",
					"        \n",
					"        # Read the file as text\n",
					"        df = spark.read.text(config_path)\n",
					"        \n",
					"        # Collect all lines and concatenate\n",
					"        json_str = df.agg(concat_ws(\"\", collect_list(\"value\"))).collect()[0][0]\n",
					"        \n",
					"        # Parse JSON\n",
					"        import json\n",
					"        config = json.loads(json_str)\n",
					"        \n",
					"        print(f\"Successfully read configuration from {config_path}\")\n",
					"        return config\n",
					"    except Exception as e:\n",
					"        print(f\"Error reading configuration from {config_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        print(traceback.format_exc())\n",
					"        return {}\n",
					"\n",
					"# Read configuration files\n",
					"storage_account = \"baubaisadfsastg\"\n",
					"validation_rules = read_json_config(\n",
					"    storage_account, \n",
					"    f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/config/rules/validation_rules.json\"\n",
					")\n",
					"\n",
					"validation_handlers = read_json_config(\n",
					"    storage_account, \n",
					"    f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/config/rules/validation_handlers.json\"\n",
					")\n",
					"\n",
					"column_constraints = read_json_config(\n",
					"    storage_account, \n",
					"    f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/config/rules/column_constraints.json\"\n",
					")"
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"source": [
					"# Create a global configuration dictionary\n",
					"config_dict = {\n",
					"    \"validation_rules\": validation_rules,\n",
					"    \"validation_handlers\": validation_handlers,\n",
					"    \"column_constraints\": column_constraints\n",
					"}"
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"source": [
					"def validate_and_transform_all_columns(df):\n",
					"    \"\"\"\n",
					"    Comprehensive validation and transformation function based on JSON configuration\n",
					"    \"\"\"\n",
					"    from pyspark.sql.functions import col, when, upper, regexp_extract, regexp_replace, length, substring\n",
					"    \n",
					"    # Skip if configurations are not available\n",
					"    if not validation_rules or not column_constraints:\n",
					"        print(\"Required configurations not available, skipping validation\")\n",
					"        return df\n",
					"    \n",
					"    print(\"Applying column validation and transformations...\")\n",
					"    \n",
					"    # 1. Apply string length constraints\n",
					"    string_columns = column_constraints.get(\"string_columns\", {})\n",
					"    for col_name, constraints in string_columns.items():\n",
					"        if col_name in df.columns and \"max_length\" in constraints:\n",
					"            max_length = constraints[\"max_length\"]\n",
					"            df = df.withColumn(\n",
					"                col_name,\n",
					"                when(\n",
					"                    (col(col_name).isNotNull()) & (length(col(col_name)) > max_length),\n",
					"                    substring(col(col_name), 1, max_length)\n",
					"                ).otherwise(col(col_name))\n",
					"            )\n",
					"            \n",
					"            # Apply case transformation if specified\n",
					"            if \"case\" in constraints and constraints[\"case\"] == \"upper\":\n",
					"                df = df.withColumn(\n",
					"                    col_name,\n",
					"                    when(col(col_name).isNotNull(), upper(col(col_name)))\n",
					"                    .otherwise(col(col_name))\n",
					"                )\n",
					"    \n",
					"    # 2. Standardize postcodes if configured\n",
					"    if \"POSTCODE_SETTINGS\" in validation_rules and \"zip\" in df.columns:\n",
					"        settings = validation_rules[\"POSTCODE_SETTINGS\"]\n",
					"        \n",
					"        if \"extraction_pattern\" in settings and \"format_template\" in settings:\n",
					"            # Standardize format of valid postcodes\n",
					"            df = df.withColumn(\n",
					"                \"zip\",\n",
					"                regexp_replace(col(\"zip\"), settings[\"extraction_pattern\"], settings[\"format_template\"])\n",
					"            )\n",
					"        \n",
					"        # Extract postcode_start if column exists\n",
					"        if \"postcode_start\" in df.columns and \"postcode_start_pattern\" in settings:\n",
					"            df = df.withColumn(\n",
					"                \"postcode_start\",\n",
					"                regexp_extract(col(\"zip\"), settings[\"postcode_start_pattern\"], 1)\n",
					"            )\n",
					"    \n",
					"    # 3. Set NULL columns as specified in configuration\n",
					"    if \"NULL_COLUMNS\" in validation_rules:\n",
					"        null_columns = validation_rules[\"NULL_COLUMNS\"]\n",
					"        for col_name in null_columns:\n",
					"            if col_name in df.columns:\n",
					"                df = df.withColumn(col_name, lit(None))\n",
					"    \n",
					"    # 4. Apply any other required transformations from configuration\n",
					"    transformations = column_constraints.get(\"transformations\", {})\n",
					"    \n",
					"    # Check if special character cleaning is enabled\n",
					"    if transformations.get(\"clean_special_characters\", False):\n",
					"        # Apply to string columns\n",
					"        for col_name in string_columns.keys():\n",
					"            if col_name in df.columns:\n",
					"                df = df.withColumn(\n",
					"                    col_name,\n",
					"                    regexp_replace(col(col_name), \"[^\\\\w\\\\s-,.]\", \"\")\n",
					"                )\n",
					"    \n",
					"    print(\"Validation and transformations applied successfully\")\n",
					"    return df"
				],
				"execution_count": 40
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Read Configuration and Mapping"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Column Mapping"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def read_column_mapping(spark, config_path):\n",
					"    \"\"\"Read and parse the column mapping configuration\"\"\"\n",
					"    try:\n",
					"        # Read the JSON file\n",
					"        mapping_df = spark.read.text(config_path)\n",
					"        \n",
					"        # Combine all lines into a single string\n",
					"        json_str = mapping_df.agg(concat_ws(\"\", collect_list(\"value\"))).collect()[0][0]\n",
					"        \n",
					"        # Parse the JSON string\n",
					"        mapping_dict = json.loads(json_str)\n",
					"        \n",
					"        # Check if it has 'schema' structure\n",
					"        if \"schema\" in mapping_dict:\n",
					"            # Convert schema format to mappings format\n",
					"            mappings = {}\n",
					"            for item in mapping_dict[\"schema\"]:\n",
					"                # Use the 'name' field as the standard column name\n",
					"                # Initially set empty variations list\n",
					"                mappings[item[\"name\"]] = []\n",
					"            \n",
					"            return mappings\n",
					"        \n",
					"        # If looking for mappings specifically\n",
					"        elif \"mappings\" in mapping_dict:\n",
					"            return mapping_dict[\"mappings\"]\n",
					"        \n",
					"        # If neither structure is found, raise an error\n",
					"        else:\n",
					"            raise KeyError(\"Neither 'schema' nor 'mappings' found in mapping file\")\n",
					"            \n",
					"    except Exception as e:\n",
					"        try:\n",
					"            # Fallback to using wholeTextFiles\n",
					"            mapping_rdd = spark.sparkContext.wholeTextFiles(config_path)\n",
					"            json_str = mapping_rdd.values().first()\n",
					"            mapping_dict = json.loads(json_str)\n",
					"            \n",
					"            # Check the structure here too\n",
					"            if \"schema\" in mapping_dict:\n",
					"                mappings = {}\n",
					"                for item in mapping_dict[\"schema\"]:\n",
					"                    mappings[item[\"name\"]] = []\n",
					"                return mappings\n",
					"            elif \"mappings\" in mapping_dict:\n",
					"                return mapping_dict[\"mappings\"]\n",
					"            else:\n",
					"                raise KeyError(\"Neither 'schema' nor 'mappings' found in mapping file\")\n",
					"                \n",
					"        except Exception as e2:\n",
					"            raise Exception(f\"Failed to read mapping file. Primary error: {str(e)}, Fallback error: {str(e2)}\")"
				],
				"execution_count": 41
			},
			{
				"cell_type": "code",
				"source": [
					"def standardize_columns(df, column_mapping):\n",
					"    \"\"\"\n",
					"    Standardize column names based on mapping\n",
					"    \n",
					"    Args:\n",
					"        df: Input DataFrame\n",
					"        column_mapping: Dictionary mapping standard names to variations\n",
					"        \n",
					"    Returns:\n",
					"        DataFrame with standardized column names\n",
					"    \"\"\"\n",
					"    # Get current columns\n",
					"    current_columns = df.columns\n",
					"    \n",
					"    # Create a reverse mapping (from variations to standard names)\n",
					"    reverse_mapping = {}\n",
					"    for standard_name, variations in column_mapping.items():\n",
					"        # Add the standard name itself to the reverse mapping\n",
					"        reverse_mapping[standard_name.lower()] = standard_name\n",
					"        \n",
					"        # Add all variations\n",
					"        for variation in variations:\n",
					"            reverse_mapping[variation.lower()] = standard_name\n",
					"    \n",
					"    # Apply renaming\n",
					"    for col in current_columns:\n",
					"        # Check if column matches any standard name or variation (case-insensitive)\n",
					"        std_name = reverse_mapping.get(col.lower())\n",
					"        if std_name and col != std_name:\n",
					"            df = df.withColumnRenamed(col, std_name)\n",
					"    \n",
					"    return df\n",
					"def standardize_columns(df, column_mapping):\n",
					"    \"\"\"\n",
					"    Standardize column names based on mapping\n",
					"    \n",
					"    Args:\n",
					"        df: Input DataFrame\n",
					"        column_mapping: Dictionary mapping standard names to variations\n",
					"        \n",
					"    Returns:\n",
					"        DataFrame with standardized column names\n",
					"    \"\"\"\n",
					"    # Get current columns\n",
					"    current_columns = df.columns\n",
					"    \n",
					"    # Create a reverse mapping (from variations to standard names)\n",
					"    reverse_mapping = {}\n",
					"    for standard_name, variations in column_mapping.items():\n",
					"        # Add the standard name itself to the reverse mapping\n",
					"        reverse_mapping[standard_name.lower()] = standard_name\n",
					"        \n",
					"        # Add all variations\n",
					"        for variation in variations:\n",
					"            reverse_mapping[variation.lower()] = standard_name\n",
					"    \n",
					"    # Apply renaming\n",
					"    for col in current_columns:\n",
					"        # Check if column matches any standard name or variation (case-insensitive)\n",
					"        std_name = reverse_mapping.get(col.lower())\n",
					"        if std_name and col != std_name:\n",
					"            df = df.withColumnRenamed(col, std_name)\n",
					"    \n",
					"    return df\n",
					"\n",
					""
				],
				"execution_count": 42
			},
			{
				"cell_type": "code",
				"source": [
					"def ensure_register_lett_exists(df):\n",
					"    \"\"\"\n",
					"    Ensure register_lett exists in the DataFrame and is synchronized with poll_number.\n",
					"    Returns the DataFrame with register_lett added/synchronized if needed.\n",
					"    \"\"\"\n",
					"    # Check if register_lett already exists\n",
					"    has_reg_lett = \"register_lett\" in df.columns\n",
					"    \n",
					"    # If it doesn't exist, add it from poll_number or Elector Number\n",
					"    if not has_reg_lett:\n",
					"        if \"poll_number\" in df.columns:\n",
					"            print(\"Adding register_lett as copy of poll_number\")\n",
					"            df = df.withColumn(\"register_lett\", col(\"poll_number\"))\n",
					"        elif \"Elector Number\" in df.columns:\n",
					"            print(\"Adding register_lett as copy of Elector Number\")\n",
					"            df = df.withColumn(\"register_lett\", col(\"Elector Number\"))\n",
					"    else:\n",
					"        # If it does exist, make sure it matches poll_number\n",
					"        if \"poll_number\" in df.columns:\n",
					"            mismatches = df.filter(col(\"register_lett\") != col(\"poll_number\")).count()\n",
					"            if mismatches > 0:\n",
					"                print(f\"Found {mismatches} mismatches between register_lett and poll_number. Synchronizing...\")\n",
					"                df = df.withColumn(\"register_lett\", col(\"poll_number\"))\n",
					"    \n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def read_schema_config(spark, config_path):\n",
					"    \"\"\"Read and parse the schema configuration\"\"\"\n",
					"    try:\n",
					"        schema_df = spark.read.text(config_path)\n",
					"        json_str = schema_df.agg(concat_ws(\"\", collect_list(\"value\"))).collect()[0][0]\n",
					"        schema_dict = json.loads(json_str)\n",
					"        return schema_dict[\"schema\"]\n",
					"    except Exception as e:\n",
					"        try:\n",
					"            schema_rdd = spark.sparkContext.wholeTextFiles(config_path)\n",
					"            json_str = schema_rdd.values().first()\n",
					"            schema_dict = json.loads(json_str)\n",
					"            return schema_dict[\"schema\"]\n",
					"        except Exception as e2:\n",
					"            raise Exception(f\"Failed to read schema file. Primary error: {str(e)}, Fallback error: {str(e2)}\")"
				],
				"execution_count": 43
			},
			{
				"cell_type": "code",
				"source": [
					"def ensure_consistent_schema(df, required_columns):\n",
					"    \"\"\"Ensure DataFrame has all required columns with correct data types\"\"\"\n",
					"    current_columns = set(df.columns)\n",
					"    \n",
					"    # Add missing columns\n",
					"    for col_name, col_type in required_columns:\n",
					"        if col_name not in current_columns:\n",
					"            df = df.withColumn(col_name, lit(None).cast(col_type))\n",
					"    \n",
					"    # Select only the required columns in the specified order\n",
					"    return df.select([col(c[0]).cast(c[1]) for c in required_columns])"
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"source": [
					"def get_new_files(spark, input_path, process_log_path):\n",
					"    \"\"\"Identify which files are new and need processing\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    # Get all files in the input directory\n",
					"    all_files = []\n",
					"    \n",
					"    # List all items in the directory\n",
					"    try:\n",
					"        items = mssparkutils.fs.ls(input_path)\n",
					"        \n",
					"        # Log what we found for debugging\n",
					"        print(f\"Found {len(items)} items in {input_path}\")\n",
					"        \n",
					"        for item in items:\n",
					"            # Try multiple ways to identify Parquet data\n",
					"            if (hasattr(item, 'path') and (\n",
					"                   item.path.endswith(\".parquet\") or \n",
					"                   # Also check for Parquet directories that Spark might have written\n",
					"                   (hasattr(item, 'isDir') and item.isDir and \n",
					"                    mssparkutils.fs.exists(f\"{item.path}/_SUCCESS\"))\n",
					"               )):\n",
					"                all_files.append(item.path)\n",
					"                print(f\"Added file/directory for processing: {item.path}\")\n",
					"    except Exception as e:\n",
					"        print(f\"Error listing files in {input_path}: {str(e)}\")\n",
					"    \n",
					"    # Read process log to get previously processed files\n",
					"    processed_files = []\n",
					"    if mssparkutils.fs.exists(process_log_path):\n",
					"        try:\n",
					"            processed_files = spark.read.parquet(process_log_path) \\\n",
					"                                  .select(\"file_path\").rdd.flatMap(lambda x: x).collect()\n",
					"        except Exception as e:\n",
					"            print(f\"Error reading process log: {str(e)}\")\n",
					"    \n",
					"    # Identify new files with more detailed logging\n",
					"    new_files = []\n",
					"    for f in all_files:\n",
					"        if f not in processed_files:\n",
					"            new_files.append(f)\n",
					"            print(f\"New file to process: {f}\")\n",
					"        else:\n",
					"            print(f\"Skipping already processed file: {f}\")\n",
					"    \n",
					"    print(f\"Found {len(new_files)} new files to process out of {len(all_files)} total files\")\n",
					"    return new_files, all_files"
				],
				"execution_count": 45
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def process_input_files(spark, file_paths, column_mapping):\n",
					"    \"\"\"Read and standardize columns from input files with reg_lett handling\"\"\"\n",
					"    processed_dfs = []\n",
					"    \n",
					"    for file_path in file_paths:\n",
					"        try:\n",
					"            # Read the file\n",
					"            df = spark.read.parquet(file_path)\n",
					"            \n",
					"            # Standardize column names based on mapping\n",
					"            standardized_df = standardize_columns(df, column_mapping)\n",
					"            \n",
					"            # Check if reg_lett is missing but poll_number or Elector Number exists\n",
					"            has_reg_lett = \"register_lett\" in standardized_df.columns or \"reg_lett\" in standardized_df.columns\n",
					"            \n",
					"            if not has_reg_lett:\n",
					"                # Find the poll number column\n",
					"                poll_number_col = None\n",
					"                if \"poll_number\" in standardized_df.columns:\n",
					"                    poll_number_col = \"poll_number\"\n",
					"                elif \"Elector Number\" in standardized_df.columns:\n",
					"                    poll_number_col = \"Elector Number\"\n",
					"                \n",
					"                # Add reg_lett as a copy of poll_number if it exists\n",
					"                if poll_number_col:\n",
					"                    print(f\"Adding register_lett as copy of {poll_number_col} for file {file_path}\")\n",
					"                    standardized_df = standardized_df.withColumn(\"register_lett\", col(poll_number_col))\n",
					"            \n",
					"            # Add source metadata\n",
					"            standardized_df = standardized_df.withColumn(\"source_file\", lit(file_path))\n",
					"            standardized_df = standardized_df.withColumn(\"process_timestamp\", current_timestamp())\n",
					"            \n",
					"            processed_dfs.append(standardized_df)\n",
					"            \n",
					"        except Exception as e:\n",
					"            print(f\"Error processing {file_path}: {str(e)}\")\n",
					"            import traceback\n",
					"            print(traceback.format_exc())\n",
					"    \n",
					"    return processed_dfs"
				],
				"execution_count": 46
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Merge Files with Standardized Schema"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def merge_dataframes(dfs):\n",
					"    \"\"\"Merge all dataframes with standardized schema, preserving original date information\"\"\"\n",
					"    if not dfs:\n",
					"        raise ValueError(\"No dataframes to merge\")\n",
					"        \n",
					"    # Merge using unionByName to handle schema variations\n",
					"    merged_df = dfs[0]\n",
					"    for df in dfs[1:]:\n",
					"        merged_df = merged_df.unionByName(df, allowMissingColumns=True)\n",
					"    \n",
					"    # Check which date column exists in the merged data\n",
					"    date_column = None\n",
					"    if \"CreationDate\" in merged_df.columns:\n",
					"        date_column = \"CreationDate\"\n",
					"    elif \"creation_date\" in merged_df.columns:\n",
					"        date_column = \"creation_date\"\n",
					"    \n",
					"    # Only perform date check if we found a date column\n",
					"    if date_column:\n",
					"        # Check if date values are too uniform\n",
					"        distinct_dates = merged_df.select(date_column).distinct().count()\n",
					"        logging.info(f\"Found {distinct_dates} distinct dates in column {date_column}\")\n",
					"        \n",
					"        if distinct_dates == 1:\n",
					"            logging.warning(f\"Only one distinct date found in {date_column} - this may indicate a problem!\")\n",
					"            \n",
					"            # Try to extract better date information from source_file if available\n",
					"            if \"source_file\" in merged_df.columns:\n",
					"                merged_df = merged_df.withColumn(\n",
					"                    \"extracted_date\",\n",
					"                    regexp_extract(col(\"source_file\"), r\"(\\d{8})_\", 1)\n",
					"                )\n",
					"                \n",
					"                # Use extracted date if available, otherwise keep existing\n",
					"                merged_df = merged_df.withColumn(\n",
					"                    date_column,  # Update whichever date column exists\n",
					"                    when(col(\"extracted_date\").isNotNull(), col(\"extracted_date\"))\n",
					"                    .otherwise(col(date_column))\n",
					"                )\n",
					"                \n",
					"                # Clean up temporary column\n",
					"                merged_df = merged_df.drop(\"extracted_date\")\n",
					"                \n",
					"                logging.info(f\"After date correction: {merged_df.select(date_column).distinct().count()} distinct dates\")\n",
					"    else:\n",
					"        logging.warning(\"No date column found in merged data!\")\n",
					"    \n",
					"    return merged_df"
				],
				"execution_count": 47
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Deduplicate Based on Latest Data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def deduplicate_by_creation_date(df):\n",
					"    \"\"\"Keep only the most recent record for each unique entity based on creation date\"\"\"\n",
					"    # Check if we have a creation date column\n",
					"    if \"CreationDate\" in df.columns:\n",
					"        # Use combination of name and address as a unique identifier instead of LA_Code\n",
					"        window_spec = Window.partitionBy(\n",
					"            \"Elector Surname\", \"Elector Forename\", \"Address1\", \"PostCode\"\n",
					"        ).orderBy(desc(\"CreationDate\"))\n",
					"        \n",
					"        deduplicated = df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                         .filter(col(\"row_num\") == 1) \\\n",
					"                         .drop(\"row_num\")\n",
					"        \n",
					"        logging.info(f\"Kept {deduplicated.count()} records after transformation\")\n",
					"        return deduplicated\n",
					"    else:\n",
					"        logging.warning(\"Missing CreationDate column, skipping transformation\")\n",
					"        return df"
				],
				"execution_count": 48
			},
			{
				"cell_type": "code",
				"source": [
					"def optimise_deduplicate_data(df):\n",
					"    \"\"\"Extremely conservative deduplication to preserve row count\"\"\"\n",
					"    # Get current columns\n",
					"    actual_columns = df.columns\n",
					"    \n",
					"    print(f\"PRE-DEDUPLICATION COUNT: {df.count()} rows\")\n",
					"    \n",
					"    # We need to use a much more specific combination of columns to identify duplicates\n",
					"    # Only remove exact duplicates (rows that are identical across all columns)\n",
					"    try:\n",
					"        # Either use a more comprehensive set of ID columns\n",
					"        id_columns = []\n",
					"        \n",
					"        # Use a different approach - focus on all the key identifying fields together\n",
					"        key_fields = [\n",
					"            \"Elector Number\",\"LA_code\",\"poll_number\", \"register_lett\", \"rec_num\",\n",
					"            \"Elector Surname\", \"Elector Forename\", \"Address1\", \"PostCode\"\n",
					"        ]\n",
					"        \n",
					"        # Only use columns that actually exist\n",
					"        id_columns = [col for col in key_fields if col in actual_columns]\n",
					"        \n",
					"        # If we have less than 3 identifying columns, use all non-metadata columns\n",
					"        if len(id_columns) < 3:\n",
					"            # Exclude metadata columns like timestamps, source_file, etc.\n",
					"            metadata_cols = [\"source_file\", \"process_timestamp\", \"CreationDate\", \"creation_date\"]\n",
					"            id_columns = [col for col in actual_columns if col not in metadata_cols]\n",
					"        \n",
					"        print(f\"Using these columns for deduplication: {id_columns}\")\n",
					"        \n",
					"        if len(id_columns) == 0:\n",
					"            print(\"No suitable columns for deduplication found - returning all data\")\n",
					"            return df\n",
					"            \n",
					"        # Use dropDuplicates instead of window functions for exact matching\n",
					"        deduplicated = df.dropDuplicates(id_columns)\n",
					"        \n",
					"        final_count = deduplicated.count()\n",
					"        print(f\"POST-DEDUPLICATION COUNT: {final_count} rows\")\n",
					"        print(f\"ROWS REMOVED: {df.count() - final_count}\")\n",
					"        \n",
					"        return deduplicated\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"ERROR IN DEDUPLICATION: {str(e)}\")\n",
					"        import traceback\n",
					"        traceback.print_exc()\n",
					"        print(\"Returning original DataFrame\")\n",
					"        return df"
				],
				"execution_count": 49
			},
			{
				"cell_type": "code",
				"source": [
					"def improved_deduplicate_data(df):\n",
					"    \"\"\"\n",
					"    Improved deduplication using window functions to keep most recent record\n",
					"    per set of identifying columns\n",
					"    \"\"\"\n",
					"    # Define identifying columns\n",
					"    id_columns = [\n",
					"        \"fname\", \"lname\", \"address\", \"poll_number\", \"rec_num\", \"hash_id\"\n",
					"    ]\n",
					"    \n",
					"    # Filter list to only include columns that exist in the dataframe\n",
					"    existing_id_columns = [col for col in id_columns if col in df.columns]\n",
					"    \n",
					"    if len(existing_id_columns) == 0:\n",
					"        print(\"No suitable columns for deduplication found - returning all data\")\n",
					"        return df\n",
					"    \n",
					"    print(f\"Deduplicating using columns: {existing_id_columns}\")\n",
					"    print(f\"Input record count: {df.count()}\")\n",
					"    \n",
					"    # Determine which date column to use for ordering\n",
					"    date_column = None\n",
					"    for possible_date_col in [\"creation_date\", \"CreationDate\", \"process_timestamp\"]:\n",
					"        if possible_date_col in df.columns:\n",
					"            date_column = possible_date_col\n",
					"            break\n",
					"    \n",
					"    if date_column:\n",
					"        print(f\"Using {date_column} for ordering records\")\n",
					"        # Create window spec partitioned by identifying columns with date ordering\n",
					"        window_spec = Window.partitionBy([col(x) for x in existing_id_columns]) \\\n",
					"                           .orderBy(col(date_column).desc())\n",
					"    else:\n",
					"        print(\"No date column found for ordering, using default ordering\")\n",
					"        # If no date column, just use window without ordering\n",
					"        window_spec = Window.partitionBy([col(x) for x in existing_id_columns])\n",
					"    \n",
					"    # Apply deduplication\n",
					"    deduplicated_df = df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                        .filter(col(\"row_num\") == 1) \\\n",
					"                        .drop(\"row_num\")\n",
					"    \n",
					"    output_count = deduplicated_df.count()\n",
					"    print(f\"Output record count: {output_count}\")\n",
					"    print(f\"Removed {df.count() - output_count} duplicate records\")\n",
					"    \n",
					"    return deduplicated_df"
				],
				"execution_count": 50
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Apply Hashing for Sensitive Data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def generate_hash_id(df):\n",
					"    \"\"\"\n",
					"    Generate a hash_id using key identifying fields and verify that\n",
					"    register_lett and poll_number are consistent.\n",
					"    \"\"\"\n",
					"    from pyspark.sql.functions import col, xxhash64, lit, concat_ws, coalesce, trim, upper\n",
					"    \n",
					"    # First, check if both register_lett and poll_number exist\n",
					"    has_reg_lett = \"register_lett\" in df.columns or \"reg_lett\" in df.columns\n",
					"    has_poll_number = \"poll_number\" in df.columns or \"Elector Number\" in df.columns\n",
					"    reg_lett_col = \"register_lett\" if \"register_lett\" in df.columns else \"reg_lett\" if \"reg_lett\" in df.columns else None\n",
					"    poll_number_col = \"poll_number\" if \"poll_number\" in df.columns else \"Elector Number\" if \"Elector Number\" in df.columns else None\n",
					"    \n",
					"    # If both exist, check if they have consistent values\n",
					"    if has_reg_lett and has_poll_number and reg_lett_col is not None and poll_number_col is not None:\n",
					"        # Compare values and log discrepancies\n",
					"        mismatched = df.filter(col(reg_lett_col) != col(poll_number_col)).count()\n",
					"        if mismatched > 0:\n",
					"            print(f\"WARNING: Found {mismatched} rows where {reg_lett_col} does not match {poll_number_col}\")\n",
					"            print(\"This may affect the accuracy of the hash_id generation\")\n",
					"            \n",
					"            # Optional: Fix the mismatch by making register_lett match poll_number\n",
					"            # df = df.withColumn(reg_lett_col, col(poll_number_col))\n",
					"    \n",
					"    # Define core identifying fields with column name variants\n",
					"    key_fields = [\n",
					"        # (field_type, primary_column_name, alternate_column_name)\n",
					"        (\"register\", \"register_lett\", \"reg_lett\"),\n",
					"        (\"forename\", \"Elector Forename\", \"fname\"),\n",
					"        (\"surname\", \"Elector Surname\", \"lname\"),\n",
					"        (\"rec_num\", \"rec_num\", \"record_number\")\n",
					"    ]\n",
					"    \n",
					"    # Find which columns exist and build hash components\n",
					"    hash_components = []\n",
					"    for field_type, primary, alternate in key_fields:\n",
					"        if primary in df.columns:\n",
					"            hash_components.append(\n",
					"                when(\n",
					"                    (col(primary).isNotNull()) & (trim(col(primary)) != \"\"), \n",
					"                    trim(upper(col(primary)))\n",
					"                ).otherwise(lit(\"\"))\n",
					"            )\n",
					"            print(f\"Using {primary} for {field_type}\")\n",
					"        elif alternate in df.columns:\n",
					"            hash_components.append(\n",
					"                when(\n",
					"                    (col(alternate).isNotNull()) & (trim(col(alternate)) != \"\"), \n",
					"                    trim(upper(col(alternate)))\n",
					"                ).otherwise(lit(\"\"))\n",
					"            )\n",
					"            print(f\"Using {alternate} for {field_type}\")\n",
					"        else:\n",
					"            print(f\"WARNING: No column found for {field_type} - using placeholder\")\n",
					"            hash_components.append(lit(\"\"))\n",
					"    \n",
					"    # Generate the hash_id\n",
					"    df_with_hash = df.withColumn(\n",
					"        \"hash_id\",\n",
					"        xxhash64(concat_ws(\"||\", *hash_components))\n",
					"    )\n",
					"    \n",
					"    # Report basic statistics\n",
					"    row_count = df_with_hash.count()\n",
					"    distinct_hash_count = df_with_hash.select(\"hash_id\").distinct().count()\n",
					"    \n",
					"    print(f\"Hash ID generation statistics:\")\n",
					"    print(f\"  Total rows: {row_count}\")\n",
					"    print(f\"  Distinct hash values: {distinct_hash_count}\")\n",
					"    \n",
					"    if row_count > 0:\n",
					"        duplication_rate = (row_count - distinct_hash_count) / row_count * 100\n",
					"        print(f\"  Duplication rate: {duplication_rate:.2f}%\")\n",
					"    \n",
					"    return df_with_hash"
				],
				"execution_count": 51
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Transform to Target Schema"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def transform_to_target_schema(df, column_mapping):\n",
					"    \"\"\"Transform the dataframe to match the target PostgreSQL schema\"\"\"\n",
					"    print(f\"Starting transformation with DataFrame containing {df.count()} rows\")\n",
					"    \n",
					"    try:\n",
					"        # First, handle special mappings that might not be in the standard mapping\n",
					"        # Specifically map LA_Code to rec_num if needed\n",
					"        if \"LA_Code\" in df.columns and \"rec_num\" not in df.columns:\n",
					"            print(\"Mapping LA_Code to rec_num\")\n",
					"            df = df.withColumnRenamed(\"LA_Code\", \"rec_num\")\n",
					"        \n",
					"        # Apply the standard process_dataframe function\n",
					"        transformed_df = process_dataframe(df)\n",
					"        \n",
					"        print(f\"Transformation complete. Result has {transformed_df.count()} rows and {len(transformed_df.columns)} columns\")\n",
					"        print(f\"Result columns: {transformed_df.columns}\")\n",
					"        \n",
					"        return transformed_df\n",
					"    except Exception as e:\n",
					"        print(f\"Error in transform_to_target_schema: {str(e)}\")\n",
					"        import traceback\n",
					"        print(traceback.format_exc())\n",
					"        \n",
					"        # Create a minimal valid DataFrame instead of returning None\n",
					"        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
					"        schema = StructType([\n",
					"            StructField(field[0], field[1], True) for field in TARGET_SCHEMA\n",
					"        ])\n",
					"        \n",
					"        # Return empty DataFrame with correct schema\n",
					"        return spark.createDataFrame([], schema)"
				],
				"execution_count": 52
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Merge with Existing Data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def merge_with_existing_data(spark, new_data, output_path):\n",
					"    \"\"\"Merge new data with existing data using SQL-based approach with hash_id join\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    # Ensure register_lett exists in new_data\n",
					"    new_data = ensure_register_lett_exists(new_data)\n",
					"    \n",
					"    # Ensure hash_id exists\n",
					"    if \"hash_id\" not in new_data.columns:\n",
					"        new_data = generate_hash_id(new_data)\n",
					"        \n",
					"    # Register temporary views\n",
					"    new_data.createOrReplaceTempView(\"new_data\")\n",
					"    \n",
					"    # Check if existing data is available\n",
					"    if mssparkutils.fs.exists(output_path):\n",
					"        try:\n",
					"            # Read existing data with schema from new_data\n",
					"            print(f\"Reading existing data from {output_path}\")\n",
					"            existing_schema = new_data.schema\n",
					"            \n",
					"            # Try to read with explicit schema inference\n",
					"            existing_data = spark.read.option(\"mergeSchema\", \"true\").schema(existing_schema).parquet(output_path)\n",
					"            \n",
					"            # Ensure register_lett exists in existing_data\n",
					"            existing_data = ensure_register_lett_exists(existing_data)\n",
					"            \n",
					"            # Ensure hash_id exists in existing data\n",
					"            if \"hash_id\" not in existing_data.columns:\n",
					"                existing_data = generate_hash_id(existing_data)\n",
					"                \n",
					"            existing_data.createOrReplaceTempView(\"existing_data\")\n",
					"            \n",
					"            # Now refresh to avoid stale references\n",
					"            spark.sql(\"REFRESH TABLE existing_data\")\n",
					"            \n",
					"            # Use SQL approach with hash_id as the join key\n",
					"            merged_data = spark.sql(\"\"\"\n",
					"                -- Keep all new records that don't exist in the existing data\n",
					"                SELECT n.*\n",
					"                FROM new_data n\n",
					"                LEFT ANTI JOIN existing_data e\n",
					"                ON n.hash_id = e.hash_id\n",
					"                \n",
					"                UNION ALL\n",
					"                \n",
					"                -- For existing records, either keep them or update with new values if available\n",
					"                SELECT \n",
					"                    COALESCE(n.part_no, e.part_no) as part_no,\n",
					"                    COALESCE(n.register_lett, e.register_lett) as register_lett,\n",
					"                    COALESCE(n.poll_number, e.poll_number) as poll_number,\n",
					"                    COALESCE(n.new_marker, e.new_marker) as new_marker,\n",
					"                    COALESCE(n.title, e.title) as title,\n",
					"                    COALESCE(n.lname, e.lname) as lname,\n",
					"                    COALESCE(n.fname, e.fname) as fname,\n",
					"                    COALESCE(n.dob, e.dob) as dob,\n",
					"                    COALESCE(n.flags, e.flags) as flags,\n",
					"                    COALESCE(n.address, e.address) as address,\n",
					"                    COALESCE(n.address2, e.address2) as address2, \n",
					"                    COALESCE(n.address3, e.address3) as address3,\n",
					"                    COALESCE(n.address4, e.address4) as address4,\n",
					"                    COALESCE(n.address5, e.address5) as address5,\n",
					"                    COALESCE(n.address6, e.address6) as address6,\n",
					"                    COALESCE(n.zip, e.zip) as zip,\n",
					"                    COALESCE(n.date_selected1, e.date_selected1) as date_selected1,\n",
					"                    COALESCE(n.date_selected2, e.date_selected2) as date_selected2,\n",
					"                    COALESCE(n.date_selected3, e.date_selected3) as date_selected3,\n",
					"                    COALESCE(n.rec_num, e.rec_num) as rec_num,\n",
					"                    COALESCE(n.perm_disqual, e.perm_disqual) as perm_disqual,\n",
					"                    COALESCE(n.source_id, e.source_id) as source_id,\n",
					"                    COALESCE(n.postcode_start, e.postcode_start) as postcode_start,\n",
					"                    e.hash_id as hash_id, -- Keep the existing hash_id\n",
					"                    -- Use the newer creation date\n",
					"                    CASE \n",
					"                        WHEN n.creation_date > e.creation_date THEN n.creation_date\n",
					"                        ELSE e.creation_date\n",
					"                    END as creation_date\n",
					"                FROM existing_data e\n",
					"                LEFT JOIN new_data n\n",
					"                ON e.hash_id = n.hash_id\n",
					"                WHERE n.hash_id IS NULL OR n.creation_date > e.creation_date\n",
					"            \"\"\")\n",
					"            \n",
					"            # Ensure register_lett is synchronized with poll_number in merged result\n",
					"            merged_data = ensure_register_lett_exists(merged_data)\n",
					"            \n",
					"            return merged_data\n",
					"            \n",
					"        except Exception as e:\n",
					"            logging.error(f\"Error merging with existing data: {str(e)}\")\n",
					"            logging.warning(\"Returning only new data due to merge error\")\n",
					"            import traceback\n",
					"            logging.error(traceback.format_exc())\n",
					"            return new_data\n",
					"    else:\n",
					"        # No existing data, just return the new data\n",
					"        logging.info(f\"Output path {output_path} does not exist yet. Using only new data.\")\n",
					"        return new_data"
				],
				"execution_count": 53
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Output and Update Process Log"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def write_output_safely(df, output_path, description=\"data\"):\n",
					"    \"\"\"Write DataFrame to storage with proper error handling\"\"\"\n",
					"    logger = logging.getLogger()\n",
					"    \n",
					"    try:\n",
					"        # Ensure output directory exists\n",
					"        from notebookutils import mssparkutils\n",
					"        dir_path = \"/\".join(output_path.split(\"/\")[:-1])\n",
					"        \n",
					"        if not mssparkutils.fs.exists(dir_path):\n",
					"            logger.info(f\"Creating output directory: {dir_path}\")\n",
					"            mssparkutils.fs.mkdirs(dir_path)\n",
					"        \n",
					"        # Write data to a temporary path first\n",
					"        temp_path = f\"{dir_path}/temp_{int(time.time())}\"\n",
					"        df.write.mode(\"overwrite\").parquet(temp_path)\n",
					"        \n",
					"        # Move the temporary path to the final output path\n",
					"        if mssparkutils.fs.exists(temp_path):\n",
					"            if mssparkutils.fs.exists(output_path):\n",
					"                mssparkutils.fs.rm(output_path, True)\n",
					"            mssparkutils.fs.mv(temp_path, output_path)\n",
					"        \n",
					"        logger.info(f\"Successfully wrote {df.count()} rows of {description} to {output_path}\")\n",
					"        return True\n",
					"    \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error writing {description} to {output_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return False"
				],
				"execution_count": 54
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"def write_output_and_update_log(spark, merged_data, output_path, new_file_paths, process_log_path):\n",
					"    \"\"\"Write the merged data to output location and update the process log\"\"\"\n",
					"    logger = logging.getLogger()\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    try:\n",
					"        # Ensure output directory exists\n",
					"        dir_path = output_path\n",
					"        if '.' in output_path.split('/')[-1]:  # Check if last part looks like a filename\n",
					"            dir_path = '/'.join(output_path.split('/')[:-1])\n",
					"            \n",
					"        if not mssparkutils.fs.exists(dir_path):\n",
					"            logger.info(f\"Creating output directory: {dir_path}\")\n",
					"            mssparkutils.fs.mkdirs(dir_path)\n",
					"        \n",
					"        # Write the merged data\n",
					"        logger.info(f\"Writing {merged_data.count()} records to {output_path}\")\n",
					"        merged_data.write.mode(\"overwrite\").parquet(output_path)\n",
					"        logger.info(\"Successfully wrote data to output location\")\n",
					"        \n",
					"        # Rest of function remains the same...\n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error writing output or updating log: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return False"
				],
				"execution_count": 55
			},
			{
				"cell_type": "code",
				"source": [
					"def update_process_log(spark, new_file_paths, process_log_path):\n",
					"    \"\"\"Update the processing log with newly processed files\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    from pyspark.sql.types import StringType, TimestampType, StructType, StructField\n",
					"    \n",
					"    try:\n",
					"        # Define schema for the process log\n",
					"        log_schema = StructType([\n",
					"            StructField(\"file_path\", StringType(), False),\n",
					"            StructField(\"process_timestamp\", TimestampType(), False)\n",
					"        ])\n",
					"        \n",
					"        # Create new log entries\n",
					"        timestamp_now = datetime.now()\n",
					"        new_log_entries = [(file_path, timestamp_now) for file_path in new_file_paths]\n",
					"        new_log_df = spark.createDataFrame(new_log_entries, log_schema)\n",
					"        \n",
					"        # Check if the process log exists\n",
					"        if mssparkutils.fs.exists(process_log_path):\n",
					"            try:\n",
					"                existing_log = spark.read.parquet(process_log_path)\n",
					"                existing_log = existing_log.withColumn(\"process_timestamp\", col(\"process_timestamp\").cast(\"timestamp\"))\n",
					"                updated_log = existing_log.union(new_log_df)\n",
					"            except Exception as e:\n",
					"                print(f\"Error reading existing log: {str(e)}. Creating new log.\")\n",
					"                updated_log = new_log_df\n",
					"        else:\n",
					"            updated_log = new_log_df\n",
					"        \n",
					"        # Write the updated log\n",
					"        temp_path = f\"{process_log_path}_temp\"\n",
					"        updated_log.write.mode(\"overwrite\").parquet(temp_path)\n",
					"        \n",
					"        # Move the temporary log to the final location\n",
					"        if mssparkutils.fs.exists(temp_path):\n",
					"            if mssparkutils.fs.exists(process_log_path):\n",
					"                mssparkutils.fs.rm(process_log_path, True)\n",
					"            mssparkutils.fs.mv(temp_path, process_log_path)\n",
					"        \n",
					"        return True\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"Error updating process log: {str(e)}\")\n",
					"        import traceback\n",
					"        print(traceback.format_exc())\n",
					"        return False"
				],
				"execution_count": 56
			},
			{
				"cell_type": "code",
				"source": [
					"def prepare_postgresql_data(df):\n",
					"    \"\"\"\n",
					"    Prepare data specifically for PostgreSQL loading\n",
					"    - Selects only required columns\n",
					"    - Applies flags field transformation (X overrules F)\n",
					"    - Ensures proper data types\n",
					"    - Replaces empty values with NULL\n",
					"    \"\"\"\n",
					"    logger = logging.getLogger()\n",
					"    logger.info(f\"Preparing PostgreSQL data from {df.count()} records\")\n",
					"    \n",
					"    try:\n",
					"        # 1. Select only columns needed for PostgreSQL\n",
					"        pg_columns = [\"part_no\", \"register_lett\", \"poll_number\", \"new_marker\", \n",
					"                     \"title\", \"lname\", \"fname\", \"dob\", \"flags\", \"address\", \n",
					"                     \"address2\", \"address3\", \"address4\", \"address5\", \"address6\", \n",
					"                     \"zip\", \"date_selected1\", \"date_selected2\", \"date_selected3\", \n",
					"                     \"rec_num\", \"perm_disqual\", \"source_id\", \"postcode_start\", \n",
					"                     \"hash_id\", \"creation_date\"]  # Keep hash_id and creation_date for tracking\n",
					"        \n",
					"        pg_data = df.select([col(c) for c in pg_columns if c in df.columns])\n",
					"        \n",
					"        # 2. Process flags column for X/F rule\n",
					"        pg_data = pg_data.withColumn(\n",
					"            \"flags\",\n",
					"            when(\n",
					"                (col(\"flags\").isNotNull()) & \n",
					"                (col(\"flags\").contains(\"XF\") | col(\"flags\").contains(\"FX\") | \n",
					"                 (col(\"flags\").contains(\"X\") & col(\"flags\").contains(\"F\"))),\n",
					"                \"X\"\n",
					"            ).otherwise(col(\"flags\"))\n",
					"        )\n",
					"        \n",
					"        # 3. Replace empty values with NULL\n",
					"        for column in pg_data.columns:\n",
					"            pg_data = pg_data.withColumn(\n",
					"                column,\n",
					"                when(\n",
					"                    col(column).isNull() | (col(column) == \"\") | (col(column) == \"NaT\") |\n",
					"                    (col(column) == \"Nan\") | (col(column) == \"NaN\"),\n",
					"                    lit(None)\n",
					"                ).otherwise(col(column))\n",
					"            )\n",
					"        \n",
					"        # 4. Apply trim to all string columns to ensure no padding issues\n",
					"        string_columns = [\"part_no\", \"new_marker\", \"title\", \"lname\", \"fname\", \n",
					"                          \"flags\", \"address\", \"address2\", \"address3\", \"address4\", \n",
					"                          \"address5\", \"address6\", \"zip\", \"perm_disqual\", \n",
					"                          \"source_id\", \"postcode_start\"]\n",
					"        \n",
					"        for col_name in string_columns:\n",
					"            if col_name in pg_data.columns:\n",
					"                pg_data = pg_data.withColumn(col_name, trim(col(col_name)))\n",
					"        \n",
					"        # 5. Ensure proper data types for numeric fields\n",
					"        pg_data = pg_data.withColumn(\"register_lett\", col(\"register_lett\").cast(\"integer\"))\n",
					"        pg_data = pg_data.withColumn(\"poll_number\", col(\"poll_number\").cast(\"integer\"))\n",
					"        pg_data = pg_data.withColumn(\"rec_num\", col(\"rec_num\").cast(\"integer\"))\n",
					"        \n",
					"        # 6. Ensure timestamp columns are properly formatted\n",
					"        date_columns = [\"date_selected1\", \"date_selected2\", \"date_selected3\"]\n",
					"        for date_col in date_columns:\n",
					"            if date_col in pg_data.columns:\n",
					"                pg_data = pg_data.withColumn(date_col, col(date_col).cast(\"timestamp\"))\n",
					"        \n",
					"        logger.info(f\"PostgreSQL data preparation complete: {pg_data.count()} records\")\n",
					"        return pg_data\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error preparing PostgreSQL data: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        # Return original dataframe if transformation fails\n",
					"        return df"
				],
				"execution_count": 57
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Main Pipeline Function"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def main():\n",
					"    \"\"\"Main function to run the incremental load process with improved error handling\"\"\"\n",
					"    # Initialize\n",
					"    print(\"Starting electoral data ETL process\")\n",
					"    print(f\"Current time: {datetime.now()}\")\n",
					"\n",
					"    # Configure logging and capture the logger\n",
					"    logger = configure_logging()\n",
					"    print(\"Logging configured successfully\")\n",
					"    logger.info(\"Logging configured successfully\")\n",
					"\n",
					"    spark = create_spark_session()\n",
					"    config = load_config()\n",
					"    storage_account = config[\"storage_account\"]\n",
					"    process_log_path = f\"{config['output_path']}_process_log\"\n",
					"    \n",
					"    # Ensure all required directories exist\n",
					"    from notebookutils import mssparkutils\n",
					"    required_paths = [\n",
					"        config['input_path'],\n",
					"        config['output_path'],\n",
					"        \"/\".join(process_log_path.split(\"/\")[:-1]),  # Parent dir for process log\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_transformed\",\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_postgresql\"\n",
					"    ]\n",
					"    \n",
					"    for path in required_paths:\n",
					"        try:\n",
					"            if not mssparkutils.fs.exists(path):\n",
					"                logger.info(f\"Creating directory: {path}\")\n",
					"                print(f\"Creating directory: {path}\")\n",
					"                mssparkutils.fs.mkdirs(path)\n",
					"                logger.info(f\"Successfully created directory: {path}\")\n",
					"                print(f\"Successfully created directory: {path}\")\n",
					"        except Exception as e:\n",
					"            logger.error(f\"Error creating directory {path}: {str(e)}\")\n",
					"            print(f\"Error creating directory {path}: {str(e)}\")\n",
					"    \n",
					"    try:\n",
					"        # 1. Get configuration and mapping\n",
					"        column_mapping = read_column_mapping(\n",
					"            spark, f\"{config['config_path']}/col_mapping_schema.json\"\n",
					"        )\n",
					"        if not column_mapping:\n",
					"            logger.error(\"Failed to read column mapping. Aborting process.\")\n",
					"            return\n",
					"\n",
					"        # 2. Identify new files\n",
					"        new_file_paths, all_file_paths = get_new_files(\n",
					"            spark, config[\"input_path\"], process_log_path\n",
					"        )\n",
					"\n",
					"        if not new_file_paths:\n",
					"            logger.info(\"No new files to process\")\n",
					"            return\n",
					"\n",
					"        logger.info(f\"Found {len(new_file_paths)} new files to process\")\n",
					"\n",
					"        # 3. Process new files\n",
					"        processed_dfs = process_input_files(spark, new_file_paths, column_mapping)\n",
					"\n",
					"        if not processed_dfs or len(processed_dfs) == 0:\n",
					"            logger.error(\"No data to process after reading files\")\n",
					"            return\n",
					"\n",
					"        logger.info(f\"Successfully processed {len(processed_dfs)} DataFrames\")\n",
					"\n",
					"        # 4. Merge new data\n",
					"        merged_new_data = merge_dataframes(processed_dfs)\n",
					"        if merged_new_data is None or merged_new_data.count() == 0:\n",
					"            logger.error(\"Merged DataFrame is empty or None. Check merge_dataframes function\")\n",
					"            return\n",
					"\n",
					"        logger.info(f\"Merged data contains {merged_new_data.count()} rows\")\n",
					"\n",
					"        # 5. Ensure register_lett matches poll_number\n",
					"        if \"register_lett\" in merged_new_data.columns and \"poll_number\" in merged_new_data.columns:\n",
					"            # Count initial mismatches\n",
					"            mismatches = merged_new_data.filter(col(\"register_lett\") != col(\"poll_number\")).count()\n",
					"            if mismatches > 0:\n",
					"                logger.info(f\"Found {mismatches} mismatches between register_lett and poll_number. Synchronizing...\")\n",
					"                merged_new_data = merged_new_data.withColumn(\"register_lett\", col(\"poll_number\"))\n",
					"                logger.info(\"register_lett synchronized with poll_number\")\n",
					"        \n",
					"        # 6. Deduplicate new data\n",
					"        deduplicated_data = optimise_deduplicate_data(merged_new_data)\n",
					"        if deduplicated_data is None or deduplicated_data.count() == 0:\n",
					"            logger.error(\"Deduplicated DataFrame is empty or None\")\n",
					"            return\n",
					"\n",
					"        logger.info(f\"After deduplication: {deduplicated_data.count()} rows\")\n",
					"\n",
					"        # 7. Transform to target schema BEFORE applying hashing\n",
					"        # This ensures all required columns like rec_num are present\n",
					"        logger.info(\"Transforming data to target schema\")\n",
					"        transformed_data = transform_to_target_schema(deduplicated_data, column_mapping)\n",
					"        if transformed_data is None or transformed_data.count() == 0:\n",
					"            logger.error(\"Transformed DataFrame is empty or None\")\n",
					"            return\n",
					"        \n",
					"        # Ensure LA_Code is mapped to rec_num if needed\n",
					"        if \"LA_Code\" in transformed_data.columns and \"rec_num\" not in transformed_data.columns:\n",
					"            logger.info(\"Mapping LA_Code to rec_num\")\n",
					"            transformed_data = transformed_data.withColumnRenamed(\"LA_Code\", \"rec_num\")\n",
					"            \n",
					"        # Apply column validation\n",
					"        try:\n",
					"            logger.info(\"Applying column validation and constraints...\")\n",
					"            transformed_data = validate_and_transform_all_columns(transformed_data)\n",
					"            logger.info(\"Column validation and constraints applied successfully\")\n",
					"        except Exception as e:\n",
					"            logger.error(f\"Error applying column validation: {str(e)}\")\n",
					"            import traceback\n",
					"            logger.error(traceback.format_exc())\n",
					"            logger.warning(\"Continuing with original transformed data\")\n",
					"        \n",
					"        # Check that transformed_data is not empty\n",
					"        if transformed_data.count() == 0:\n",
					"            logger.warning(\"Transformed DataFrame is empty - nothing to process\")\n",
					"            return\n",
					"            \n",
					"        # 8. Now apply hashing AFTER the schema transformation\n",
					"        # This ensures rec_num and other required fields exist\n",
					"        logger.info(\"Generating hash_id...\")\n",
					"        hashed_data = generate_hash_id(transformed_data)\n",
					"        if hashed_data is None or hashed_data.count() == 0:\n",
					"            logger.error(\"Hashed DataFrame is empty or None\")\n",
					"            return\n",
					"        logger.info(f\"Applied hash generation to {hashed_data.count()} rows\")\n",
					"\n",
					"        # 9. Apply deduplicate using hash_id to ensure uniqueness\n",
					"        final_data = hashed_data.dropDuplicates([\"hash_id\"])\n",
					"        logger.info(f\"After hash-based deduplication: {final_data.count()} rows\")\n",
					"\n",
					"        # 10. Ensure schema consistency with TARGET_SCHEMA\n",
					"        final_data = ensure_consistent_schema(final_data, TARGET_SCHEMA)\n",
					"        logger.info(f\"Schema standardized with {len(TARGET_SCHEMA)} columns\")\n",
					"\n",
					"        if final_data.count() > 0:\n",
					"            invalid_postcodes = final_data.filter(\n",
					"                ~col(\"zip\").rlike(r'^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$') & \n",
					"                col(\"zip\").isNotNull()\n",
					"            ).count()\n",
					"            if invalid_postcodes > 0:\n",
					"                logger.warning(f\"Found {invalid_postcodes} records with potentially invalid postcodes\")\n",
					"            else:\n",
					"                logger.info(\"All postcodes passed validation\")\n",
					"\n",
					"        # 11. Merge with existing data using our improved merge function\n",
					"        logger.info(\"Merging with existing data...\")\n",
					"        final_data = merge_with_existing_data(spark, final_data, config[\"output_path\"])\n",
					"        if final_data is None or final_data.count() == 0:\n",
					"            logger.error(\"Final DataFrame after merge is empty or None\")\n",
					"            return\n",
					"\n",
					"        logger.info(f\"Final merged data contains {final_data.count()} rows\")\n",
					"\n",
					"        # 12. Write output to both locations\n",
					"        logger.info(f\"Writing {final_data.count()} records to {config['output_path']}\")\n",
					"\n",
					"        # Make sure directories exist\n",
					"        for path in [\n",
					"            config[\"output_path\"],\n",
					"            f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_transformed\",\n",
					"        ]:\n",
					"            dir_path = path\n",
					"            if not mssparkutils.fs.exists(dir_path):\n",
					"                logger.info(f\"Creating directory: {dir_path}\")\n",
					"                mssparkutils.fs.mkdirs(dir_path)\n",
					"\n",
					"        if final_data.count() == 0:\n",
					"            logger.warning(\"Final data is empty, nothing to write\")\n",
					"            return\n",
					"\n",
					"        # Write to the primary output locations\n",
					"        success1 = write_output_safely(\n",
					"            final_data, config[\"output_path\"], \"transformed data\"\n",
					"        )\n",
					"        success2 = False  # Initialize the variable\n",
					"\n",
					"        if success1:\n",
					"            success2 = write_output_safely(\n",
					"                final_data,\n",
					"                f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_transformed\",\n",
					"                \"transformed data\",\n",
					"            )\n",
					"\n",
					"        # Branch for PostgreSQL-specific processing\n",
					"        if final_data.count() > 0:\n",
					"            logger.info(\"Starting PostgreSQL-specific transformation\")\n",
					"            try: \n",
					"                postgresql_data = prepare_postgresql_data(final_data)\n",
					"                \n",
					"                # Define PostgreSQL output path\n",
					"                postgresql_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_postgresql\"\n",
					"                \n",
					"                # Ensure directory exists\n",
					"                if not mssparkutils.fs.exists(postgresql_path):\n",
					"                    logger.info(f\"Creating directory: {postgresql_path}\")\n",
					"                    mssparkutils.fs.mkdirs(postgresql_path)\n",
					"                \n",
					"                # Write PostgreSQL data\n",
					"                logger.info(f\"Writing {postgresql_data.count()} records to PostgreSQL format\")\n",
					"                postgresql_data.write.mode(\"overwrite\").parquet(postgresql_path)\n",
					"                \n",
					"                logger.info(f\"Successfully wrote PostgreSQL data to {postgresql_path}\")\n",
					"            except Exception as pg_error:\n",
					"                logger.error(f\"Error in PostgreSQL branch: {str(pg_error)}\")\n",
					"                import traceback\n",
					"                logger.error(traceback.format_exc())\n",
					"\n",
					"        if success1 and success2:\n",
					"            logger.info(f\"Successfully processed {len(new_file_paths)} files\")\n",
					"            logger.info(f\"Total records in output: {final_data.count()}\")\n",
					"\n",
					"            # Update the process log\n",
					"            log_success = update_process_log(spark, new_file_paths, process_log_path)\n",
					"            if log_success:\n",
					"                logger.info(f\"Updated process log with {len(new_file_paths)} files\")\n",
					"            else:\n",
					"                logger.warning(\n",
					"                    \"Failed to update process log, but data was written successfully\"\n",
					"                )\n",
					"        else:\n",
					"            logger.error(\"Failed to write output to one or more destinations\")\n",
					"\n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error in incremental load process: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        raise\n",
					"\n",
					"if __name__ == \"__main__\":\n",
					"    main()"
				],
				"execution_count": 59
			}
		]
	}
}