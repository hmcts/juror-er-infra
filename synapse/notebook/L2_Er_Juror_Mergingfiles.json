{
	"name": "L2_Er_Juror_Mergingfiles",
	"properties": {
		"folder": {
			"name": "crime"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkprod",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "92a33952-65d8-4b52-904d-f36c3e381cf6"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5ca62022-6aa2-4cee-aaa7-e7536c8d566c/resourceGroups/baubais-data-factory-rg-prod/providers/Microsoft.Synapse/workspaces/baubais-synapse-prod/bigDataPools/sparkprod",
				"name": "sparkprod",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkprod",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Electoral Roll Data ETL Process\n",
					"\n",
					"### **L2_Er_Juror_Mergingfiles: Electoral Roll Data ETL Process**\n",
					"\n",
					"This script implements an **ETL (Extract, Transform, Load)** pipeline for processing electoral roll data. It handles data ingestion, transformation, deduplication, hashing, and merging with existing data. The final output is written to Azure Data Lake Storage in a standardized format.\n",
					"\n",
					"---\n",
					"\n",
					"### **Overview**\n",
					"\n",
					"The script processes electoral roll data from multiple sources, standardizes column names, removes duplicates, hashes sensitive information, and prepares the data for storage in Azure Data Lake. It also maintains a process log to track processed files and ensures schema consistency.\n",
					"\n",
					"---\n",
					"\n",
					"### **Key Steps in the ETL Process**\n",
					"\n",
					"#### **1. Setup Functions**\n",
					"- **`create_spark_session()`**:\n",
					"  - Initializes a Spark session with necessary configurations.\n",
					"- **`configure_logging()`**:\n",
					"  - Sets up logging to track the ETL process and writes logs to a file (`electoral_etl.log`).\n",
					"- **`load_config()`**:\n",
					"  - Loads configuration details such as storage paths for input, output, and schema files.\n",
					"\n",
					"---\n",
					"\n",
					"#### **2. Configuration Processing**\n",
					"- **`read_column_mapping(spark, config_path)`**:\n",
					"  - Reads and parses a JSON configuration file containing column mappings.\n",
					"  - Standardizes column names based on the mapping.\n",
					"\n",
					"- **`standardize_columns(df, column_mapping)`**:\n",
					"  - Renames columns in the input DataFrame to match the standardized schema.\n",
					"\n",
					"- **`read_schema_config(spark, config_path)`**:\n",
					"  - Reads and parses the schema configuration to ensure the DataFrame matches the target schema.\n",
					"\n",
					"- **`ensure_consistent_schema(df, required_columns)`**:\n",
					"  - Ensures the DataFrame has all required columns with correct data types.\n",
					"\n",
					"---\n",
					"\n",
					"#### **3. Incremental Data Processing**\n",
					"- **`get_new_files(spark, input_path, process_log_path)`**:\n",
					"  - Identifies new files in the input directory that have not been processed yet.\n",
					"  - Compares the list of files in the input directory with the process log.\n",
					"\n",
					"- **`process_input_files(spark, file_paths, column_mapping)`**:\n",
					"  - Reads and standardizes columns from the identified new files.\n",
					"  - Adds metadata such as the source file path and processing timestamp.\n",
					"\n",
					"---\n",
					"\n",
					"#### **4. Data Transformation**\n",
					"- **`merge_dataframes(dfs)`**:\n",
					"  - Merges multiple DataFrames with standardized schemas into a single DataFrame.\n",
					"  - Handles schema variations using `unionByName`.\n",
					"\n",
					"- **`deduplicate_by_creation_date(df)`**:\n",
					"  - Removes duplicate records by keeping only the most recent record for each unique entity based on the `CreationDate` column.\n",
					"\n",
					"- **`optimise_deduplicate_data(df)`**:\n",
					"  - Performs conservative deduplication by removing exact duplicates across all columns.\n",
					"\n",
					"- **`improved_deduplicate_data(df)`**:\n",
					"  - Uses window functions to keep the most recent record for each unique entity based on identifying columns.\n",
					"\n",
					"- **`apply_hashing(df)`**:\n",
					"  - Hashes sensitive fields (e.g., names, addresses, dates of birth) to create a unique identifier (`hash_id`).\n",
					"\n",
					"- **`apply_hashing_to_voters(df)`**:\n",
					"  - Applies hashing specifically for the `voters_temp` table structure.\n",
					"\n",
					"- **`transform_to_target_schema(df, column_mapping)`**:\n",
					"  - Transforms the DataFrame to match the target schema for PostgreSQL.\n",
					"  - Applies validation rules to ensure data quality.\n",
					"\n",
					"---\n",
					"\n",
					"#### **5. Merging with Existing Data**\n",
					"- **`merge_with_existing_data(spark, new_data, output_path)`**:\n",
					"  - Merges new data with existing data in the output path.\n",
					"  - Uses SQL-based logic to handle updates and inserts:\n",
					"    - Keeps all new records that do not exist in the existing data.\n",
					"    - Updates existing records with newer values if available.\n",
					"\n",
					"---\n",
					"\n",
					"#### **6. Writing Output and Updating Process Log**\n",
					"- **`write_output_safely(df, output_path, description)`**:\n",
					"  - Writes the DataFrame to the specified output path in Azure Data Lake Storage.\n",
					"  - Ensures proper error handling and directory creation.\n",
					"\n",
					"- **`write_output_and_update_log(spark, merged_data, new_file_paths, process_log_path)`**:\n",
					"  - Writes the merged data to the output location and updates the process log with newly processed files.\n",
					"\n",
					"- **`update_process_log(spark, new_file_paths, process_log_path)`**:\n",
					"  - Updates the process log to track which files have been processed.\n",
					"\n",
					"---\n",
					"\n",
					"### **Main Pipeline Function**\n",
					"\n",
					"The `main()` function orchestrates the entire ETL process:\n",
					"\n",
					"1. **Initialization**:\n",
					"   - Configures logging and initializes the Spark session.\n",
					"   - Loads configuration and column mapping.\n",
					"\n",
					"2. **File Processing**:\n",
					"   - Identifies new files to process.\n",
					"   - Reads and standardizes the new files.\n",
					"\n",
					"3. **Data Transformation**:\n",
					"   - Merges new data into a single DataFrame.\n",
					"   - Deduplicates the data and applies hashing to sensitive fields.\n",
					"   - Transforms the data to match the target schema.\n",
					"\n",
					"4. **Merging with Existing Data**:\n",
					"   - Merges the transformed data with existing data in the output path.\n",
					"\n",
					"5. **Writing Output**:\n",
					"   - Writes the final data to Azure Data Lake Storage.\n",
					"   - Updates the process log.\n",
					"\n",
					"6. **Error Handling**:\n",
					"   - Logs errors and continues processing where possible.\n",
					"\n",
					"---\n",
					"\n",
					"### **Key Features**\n",
					"\n",
					"1. **Schema Standardization**:\n",
					"   - Ensures all input files conform to a consistent schema.\n",
					"\n",
					"2. **Deduplication**:\n",
					"   - Removes duplicate records based on creation date or identifying columns.\n",
					"\n",
					"3. **Hashing**:\n",
					"   - Hashes sensitive fields to protect privacy.\n",
					"\n",
					"4. **Incremental Processing**:\n",
					"   - Processes only new files that have not been processed before.\n",
					"\n",
					"5. **Merge Logic**:\n",
					"   - Combines new data with existing data, handling updates and inserts.\n",
					"\n",
					"6. **Error Handling**:\n",
					"   - Logs errors and continues processing where possible.\n",
					"\n",
					"---\n",
					"\n",
					"### **Output**\n",
					"\n",
					"1. **Transformed Data**:\n",
					"   - The final transformed data is written to the specified output path in Azure Data Lake Storage.\n",
					"\n",
					"2. **Process Log**:\n",
					"   - A log of processed files is maintained to avoid reprocessing.\n",
					"\n",
					"---\n",
					"\n",
					"### **Next Steps**\n",
					"\n",
					"1. **Run the Script**:\n",
					"   - Execute the `main()` function to process the electoral roll data.\n",
					"\n",
					"2. **Verify the Output**:\n",
					"   - Check the output path in Azure Data Lake Storage to ensure the data is written correctly.\n",
					"\n",
					"3. **Monitor Logs**:\n",
					"   - Review the `electoral_etl.log` file for any errors or warnings.\n",
					"\n",
					"4. **Integrate with Downstream Systems**:\n",
					"   - Use the transformed data for analytics or load it into a database (e.g., PostgreSQL).\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession, Window\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"import json\n",
					"import logging\n",
					"from datetime import datetime\n",
					"import time\n",
					"import os\n",
					"import re\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql.functions import col, when, upper, regexp_extract, regexp_replace, length, substring, lit"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def configure_logging():\n",
					"    \"\"\"Set up proper logging with rotating file handler\"\"\"\n",
					"    log_format = '%(asctime)s [%(levelname)s] %(message)s'\n",
					"    logging.basicConfig(level=logging.INFO, format=log_format)\n",
					"    \n",
					"    # Add a handler that writes to a log file\n",
					"    file_handler = logging.FileHandler('electoral_etl.log')\n",
					"    file_handler.setFormatter(logging.Formatter(log_format))\n",
					"    \n",
					"    # Get the root logger and add the file handler\n",
					"    root_logger = logging.getLogger()\n",
					"    root_logger.addHandler(file_handler)\n",
					"    \n",
					"    return root_logger"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def create_spark_session():\n",
					"    \"\"\"Create and return a Spark session with Delta Lake support\"\"\"\n",
					"    return SparkSession.builder \\\n",
					"        .appName(\"Electoral Data ETL\") \\\n",
					"        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
					"        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
					"        .getOrCreate()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def normalize_storage_path(path):\n",
					"    \"\"\"Normalize storage path to ensure consistency\"\"\"\n",
					"    # Remove any trailing slashes\n",
					"    path = path.rstrip('/')\n",
					"    \n",
					"    # Check if path is missing container info\n",
					"    if not path.startswith(\"abfss://\"):\n",
					"        # Add default storage and container\n",
					"        path = f\"abfss://juror-etl@baubaisadfsaprod.dfs.core.windows.net/{path}\"\n",
					"    \n",
					"    return path"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def ensure_directory_exists(path):\n",
					"    \"\"\"Create directory if it doesn't exist\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    try:\n",
					"        if not mssparkutils.fs.exists(path):\n",
					"            logger = logging.getLogger()\n",
					"            logger.info(f\"Creating directory: {path}\")\n",
					"            mssparkutils.fs.mkdirs(path)\n",
					"            logger.info(f\"Successfully created directory: {path}\")\n",
					"            return True\n",
					"        return True\n",
					"    except Exception as e:\n",
					"        logger = logging.getLogger()\n",
					"        logger.error(f\"Error creating directory {path}: {str(e)}\")\n",
					"        print(f\"Error creating directory {path}: {str(e)}\")\n",
					"        return False"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import StringType, IntegerType, TimestampType\n",
					"\n",
					"# Define global schema\n",
					"TARGET_SCHEMA = [\n",
					"    (\"part_no\", StringType()),\n",
					"    (\"register_lett\", IntegerType()),\n",
					"    (\"poll_number\", IntegerType()),\n",
					"    (\"new_marker\", StringType()),\n",
					"    (\"title\", StringType()),\n",
					"    (\"lname\", StringType()),\n",
					"    (\"fname\", StringType()),\n",
					"    (\"dob\", StringType()),\n",
					"    (\"flags\", StringType()),\n",
					"    (\"address\", StringType()),\n",
					"    (\"address2\", StringType()),\n",
					"    (\"address3\", StringType()),\n",
					"    (\"address4\", StringType()),\n",
					"    (\"address5\", StringType()),\n",
					"    (\"address6\", StringType()),\n",
					"    (\"zip\", StringType()),\n",
					"    (\"date_selected1\", TimestampType()),\n",
					"    (\"date_selected2\", TimestampType()),\n",
					"    (\"date_selected3\", TimestampType()),\n",
					"    (\"rec_num\", IntegerType()),\n",
					"    (\"perm_disqual\", StringType()),\n",
					"    (\"source_id\", StringType()),\n",
					"    (\"postcode_start\", StringType()),\n",
					"    (\"hash_id\", StringType()),\n",
					"    (\"creation_date\", StringType())\n",
					"]\n",
					"HASH_SENSITIVE_DATA = False "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def load_config():\n",
					"    \"\"\"Load configuration including storage paths\"\"\"\n",
					"    storage_account = \"baubaisadfsaprod\"\n",
					"    config = {\n",
					"        \"storage_account\": storage_account,\n",
					"        \"input_path\": f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/transformation\",\n",
					"        \"config_path\": f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/config/schema\",\n",
					"        \"output_path\": f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_transformed\"\n",
					"    }\n",
					"    \n",
					"    # Normalize all paths\n",
					"    config[\"output_path\"] = normalize_storage_path(config[\"output_path\"])\n",
					"    config[\"input_path\"] = normalize_storage_path(config[\"input_path\"])\n",
					"    config[\"config_path\"] = normalize_storage_path(config[\"config_path\"])\n",
					"    \n",
					"    return config"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def cast_to_target_schema(df):\n",
					"    \"\"\"Force DataFrame to match the TARGET_SCHEMA types\"\"\"\n",
					"    from pyspark.sql.functions import col\n",
					"    \n",
					"    schema_dict = dict(TARGET_SCHEMA)\n",
					"\n",
					"    for column_name, data_type in schema_dict.items():\n",
					"        if column_name in df.columns:\n",
					"            df = df.withColumn(column_name, col(column_name).cast(data_type))\n",
					"    \n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def regexp_escape(column):\n",
					"    \"\"\"Escape special characters in a column for use in a regular expression\"\"\"\n",
					"    # List of characters that need escaping in regex\n",
					"    special_chars = [\"\\\\\", \".\", \"^\", \"$\", \"*\", \"+\", \"?\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"|\"]\n",
					"    \n",
					"    # Start with the original column\n",
					"    escaped = column\n",
					"    \n",
					"    # Escape each special character\n",
					"    for char in special_chars:\n",
					"        escaped = regexp_replace(escaped, lit(char), concat(lit(\"\\\\\"), lit(char)))\n",
					"        \n",
					"    return escaped"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def process_postcodes(df: DataFrame) -> DataFrame:\n",
					"    \"\"\"\n",
					"    Process postcodes to standardize format and handle invalid values\n",
					"    \"\"\"\n",
					"    # Get postcode settings from our loaded configuration\n",
					"    postcode_settings = config_dict.get(\"validation_rules\", {}).get(\"POSTCODE_SETTINGS\", {})\n",
					"    \n",
					"    # Check if we have the necessary postcode columns\n",
					"    possible_columns = postcode_settings.get(\"possible_columns\", [\"PostCode\", \"zip\"])\n",
					"    postcode_columns = [col_name for col_name in possible_columns if col_name in df.columns]\n",
					"    \n",
					"    if not postcode_columns:\n",
					"        print(\"No postcode columns found in DataFrame\")\n",
					"        return df\n",
					"        \n",
					"    # Get the main postcode column (usually \"zip\" in target schema)\n",
					"    main_postcode_col = \"zip\" if \"zip\" in df.columns else postcode_columns[0]\n",
					"    \n",
					"    # Get validation settings\n",
					"    validation_pattern = postcode_settings.get(\"validation_pattern\", r'^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$')\n",
					"    extraction_pattern = postcode_settings.get(\"extraction_pattern\", r'([A-Z0-9]{1,4})[ -]*([0-9][A-Z]{2})')\n",
					"    format_template = postcode_settings.get(\"format_template\", '$1 $2')\n",
					"    remove_invalid = postcode_settings.get(\"remove_invalid\", False)\n",
					"    \n",
					"    # Standardize format\n",
					"    df = df.withColumn(\n",
					"        main_postcode_col,\n",
					"        when(col(main_postcode_col).isNotNull(),\n",
					"             regexp_replace(upper(col(main_postcode_col)), extraction_pattern, format_template)\n",
					"        ).otherwise(col(main_postcode_col))\n",
					"    )\n",
					"    \n",
					"    # Filter invalid if requested\n",
					"    if remove_invalid:\n",
					"        df = df.filter(\n",
					"            col(main_postcode_col).isNull() | col(main_postcode_col).rlike(validation_pattern)\n",
					"        )\n",
					"    \n",
					"    # Extract postcode_start if needed\n",
					"    if \"postcode_start\" in df.columns:\n",
					"        postcode_start_pattern = postcode_settings.get(\"postcode_start_pattern\", r'^([A-Z]{1,2}[0-9][A-Z0-9]?)')\n",
					"        df = df.withColumn(\n",
					"            \"postcode_start\",\n",
					"            regexp_extract(col(main_postcode_col), postcode_start_pattern, 1)\n",
					"        )\n",
					"    \n",
					"    return df\n",
					"\n",
					"def enforce_null_columns(df: DataFrame) -> DataFrame:\n",
					"    \"\"\"\n",
					"    Set specific columns to NULL as defined in configuration\n",
					"    \"\"\"\n",
					"    null_columns = config_dict.get(\"validation_rules\", {}).get(\"NULL_COLUMNS\", [])\n",
					"    \n",
					"    for col_name in null_columns:\n",
					"        if col_name in df.columns:\n",
					"            df = df.withColumn(col_name, lit(None))\n",
					"    \n",
					"    return df\n",
					"\n",
					"def create_target_schema_df(df: DataFrame) -> DataFrame:\n",
					"    \"\"\"\n",
					"    Transform DataFrame to target schema\n",
					"    \"\"\"\n",
					"    # Get target schema from configuration\n",
					"    target_schema = config_dict.get(\"validation_rules\", {}).get(\"TARGET_SCHEMA\", [])\n",
					"    column_mappings = config_dict.get(\"validation_rules\", {}).get(\"COLUMN_MAPPINGS\", {})\n",
					"    \n",
					"    if not target_schema:\n",
					"        print(\"No target schema defined in configuration\")\n",
					"        return df\n",
					"    \n",
					"    # Get all target column names\n",
					"    target_columns = [col_def[\"name\"] for col_def in target_schema]\n",
					"    \n",
					"    # First, apply column mappings if needed\n",
					"    for source_col, target_col in column_mappings.items():\n",
					"        if source_col in df.columns and source_col != target_col:\n",
					"            # If target column doesn't exist, rename the source column\n",
					"            if target_col not in df.columns:\n",
					"                df = df.withColumnRenamed(source_col, target_col)\n",
					"            # If both exist, keep target and drop source\n",
					"            elif source_col != target_col:\n",
					"                df = df.drop(source_col)\n",
					"    \n",
					"    # Add any missing columns with NULL values\n",
					"    for col_def in target_schema:\n",
					"        col_name = col_def[\"name\"]\n",
					"        if col_name not in df.columns:\n",
					"            df = df.withColumn(col_name, lit(None))\n",
					"    \n",
					"    # Select only the columns in the target schema, in the correct order\n",
					"    df = df.select(target_columns)\n",
					"    \n",
					"    return df\n",
					"\n",
					"def enforce_column_lengths(df: DataFrame) -> DataFrame:\n",
					"    \"\"\"\n",
					"    Enforce maximum length constraints on string columns\n",
					"    \"\"\"\n",
					"    # Get string columns with length constraints from configuration\n",
					"    string_columns = config_dict.get(\"column_constraints\", {}).get(\"string_columns\", {})\n",
					"    \n",
					"    for col_name, constraints in string_columns.items():\n",
					"        if col_name in df.columns and \"max_length\" in constraints:\n",
					"            max_length = constraints[\"max_length\"]\n",
					"            df = df.withColumn(\n",
					"                col_name,\n",
					"                when(\n",
					"                    (col(col_name).isNotNull()) & (length(col(col_name)) > max_length),\n",
					"                    substring(col(col_name), 1, max_length)\n",
					"                ).otherwise(col(col_name))\n",
					"            )\n",
					"            \n",
					"            # Apply case transformation if specified\n",
					"            if \"case\" in constraints and constraints[\"case\"] == \"upper\":\n",
					"                df = df.withColumn(\n",
					"                    col_name,\n",
					"                    when(col(col_name).isNotNull(), upper(col(col_name)))\n",
					"                    .otherwise(col(col_name))\n",
					"                )\n",
					"    \n",
					"    return df\n",
					"\n",
					"def process_dataframe(df):\n",
					"    \"\"\"\n",
					"    Apply validation and transformation rules to a DataFrame.\n",
					"    Now includes enhanced cleansing from prepare_postgresql_data\n",
					"    \"\"\"\n",
					"    logger = logging.getLogger()\n",
					"    logger.info(f\"Starting data processing with {df.count()} rows\")\n",
					"    \n",
					"    try:\n",
					"        # Apply standard transformations\n",
					"        df = process_postcodes(df)\n",
					"        df = enforce_null_columns(df)\n",
					"        df = create_target_schema_df(df)\n",
					"        df = enforce_column_lengths(df)\n",
					"        \n",
					"        # NEW: Add enhanced cleansing from prepare_postgresql_data\n",
					"        \n",
					"        # 1. Clean timestamp fields first to replace NaT with NULL\n",
					"        timestamp_columns = [\"date_selected1\", \"date_selected2\", \"date_selected3\"]\n",
					"        for col_name in timestamp_columns:\n",
					"            if col_name in df.columns:\n",
					"                df = df.withColumn(\n",
					"                    col_name,\n",
					"                    when(\n",
					"                        (col(col_name).isNull()) | \n",
					"                        (col(col_name).cast(\"string\").isin(\"NaT\", \"nat\", \"NaN\", \"nan\", \"None\", \"\")),\n",
					"                        lit(None)\n",
					"                    ).otherwise(col(col_name))\n",
					"                )\n",
					"        \n",
					"        # 2. Replace empty values with NULL for all columns\n",
					"        for column in df.columns:\n",
					"            df = df.withColumn(\n",
					"                column,\n",
					"                when(\n",
					"                    (col(column).isNull()) | \n",
					"                    (col(column).cast(\"string\").isin(\"\", \"nan\", \"NaN\")),\n",
					"                    lit(None)\n",
					"                ).otherwise(col(column))\n",
					"            )\n",
					"        \n",
					"        # 3. Apply trim to all string columns to ensure no padding issues\n",
					"        string_columns = [\"part_no\", \"new_marker\", \"title\", \"lname\", \"fname\", \n",
					"                          \"flags\", \"address\", \"address2\", \"address3\", \"address4\", \n",
					"                          \"address5\", \"address6\", \"zip\", \"perm_disqual\", \n",
					"                          \"source_id\", \"postcode_start\", \"hash_id\", \"creation_date\"]\n",
					"        \n",
					"        for col_name in string_columns:\n",
					"            if col_name in df.columns:\n",
					"                df = df.withColumn(col_name, trim(col(col_name)))\n",
					"        \n",
					"        # Note: We're not adding the flags processing here as requested\n",
					"        \n",
					"        logger.info(f\"Completed data processing with {df.count()} rows\")\n",
					"        return df\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error during DataFrame processing: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        \n",
					"        # Return the original DataFrame as fallback\n",
					"        logger.warning(\"Returning original DataFrame due to processing error\")\n",
					"        return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import concat_ws, collect_list\n",
					"def read_json_config(storage_account, config_path):\n",
					"    \"\"\"Read JSON configuration from Azure Data Lake Storage using Spark's text reader\"\"\"\n",
					"    try:\n",
					"        from pyspark.sql import SparkSession\n",
					"        spark = SparkSession.builder.getOrCreate()\n",
					"        \n",
					"        # Read the file as text\n",
					"        df = spark.read.text(config_path)\n",
					"        \n",
					"        # Collect all lines and concatenate\n",
					"        json_str = df.agg(concat_ws(\"\", collect_list(\"value\"))).collect()[0][0]\n",
					"        \n",
					"        # Parse JSON\n",
					"        import json\n",
					"        config = json.loads(json_str)\n",
					"        \n",
					"        print(f\"Successfully read configuration from {config_path}\")\n",
					"        return config\n",
					"    except Exception as e:\n",
					"        print(f\"Error reading configuration from {config_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        print(traceback.format_exc())\n",
					"        return {}\n",
					"\n",
					"# Read configuration files\n",
					"storage_account = \"baubaisadfsaprod\"\n",
					"validation_rules = read_json_config(\n",
					"    storage_account, \n",
					"    f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/config/rules/validation_rules.json\"\n",
					")\n",
					"\n",
					"validation_handlers = read_json_config(\n",
					"    storage_account, \n",
					"    f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/config/rules/validation_handlers.json\"\n",
					")\n",
					"\n",
					"column_constraints = read_json_config(\n",
					"    storage_account, \n",
					"    f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/config/rules/column_constraints.json\"\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Create a global configuration dictionary\n",
					"config_dict = {\n",
					"    \"validation_rules\": validation_rules,\n",
					"    \"validation_handlers\": validation_handlers,\n",
					"    \"column_constraints\": column_constraints\n",
					"}"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def validate_and_transform_all_columns(df):\n",
					"    \"\"\"\n",
					"    Simplified validation and transformation function that assumes character-level\n",
					"    cleaning has already been performed in L1.\n",
					"    \n",
					"    Focuses on data type validation, length constraints, and postcode validation.\n",
					"    \"\"\"\n",
					"    from pyspark.sql.functions import col, when, upper, regexp_extract, regexp_replace, length, substring\n",
					"    \n",
					"    # Skip if configurations are not available\n",
					"    if not validation_rules or not column_constraints:\n",
					"        print(\"Required configurations not available, skipping validation\")\n",
					"        return df\n",
					"    \n",
					"    print(\"Applying column validation and transformations...\")\n",
					"    \n",
					"    # 1. Apply string length constraints\n",
					"    string_columns = column_constraints.get(\"string_columns\", {})\n",
					"    for col_name, constraints in string_columns.items():\n",
					"        if col_name in df.columns and \"max_length\" in constraints:\n",
					"            max_length = constraints[\"max_length\"]\n",
					"            df = df.withColumn(\n",
					"                col_name,\n",
					"                when(\n",
					"                    (col(col_name).isNotNull()) & (length(col(col_name)) > max_length),\n",
					"                    substring(col(col_name), 1, max_length)\n",
					"                ).otherwise(col(col_name))\n",
					"            )\n",
					"            \n",
					"            # Apply case transformation if specified\n",
					"            if \"case\" in constraints and constraints[\"case\"] == \"upper\":\n",
					"                df = df.withColumn(\n",
					"                    col_name,\n",
					"                    when(col(col_name).isNotNull(), upper(col(col_name)))\n",
					"                    .otherwise(col(col_name))\n",
					"                )\n",
					"    \n",
					"    # 2. Standardize postcodes if configured - RETAIN THIS LOGIC\n",
					"    if \"POSTCODE_SETTINGS\" in validation_rules and \"zip\" in df.columns:\n",
					"        settings = validation_rules[\"POSTCODE_SETTINGS\"]\n",
					"        \n",
					"        if \"extraction_pattern\" in settings and \"format_template\" in settings:\n",
					"            # Standardize format of valid postcodes\n",
					"            df = df.withColumn(\n",
					"                \"zip\",\n",
					"                regexp_replace(col(\"zip\"), settings[\"extraction_pattern\"], settings[\"format_template\"])\n",
					"            )\n",
					"        \n",
					"        # Extract postcode_start if column exists\n",
					"        if \"postcode_start\" in df.columns and \"postcode_start_pattern\" in settings:\n",
					"            df = df.withColumn(\n",
					"                \"postcode_start\",\n",
					"                regexp_extract(col(\"zip\"), settings[\"postcode_start_pattern\"], 1)\n",
					"            )\n",
					"    \n",
					"    # 3. Set NULL columns as specified in configuration\n",
					"    if \"NULL_COLUMNS\" in validation_rules:\n",
					"        null_columns = validation_rules[\"NULL_COLUMNS\"]\n",
					"        for col_name in null_columns:\n",
					"            if col_name in df.columns:\n",
					"                df = df.withColumn(col_name, lit(None))\n",
					"    \n",
					"    print(\"Validation and transformations applied successfully\")\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Read Configuration and Mapping"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Column Mapping"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def read_column_mapping(spark, config_path):\n",
					"    \"\"\"Read and parse the column mapping configuration\"\"\"\n",
					"    try:\n",
					"        # Read the JSON file\n",
					"        mapping_df = spark.read.text(config_path)\n",
					"        \n",
					"        # Combine all lines into a single string\n",
					"        json_str = mapping_df.agg(concat_ws(\"\", collect_list(\"value\"))).collect()[0][0]\n",
					"        \n",
					"        # Parse the JSON string\n",
					"        mapping_dict = json.loads(json_str)\n",
					"        \n",
					"        # Check if it has 'schema' structure\n",
					"        if \"schema\" in mapping_dict:\n",
					"            # Convert schema format to mappings format\n",
					"            mappings = {}\n",
					"            for item in mapping_dict[\"schema\"]:\n",
					"                # Use the 'name' field as the standard column name\n",
					"                # Initially set empty variations list\n",
					"                mappings[item[\"name\"]] = []\n",
					"            \n",
					"            return mappings\n",
					"        \n",
					"        # If looking for mappings specifically\n",
					"        elif \"mappings\" in mapping_dict:\n",
					"            return mapping_dict[\"mappings\"]\n",
					"        \n",
					"        # If neither structure is found, raise an error\n",
					"        else:\n",
					"            raise KeyError(\"Neither 'schema' nor 'mappings' found in mapping file\")\n",
					"            \n",
					"    except Exception as e:\n",
					"        try:\n",
					"            # Fallback to using wholeTextFiles\n",
					"            mapping_rdd = spark.sparkContext.wholeTextFiles(config_path)\n",
					"            json_str = mapping_rdd.values().first()\n",
					"            mapping_dict = json.loads(json_str)\n",
					"            \n",
					"            # Check the structure here too\n",
					"            if \"schema\" in mapping_dict:\n",
					"                mappings = {}\n",
					"                for item in mapping_dict[\"schema\"]:\n",
					"                    mappings[item[\"name\"]] = []\n",
					"                return mappings\n",
					"            elif \"mappings\" in mapping_dict:\n",
					"                return mapping_dict[\"mappings\"]\n",
					"            else:\n",
					"                raise KeyError(\"Neither 'schema' nor 'mappings' found in mapping file\")\n",
					"                \n",
					"        except Exception as e2:\n",
					"            raise Exception(f\"Failed to read mapping file. Primary error: {str(e)}, Fallback error: {str(e2)}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def standardize_columns(df, column_mapping):\n",
					"    \"\"\"\n",
					"    Standardize column names based on mapping\n",
					"    \n",
					"    Args:\n",
					"        df: Input DataFrame\n",
					"        column_mapping: Dictionary mapping standard names to variations\n",
					"        \n",
					"    Returns:\n",
					"        DataFrame with standardized column names\n",
					"    \"\"\"\n",
					"    # Get current columns\n",
					"    current_columns = df.columns\n",
					"    \n",
					"    # Create a reverse mapping (from variations to standard names)\n",
					"    reverse_mapping = {}\n",
					"    for standard_name, variations in column_mapping.items():\n",
					"        # Add the standard name itself to the reverse mapping\n",
					"        reverse_mapping[standard_name.lower()] = standard_name\n",
					"        \n",
					"        # Add all variations\n",
					"        for variation in variations:\n",
					"            reverse_mapping[variation.lower()] = standard_name\n",
					"    \n",
					"    # Apply renaming\n",
					"    for col in current_columns:\n",
					"        # Check if column matches any standard name or variation (case-insensitive)\n",
					"        std_name = reverse_mapping.get(col.lower())\n",
					"        if std_name and col != std_name:\n",
					"            df = df.withColumnRenamed(col, std_name)\n",
					"    \n",
					"    return df\n",
					"\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def ensure_register_poll_consistency(df):\n",
					"    \"\"\"\n",
					"    Ensure both register_lett and poll_number exist and are synchronized.\n",
					"    This function handles bidirectional synchronization between these fields.\n",
					"    \"\"\"\n",
					"    # Check if register_lett exists\n",
					"    has_reg_lett = \"register_lett\" in df.columns\n",
					"    has_poll_number = \"poll_number\" in df.columns\n",
					"    has_elector_number = \"Elector Number\" in df.columns\n",
					"    \n",
					"    # Create missing columns if needed\n",
					"    if not has_reg_lett and has_poll_number:\n",
					"        print(\"Adding register_lett as copy of poll_number\")\n",
					"        df = df.withColumn(\"register_lett\", col(\"poll_number\"))\n",
					"    elif not has_reg_lett and has_elector_number:\n",
					"        print(\"Adding register_lett as copy of Elector Number\")\n",
					"        df = df.withColumn(\"register_lett\", col(\"Elector Number\"))\n",
					"    \n",
					"    # Now ensure poll_number exists too\n",
					"    if not has_poll_number and has_reg_lett:\n",
					"        print(\"Adding poll_number as copy of register_lett\")\n",
					"        df = df.withColumn(\"poll_number\", col(\"register_lett\"))\n",
					"    elif not has_poll_number and has_elector_number:\n",
					"        print(\"Adding poll_number as copy of Elector Number\")\n",
					"        df = df.withColumn(\"poll_number\", col(\"Elector Number\"))\n",
					"    \n",
					"    # Now synchronize if both exist but have different values\n",
					"    if \"register_lett\" in df.columns and \"poll_number\" in df.columns:\n",
					"        # Count mismatches\n",
					"        mismatches = df.filter(col(\"register_lett\") != col(\"poll_number\")).count()\n",
					"        if mismatches > 0:\n",
					"            print(f\"Found {mismatches} mismatches between register_lett and poll_number. Synchronizing...\")\n",
					"            # Prefer non-null values\n",
					"            df = df.withColumn(\"register_lett\", \n",
					"                              when(col(\"register_lett\").isNull(), col(\"poll_number\"))\n",
					"                              .otherwise(col(\"register_lett\")))\n",
					"            df = df.withColumn(\"poll_number\", \n",
					"                              when(col(\"poll_number\").isNull(), col(\"register_lett\"))\n",
					"                              .otherwise(col(\"poll_number\")))\n",
					"            # Now make them identical - prefer poll_number for consistency\n",
					"            df = df.withColumn(\"register_lett\", col(\"poll_number\"))\n",
					"            print(\"register_lett and poll_number synchronized\")\n",
					"    \n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def read_schema_config(spark, config_path):\n",
					"    \"\"\"Read and parse the schema configuration\"\"\"\n",
					"    try:\n",
					"        schema_df = spark.read.text(config_path)\n",
					"        json_str = schema_df.agg(concat_ws(\"\", collect_list(\"value\"))).collect()[0][0]\n",
					"        schema_dict = json.loads(json_str)\n",
					"        return schema_dict[\"schema\"]\n",
					"    except Exception as e:\n",
					"        try:\n",
					"            schema_rdd = spark.sparkContext.wholeTextFiles(config_path)\n",
					"            json_str = schema_rdd.values().first()\n",
					"            schema_dict = json.loads(json_str)\n",
					"            return schema_dict[\"schema\"]\n",
					"        except Exception as e2:\n",
					"            raise Exception(f\"Failed to read schema file. Primary error: {str(e)}, Fallback error: {str(e2)}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def ensure_consistent_schema(df, required_columns):\n",
					"    \"\"\"Ensure DataFrame has all required columns with correct data types\"\"\"\n",
					"    current_columns = set(df.columns)\n",
					"    \n",
					"    # Add missing columns\n",
					"    for col_name, col_type in required_columns:\n",
					"        if col_name not in current_columns:\n",
					"            df = df.withColumn(col_name, lit(None).cast(col_type))\n",
					"    \n",
					"    # Select only the required columns in the specified order\n",
					"    return df.select([col(c[0]).cast(c[1]) for c in required_columns])"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def get_new_files(spark, input_path, process_log_path):\n",
					"    \"\"\"Identify which files are new and need processing with improved error handling\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    # Override process_log_path to use our specific L2 text log file\n",
					"    l2_log_path = f\"abfss://juror-etl@baubaisadfsaprod.dfs.core.windows.net/l2_process_logs/processed_files_log.txt\"\n",
					"    \n",
					"    # Get all files in the input directory\n",
					"    all_files = []\n",
					"    \n",
					"    # List all items in the directory\n",
					"    try:\n",
					"        items = mssparkutils.fs.ls(input_path)\n",
					"        print(f\"Found {len(items)} items in {input_path}\")\n",
					"        \n",
					"        for item in items:\n",
					"            if (hasattr(item, 'path') and (\n",
					"                   item.path.endswith(\".parquet\") or \n",
					"                   (hasattr(item, 'isDir') and item.isDir and \n",
					"                    mssparkutils.fs.exists(f\"{item.path}/_SUCCESS\"))\n",
					"               )):\n",
					"                all_files.append(item.path)\n",
					"                print(f\"Added file/directory for processing: {item.path}\")\n",
					"    except Exception as e:\n",
					"        print(f\"Error listing files in {input_path}: {str(e)}\")\n",
					"    \n",
					"    # Read process log to get previously processed files\n",
					"    processed_files = []\n",
					"    if mssparkutils.fs.exists(l2_log_path):\n",
					"        try:\n",
					"            # Read as plain text file using direct file system API\n",
					"            print(f\"Reading process log from {l2_log_path}\")\n",
					"            content = mssparkutils.fs.read(l2_log_path)\n",
					"            \n",
					"            # Process each line\n",
					"            for line in content.split(\"\\n\"):\n",
					"                if line.strip():\n",
					"                    # Extract just the file path (before the | symbol if it exists)\n",
					"                    if \"|\" in line:\n",
					"                        file_path = line.split(\"|\")[0].strip()\n",
					"                    else:\n",
					"                        file_path = line.strip()\n",
					"                    processed_files.append(file_path)\n",
					"            \n",
					"            print(f\"Read {len(processed_files)} processed files from log\")\n",
					"        except Exception as e:\n",
					"            print(f\"Error reading process log: {str(e)}\")\n",
					"            print(\"Treating all files as new since process log couldn't be read\")\n",
					"    else:\n",
					"        print(f\"Process log doesn't exist at {l2_log_path}, treating all files as new\")\n",
					"    \n",
					"    # Identify new files with more detailed logging\n",
					"    new_files = []\n",
					"    for f in all_files:\n",
					"        if f not in processed_files:\n",
					"            new_files.append(f)\n",
					"            print(f\"New file to process: {f}\")\n",
					"        else:\n",
					"            print(f\"Skipping already processed file: {f}\")\n",
					"    \n",
					"    print(f\"Found {len(new_files)} new files to process out of {len(all_files)} total files\")\n",
					"    return new_files, all_files"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def process_input_files(spark, file_paths, column_mapping):\n",
					"    \"\"\"Read and standardize columns from input files with reg_lett handling\"\"\"\n",
					"    processed_dfs = []\n",
					"    \n",
					"    for file_path in file_paths:\n",
					"        try:\n",
					"            # Read the file\n",
					"            df = spark.read.parquet(file_path)\n",
					"            \n",
					"            # Standardize column names based on mapping\n",
					"            standardized_df = standardize_columns(df, column_mapping)\n",
					"            \n",
					"            # Check if reg_lett is missing but poll_number or Elector Number exists\n",
					"            has_reg_lett = \"register_lett\" in standardized_df.columns or \"reg_lett\" in standardized_df.columns\n",
					"            \n",
					"            if not has_reg_lett:\n",
					"                # Find the poll number column\n",
					"                poll_number_col = None\n",
					"                if \"poll_number\" in standardized_df.columns:\n",
					"                    poll_number_col = \"poll_number\"\n",
					"                elif \"Elector Number\" in standardized_df.columns:\n",
					"                    poll_number_col = \"Elector Number\"\n",
					"                \n",
					"                # Add reg_lett as a copy of poll_number if it exists\n",
					"                if poll_number_col:\n",
					"                    print(f\"Adding register_lett as copy of {poll_number_col} for file {file_path}\")\n",
					"                    standardized_df = standardized_df.withColumn(\"register_lett\", col(poll_number_col))\n",
					"            \n",
					"            # Add source metadata\n",
					"            standardized_df = standardized_df.withColumn(\"source_file\", lit(file_path))\n",
					"            standardized_df = standardized_df.withColumn(\"process_timestamp\", current_timestamp())\n",
					"            \n",
					"            processed_dfs.append(standardized_df)\n",
					"            \n",
					"        except Exception as e:\n",
					"            print(f\"Error processing {file_path}: {str(e)}\")\n",
					"            import traceback\n",
					"            print(traceback.format_exc())\n",
					"    \n",
					"    return processed_dfs"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def process_input_file_with_tracking(spark, file_path, column_mapping, config):\n",
					"    \"\"\"Process a single input file with enhanced tracking of record contributions\"\"\"\n",
					"    logger = logging.getLogger()\n",
					"    \n",
					"    try:\n",
					"      \n",
					"       #Read config\n",
					"        output_path = config[\"output_path\"]\n",
					"        # Read the file\n",
					"        df = spark.read.parquet(file_path)\n",
					"        initial_count = df.count()\n",
					"        \n",
					"        # Process the file\n",
					"        standardized_df = standardize_columns(df, column_mapping)\n",
					"        standardized_df = ensure_register_poll_consistency(standardized_df)\n",
					"        \n",
					"        # Add metadata\n",
					"        file_name = os.path.basename(file_path)\n",
					"        process_time = datetime.now().isoformat()\n",
					"        \n",
					"        standardized_df = standardized_df.withColumn(\"source_file\", lit(file_path))\n",
					"        standardized_df = standardized_df.withColumn(\"process_timestamp\", lit(process_time))\n",
					"        standardized_df = standardized_df.withColumn(\"batch_id\", lit(process_time.split(\"T\")[0]))\n",
					"        \n",
					"        # Apply transformations\n",
					"        transformed_df = transform_to_target_schema(standardized_df, column_mapping)\n",
					"        if HASH_SENSITIVE_DATA:\n",
					"            transformed_df = apply_hashing_to_voters(transformed_df) \n",
					"        \n",
					"        # Generate hash_id if not exists\n",
					"        transformed_df = generate_hash_id(transformed_df)\n",
					"        \n",
					"        # Write to Delta with tracking\n",
					"        result = write_transformed_data_delta(transformed_df, output_path)\n",
					"        \n",
					"        # Determine file contribution status\n",
					"        if result[\"status\"] == \"success\":\n",
					"            if result[\"new_records\"] > 0:\n",
					"                status = \"PROCESSED_CONTRIBUTED\"\n",
					"            else:\n",
					"                status = \"PROCESSED_NO_CONTRIBUTION\"\n",
					"                \n",
					"            # Update log\n",
					"            update_enhanced_process_log(\n",
					"                spark, \n",
					"                file_path, \n",
					"                status, \n",
					"                {\"record_count\": initial_count, \"new_records\": result[\"new_records\"]}\n",
					"            )\n",
					"            \n",
					"            return True, result[\"new_records\"]\n",
					"        else:\n",
					"            # Update log with error\n",
					"            update_enhanced_process_log(\n",
					"                spark, \n",
					"                file_path, \n",
					"                \"PROCESSING_ERROR\", \n",
					"                {\"error\": result.get(\"error\", \"Unknown error\")}\n",
					"            )\n",
					"            return False, 0\n",
					"            \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error processing {file_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        \n",
					"        # Update log with error\n",
					"        update_enhanced_process_log(\n",
					"            spark, \n",
					"            file_path, \n",
					"            \"PROCESSING_ERROR\", \n",
					"            {\"error\": str(e)}\n",
					"        )\n",
					"        return False, 0"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Merge Files with Standardized Schema"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def merge_dataframes(dfs):\n",
					"    \"\"\"Merge all dataframes with standardized schema, preserving original date information\"\"\"\n",
					"    if not dfs:\n",
					"        raise ValueError(\"No dataframes to merge\")\n",
					"        \n",
					"    # Merge using unionByName to handle schema variations\n",
					"    merged_df = dfs[0]\n",
					"    for df in dfs[1:]:\n",
					"        merged_df = merged_df.unionByName(df, allowMissingColumns=True)\n",
					"    \n",
					"    # Check which date column exists in the merged data\n",
					"    date_column = None\n",
					"    if \"CreationDate\" in merged_df.columns:\n",
					"        date_column = \"CreationDate\"\n",
					"    elif \"creation_date\" in merged_df.columns:\n",
					"        date_column = \"creation_date\"\n",
					"    \n",
					"    # Only perform date check if we found a date column\n",
					"    if date_column:\n",
					"        # Check if date values are too uniform\n",
					"        distinct_dates = merged_df.select(date_column).distinct().count()\n",
					"        logging.info(f\"Found {distinct_dates} distinct dates in column {date_column}\")\n",
					"        \n",
					"        if distinct_dates == 1:\n",
					"            logging.warning(f\"Only one distinct date found in {date_column} - this may indicate a problem!\")\n",
					"            \n",
					"            # Try to extract better date information from source_file if available\n",
					"            if \"source_file\" in merged_df.columns:\n",
					"                merged_df = merged_df.withColumn(\n",
					"                    \"extracted_date\",\n",
					"                    regexp_extract(col(\"source_file\"), r\"(\\d{8})_\", 1)\n",
					"                )\n",
					"                \n",
					"                # Use extracted date if available, otherwise keep existing\n",
					"                merged_df = merged_df.withColumn(\n",
					"                    date_column,  # Update whichever date column exists\n",
					"                    when(col(\"extracted_date\").isNotNull(), col(\"extracted_date\"))\n",
					"                    .otherwise(col(date_column))\n",
					"                )\n",
					"                \n",
					"                # Clean up temporary column\n",
					"                merged_df = merged_df.drop(\"extracted_date\")\n",
					"                \n",
					"                logging.info(f\"After date correction: {merged_df.select(date_column).distinct().count()} distinct dates\")\n",
					"    else:\n",
					"        logging.warning(\"No date column found in merged data!\")\n",
					"    \n",
					"    return merged_df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def write_transformed_data_delta(df, output_path):\n",
					"    \"\"\"Write data to Delta table with proper handling for initial vs incremental writes\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    from delta.tables import DeltaTable\n",
					"    from pyspark.sql.utils import AnalysisException\n",
					"    logger = logging.getLogger()\n",
					"\n",
					"    try:\n",
					"        # Enforce schema before any write\n",
					"        df = cast_to_target_schema(df)\n",
					"        # Try to read existing table to determine if this is a merge\n",
					"        try:\n",
					"            delta_table = DeltaTable.forPath(spark, output_path)\n",
					"            existing_data = spark.read.format(\"delta\").load(output_path)\n",
					"            existing_count = existing_data.count()\n",
					"\n",
					"            logger.info(f\"Delta table already exists at {output_path}. Performing MERGE.\")\n",
					"            delta_table.alias(\"existing\").merge(\n",
					"                df.alias(\"updates\"),\n",
					"                \"existing.hash_id = updates.hash_id\"\n",
					"            ).whenNotMatchedInsertAll().execute()\n",
					"\n",
					"            new_total_count = spark.read.format(\"delta\").load(output_path).count()\n",
					"            new_records = new_total_count - existing_count\n",
					"\n",
					"            logger.info(f\"✅ Merge complete: {new_records} new records added.\")\n",
					"            return {\"status\": \"success\", \"new_records\": new_records, \"total_records\": new_total_count}\n",
					"\n",
					"        except AnalysisException:\n",
					"            logger.info(f\"🆕 Delta table not found at {output_path}. Performing INITIAL LOAD.\")\n",
					"            df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
					"            new_records = df.count()\n",
					"            logger.info(f\"✅ Initial load complete: {new_records} records written.\")\n",
					"            return {\"status\": \"success\", \"new_records\": new_records, \"total_records\": new_records}\n",
					"\n",
					"    except Exception as e:\n",
					"        logger.error(f\"❌ Error writing to Delta table at {output_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return {\"status\": \"error\", \"error\": str(e)}"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Deduplicate Based on Latest Data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def deduplicate_by_creation_date(df):\n",
					"    \"\"\"Keep only the most recent record for each unique entity based on creation date\"\"\"\n",
					"    # Check if we have a creation date column\n",
					"    if \"CreationDate\" in df.columns:\n",
					"        # Use combination of name and address as a unique identifier instead of LA_Code\n",
					"        window_spec = Window.partitionBy(\n",
					"            \"Elector Surname\", \"Elector Forename\", \"Address1\", \"PostCode\"\n",
					"        ).orderBy(desc(\"CreationDate\"))\n",
					"        \n",
					"        deduplicated = df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                         .filter(col(\"row_num\") == 1) \\\n",
					"                         .drop(\"row_num\")\n",
					"        \n",
					"        logging.info(f\"Kept {deduplicated.count()} records after transformation\")\n",
					"        return deduplicated\n",
					"    else:\n",
					"        logging.warning(\"Missing CreationDate column, skipping transformation\")\n",
					"        return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def optimise_deduplicate_data(df):\n",
					"    \"\"\"\n",
					"    Deduplicate data using exactly the same fields that will be used for hash_id generation:\n",
					"    - register_lett/reg_lett\n",
					"    - Elector Forename/fname\n",
					"    - Elector Surname/lname\n",
					"    - rec_num/LA_Code\n",
					"    \"\"\"\n",
					"    print(f\"PRE-DEDUPLICATION COUNT: {df.count()} rows\")\n",
					"    \n",
					"    # Define key fields that match our hash_id generation logic EXACTLY\n",
					"    key_fields = []\n",
					"    \n",
					"    # Map of field types to possible column names (in priority order)\n",
					"    field_mapping = {\n",
					"        \"register\": [\"register_lett\", \"reg_lett\"],\n",
					"        \"forename\": [\"Elector Forename\", \"fname\"],\n",
					"        \"surname\": [\"Elector Surname\", \"lname\"],\n",
					"        \"rec_num\": [\"rec_num\", \"LA_Code\"]\n",
					"    }\n",
					"    \n",
					"    # Find which columns exist\n",
					"    actual_columns = set(df.columns)\n",
					"    for field_type, possible_names in field_mapping.items():\n",
					"        found = False\n",
					"        for name in possible_names:\n",
					"            if name in actual_columns:\n",
					"                key_fields.append(name)\n",
					"                found = True\n",
					"                break\n",
					"        if not found:\n",
					"            print(f\"WARNING: No column found for {field_type}\")\n",
					"    \n",
					"    # If we don't have enough identifying columns, add just elector_number\n",
					"    if len(key_fields) < 2 and \"Elector Number\" in actual_columns:\n",
					"        print(\"Adding Elector Number as additional identifying field\")\n",
					"        key_fields.append(\"Elector Number\")\n",
					"    \n",
					"    print(f\"Using these columns for deduplication: {key_fields}\")\n",
					"    \n",
					"    if len(key_fields) == 0:\n",
					"        print(\"No suitable columns for deduplication found - returning all data\")\n",
					"        return df\n",
					"    \n",
					"    # Use dropDuplicates with our consistent key fields\n",
					"    deduplicated = df.dropDuplicates(key_fields)\n",
					"    \n",
					"    final_count = deduplicated.count()\n",
					"    print(f\"POST-DEDUPLICATION COUNT: {final_count} rows\")\n",
					"    print(f\"ROWS REMOVED: {df.count() - final_count}\")\n",
					"    \n",
					"    return deduplicated"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def improved_deduplicate_data(df):\n",
					"    \"\"\"\n",
					"    Improved deduplication using window functions to keep most recent record\n",
					"    per set of identifying columns\n",
					"    \"\"\"\n",
					"    # Define identifying columns\n",
					"    id_columns = [\n",
					"        \"fname\", \"lname\", \"address\", \"poll_number\", \"rec_num\", \"hash_id\"\n",
					"    ]\n",
					"    \n",
					"    # Filter list to only include columns that exist in the dataframe\n",
					"    existing_id_columns = [col for col in id_columns if col in df.columns]\n",
					"    \n",
					"    if len(existing_id_columns) == 0:\n",
					"        print(\"No suitable columns for deduplication found - returning all data\")\n",
					"        return df\n",
					"    \n",
					"    print(f\"Deduplicating using columns: {existing_id_columns}\")\n",
					"    print(f\"Input record count: {df.count()}\")\n",
					"    \n",
					"    # Determine which date column to use for ordering\n",
					"    date_column = None\n",
					"    for possible_date_col in [\"creation_date\", \"CreationDate\", \"process_timestamp\"]:\n",
					"        if possible_date_col in df.columns:\n",
					"            date_column = possible_date_col\n",
					"            break\n",
					"    \n",
					"    if date_column:\n",
					"        print(f\"Using {date_column} for ordering records\")\n",
					"        # Create window spec partitioned by identifying columns with date ordering\n",
					"        window_spec = Window.partitionBy([col(x) for x in existing_id_columns]) \\\n",
					"                           .orderBy(col(date_column).desc())\n",
					"    else:\n",
					"        print(\"No date column found for ordering, using default ordering\")\n",
					"        # If no date column, just use window without ordering\n",
					"        window_spec = Window.partitionBy([col(x) for x in existing_id_columns])\n",
					"    \n",
					"    # Apply deduplication\n",
					"    deduplicated_df = df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                        .filter(col(\"row_num\") == 1) \\\n",
					"                        .drop(\"row_num\")\n",
					"    \n",
					"    output_count = deduplicated_df.count()\n",
					"    print(f\"Output record count: {output_count}\")\n",
					"    print(f\"Removed {df.count() - output_count} duplicate records\")\n",
					"    \n",
					"    return deduplicated_df"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Apply Hashing for Sensitive Data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def generate_hash_id(df):\n",
					"    \"\"\"Corrected: generate hash_id using register_lett + fname + lname + rec_num\"\"\"\n",
					"    from pyspark.sql.functions import xxhash64, concat_ws, col, upper, trim, lit, coalesce\n",
					"    \n",
					"    logger = logging.getLogger()\n",
					"    logger.info(\"Generating corrected hash_id with full name and identifiers\")\n",
					"    \n",
					"    # Debug: Print initial count\n",
					"    initial_count = df.count()\n",
					"    logger.info(f\"Initial row count: {initial_count}\")\n",
					"    \n",
					"    # Map of field types to possible column names (in priority order)\n",
					"    field_mapping = {\n",
					"        \"register\": [\"register_lett\", \"poll_number\", \"Elector Number\"],\n",
					"        \"forename\": [\"fname\", \"Elector Forename\"],\n",
					"        \"surname\": [\"lname\", \"Elector Surname\"],\n",
					"        \"rec_num\": [\"rec_num\", \"LA_Code\"],\n",
					"        \"zip_code\": [\"zip\", \"PostCode\"],\n",
					"        \"address\": [\"address\", \"Address1\", \"address1\"]\n",
					"    }\n",
					"    \n",
					"    # Find which columns exist for each field\n",
					"    field_values = {}\n",
					"    for field_type, possible_columns in field_mapping.items():\n",
					"        for col_name in possible_columns:\n",
					"            if col_name in df.columns:\n",
					"                field_values[field_type] = col(col_name)\n",
					"                logger.info(f\"Using {col_name} for {field_type}\")\n",
					"                break\n",
					"        if field_type not in field_values:\n",
					"            logger.warning(f\"No columns found for {field_type}, using empty string\")\n",
					"            field_values[field_type] = lit(\"\")\n",
					"    \n",
					"    # Generate the hash_id\n",
					"    hashed_df = df.withColumn(\n",
					"        \"hash_id\",\n",
					"        xxhash64(concat_ws(\n",
					"            \"||\",\n",
					"            coalesce(trim(field_values.get(\"register\", lit(\"\"))), lit(\"\")),\n",
					"            coalesce(upper(trim(field_values.get(\"forename\", lit(\"\")))), lit(\"\")),\n",
					"            coalesce(upper(trim(field_values.get(\"surname\", lit(\"\")))), lit(\"\")),\n",
					"            coalesce(trim(field_values.get(\"rec_num\", lit(\"\"))), lit(\"\")),\n",
					"            coalesce(trim(field_values.get(\"zip_code\", lit(\"\"))), lit(\"\")),\n",
					"            coalesce(trim(field_values.get(\"address\", lit(\"\"))), lit(\"\"))\n",
					"        ))\n",
					"    )\n",
					"    \n",
					"    # Debug info\n",
					"    null_hashes = hashed_df.filter(col(\"hash_id\").isNull()).count()\n",
					"    distinct_hashes = hashed_df.select(\"hash_id\").distinct().count()\n",
					"    \n",
					"    logger.info(f\"Created hash_id column with {distinct_hashes} distinct values\")\n",
					"    logger.info(f\"Records with null hash_id: {null_hashes}\")\n",
					"    \n",
					"    if distinct_hashes < 100 and initial_count > 1000:\n",
					"        logger.warning(f\"Very few distinct hash values ({distinct_hashes}) for {initial_count} records\")\n",
					"    \n",
					"    return hashed_df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def apply_hashing_to_voters(df):\n",
					"    \"\"\"\n",
					"    Apply hashing to sensitive fields only for testing purposes.\n",
					"    This is separate from hash_id creation which should be done first.\n",
					"    \n",
					"    Hashes:\n",
					"    - fname/Elector Forename\n",
					"    - lname/Elector Surname\n",
					"    - dob/Elector DOB\n",
					"    - address fields\n",
					"    \"\"\"\n",
					"    from pyspark.sql.functions import xxhash64, substring, hex, col, when, lit\n",
					"    \n",
					"    logger = logging.getLogger()\n",
					"    logger.info(\"Applying hashing to sensitive fields for testing\")\n",
					"    \n",
					"    # Save the original row count for verification\n",
					"    original_count = df.count()\n",
					"    \n",
					"    # Define sensitive fields to hash with their variations\n",
					"    sensitive_fields = {\n",
					"        \"names\": [\"fname\", \"lname\", \"Elector Forename\", \"Elector Surname\"],\n",
					"        \"dates\": [\"dob\", \"Elector DOB\"],\n",
					"        \"addresses\": [\"address\", \"address1\", \"address2\", \"address3\", \"address4\",\n",
					"                    \"Address1\", \"Address2\", \"Address3\", \"Address4\", \"Address5\"]\n",
					"    }\n",
					"    \n",
					"    # Hash each sensitive field that exists in the DataFrame\n",
					"    for category, fields in sensitive_fields.items():\n",
					"        for field in fields:\n",
					"            if field in df.columns:\n",
					"                logger.info(f\"Hashing sensitive field: {field}\")\n",
					"                \n",
					"                # Different hashing approach based on field type\n",
					"                if category == \"names\":\n",
					"                    # Names - hash to 20 chars\n",
					"                    df = df.withColumn(\n",
					"                        field,\n",
					"                        when(col(field).isNotNull(),\n",
					"                             substring(hex(xxhash64(col(field))), 1, 20)\n",
					"                        ).otherwise(col(field))\n",
					"                    )\n",
					"                elif category == \"dates\":\n",
					"                    # DOB - special format for dates\n",
					"                    df = df.withColumn(\n",
					"                        field,\n",
					"                        when(col(field).isNotNull(),\n",
					"                             substring(hex(xxhash64(col(field))), 1, 10)\n",
					"                        ).otherwise(col(field))\n",
					"                    )\n",
					"                elif category == \"addresses\":\n",
					"                    # Addresses - hash to 35 chars\n",
					"                    df = df.withColumn(\n",
					"                        field,\n",
					"                        when(col(field).isNotNull(),\n",
					"                             substring(hex(xxhash64(col(field))), 1, 35)\n",
					"                        ).otherwise(col(field))\n",
					"                    )\n",
					"    \n",
					"    # Verify we have the same number of rows\n",
					"    final_count = df.count()\n",
					"    if final_count != original_count:\n",
					"        logger.warning(f\"Row count changed during sensitive data hashing: {original_count} -> {final_count}\")\n",
					"    \n",
					"    logger.info(\"Completed hashing of sensitive fields\")\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Transform to Target Schema"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def fix_address_fields(df):\n",
					"    \"\"\"\n",
					"    Fixes address fields:\n",
					"    - Moves address3 to address4 if address4 is null and address3 is not null and doesn't start with number\n",
					"    - Moves address2 to address4 if both address3 and address4 are null and address2 doesn't start with number\n",
					"    - If address4 is still null, tries to use address5\n",
					"    - Removes ZIP/postcode from end of any address field\n",
					"    - Sets address6 to NULL\n",
					"    \"\"\"\n",
					"    address_cols = [\"address\", \"address2\", \"address3\", \"address4\", \"address5\", \"address6\"]\n",
					"\n",
					"    # Step 1: Trim and nullify empty strings\n",
					"    for col_name in address_cols:\n",
					"        if col_name in df.columns:\n",
					"            df = df.withColumn(\n",
					"                col_name,\n",
					"                when(\n",
					"                    (col(col_name).isNull()) | (trim(col(col_name)) == \"\"),\n",
					"                    lit(None)\n",
					"                ).otherwise(trim(col(col_name)))\n",
					"            )\n",
					"\n",
					"    # Step 2: Remove ZIP from address fields\n",
					"    if \"zip\" in df.columns:\n",
					"        zip_col = upper(trim(col(\"zip\")))\n",
					"        for addr_col in address_cols:\n",
					"            if addr_col in df.columns:\n",
					"                # Remove exact ZIP matches\n",
					"                df = df.withColumn(\n",
					"                    addr_col,\n",
					"                    when(\n",
					"                        col(addr_col).isNotNull() & zip_col.isNotNull() &\n",
					"                        (upper(trim(col(addr_col))) == zip_col),\n",
					"                        lit(None)\n",
					"                    ).otherwise(col(addr_col))\n",
					"                )\n",
					"                \n",
					"                # Also try to remove ZIP when it appears at the end of address line with optional comma/space\n",
					"                df = df.withColumn(\n",
					"                    addr_col,\n",
					"                    when(\n",
					"                        col(addr_col).isNotNull() & zip_col.isNotNull() &\n",
					"                        upper(col(addr_col)).rlike(f\".*[,\\\\s-]*{regexp_escape(zip_col)}$\"),\n",
					"                        regexp_replace(col(addr_col), f\"[,\\\\s-]*{regexp_escape(zip_col)}$\", \"\")\n",
					"                    ).otherwise(col(addr_col))\n",
					"                )\n",
					"\n",
					"    # Step 3: Address field shuffling according to requirements\n",
					"    if \"address4\" in df.columns:\n",
					"        # Move address3 to address4 if address4 is null and address3 doesn't start with a number\n",
					"        if \"address3\" in df.columns:\n",
					"            df = df.withColumn(\n",
					"                \"address4\",\n",
					"                when(\n",
					"                    col(\"address4\").isNull() &\n",
					"                    col(\"address3\").isNotNull() &\n",
					"                    ~col(\"address3\").rlike(\"^[0-9]\"),\n",
					"                    col(\"address3\")\n",
					"                ).otherwise(col(\"address4\"))\n",
					"            )\n",
					"            # Clear address3 if moved\n",
					"            df = df.withColumn(\n",
					"                \"address3\",\n",
					"                when(\n",
					"                    col(\"address4\") == col(\"address3\"),\n",
					"                    lit(None)\n",
					"                ).otherwise(col(\"address3\"))\n",
					"            )\n",
					"\n",
					"        # Move address2 to address4 if address4 is null, address3 is null, and address2 doesn't start with a number\n",
					"        if \"address2\" in df.columns:\n",
					"            df = df.withColumn(\n",
					"                \"address4\",\n",
					"                when(\n",
					"                    col(\"address4\").isNull() &\n",
					"                    (col(\"address3\").isNull() | trim(col(\"address3\")) == \"\") &\n",
					"                    col(\"address2\").isNotNull() &\n",
					"                    ~col(\"address2\").rlike(\"^[0-9]\"),\n",
					"                    col(\"address2\")\n",
					"                ).otherwise(col(\"address4\"))\n",
					"            )\n",
					"            # Clear address2 if moved\n",
					"            df = df.withColumn(\n",
					"                \"address2\",\n",
					"                when(\n",
					"                    col(\"address4\") == col(\"address2\"),\n",
					"                    lit(None)\n",
					"                ).otherwise(col(\"address2\"))\n",
					"            )\n",
					"\n",
					"        # If address4 is still null, try address5\n",
					"        if \"address5\" in df.columns:\n",
					"            df = df.withColumn(\n",
					"                \"address4\",\n",
					"                when(\n",
					"                    col(\"address4\").isNull() &\n",
					"                    col(\"address5\").isNotNull(),\n",
					"                    col(\"address5\")\n",
					"                ).otherwise(col(\"address4\"))\n",
					"            )\n",
					"            # Clear address5 if moved\n",
					"            df = df.withColumn(\n",
					"                \"address5\",\n",
					"                when(\n",
					"                    col(\"address4\") == col(\"address5\"),\n",
					"                    lit(None)\n",
					"                ).otherwise(col(\"address5\"))\n",
					"            )\n",
					"\n",
					"    # Step 4: Nullify address6\n",
					"    if \"address6\" in df.columns:\n",
					"        df = df.withColumn(\"address6\", lit(None))\n",
					"\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def comprehensive_address_validation(df, config_dict):\n",
					"    \"\"\"\n",
					"    Comprehensive validation based on configuration rules\n",
					"    \"\"\"\n",
					"    from pyspark.sql.functions import col, when, lit, length, trim, regexp_replace, upper\n",
					"    \n",
					"    logger = logging.getLogger()\n",
					"    logger.info(\"Starting comprehensive address validation\")\n",
					"    \n",
					"    # Extract validation rules from config\n",
					"    validation_rules = config_dict.get(\"validation_rules\", {}).get(\"VALIDATION_RULES\", {})\n",
					"    \n",
					"    # If no validation rules defined, use sensible defaults\n",
					"    if not validation_rules:\n",
					"        logger.warning(\"No validation rules found in config, using defaults\")\n",
					"        validation_rules = {\n",
					"            \"address_validation\": {\n",
					"                \"fix_empty_address4\": True,\n",
					"                \"remove_empty_address4\": False,\n",
					"                \"remove_duplicate_postcodes\": True,\n",
					"                \"remove_other_electors\": True,  #rule for removing OTHER ELECTORS\n",
					"                \"address_fields\": [\"address\", \"address2\", \"address3\", \"address4\", \"address5\", \"address6\"]\n",
					"            },\n",
					"            \"name_validation\": {\n",
					"                \"remove_empty_names\": True,\n",
					"                \"required_name_fields\": [\"fname\", \"lname\"]\n",
					"            },\n",
					"            \"title_validation\": {\n",
					"                \"clean_special_chars\": True,\n",
					"                \"remove_invalid_with_missing_fields\": True,\n",
					"                \"required_fields_for_title_check\": [\"fname\", \"lname\", \"address\"]\n",
					"            },\n",
					"            \"completeness_validation\": {\n",
					"                \"remove_empty_address_with_names\": True,\n",
					"                \"remove_empty_zip\": True\n",
					"            }\n",
					"        }\n",
					"    \n",
					"    # Save initial count for comparison\n",
					"    initial_count = df.count()\n",
					"    removal_counts = {\"total\": 0}\n",
					"    \n",
					"    #--------------------------------------------------------------------------------\n",
					"    # PHASE 1: FIX ISSUES WHERE POSSIBLE\n",
					"    #--------------------------------------------------------------------------------\n",
					"    \n",
					"    # 1. Fix empty address4 by moving up from address5/6 if configured\n",
					"    address_rules = validation_rules.get(\"address_validation\", {})\n",
					"    if address_rules.get(\"fix_empty_address4\", True) and \"address4\" in df.columns:\n",
					"        # Fix from address5\n",
					"        if \"address5\" in df.columns:\n",
					"            df = df.withColumn(\n",
					"                \"address4\", \n",
					"                when(\n",
					"                    (col(\"address4\").isNull() | (trim(col(\"address4\")) == \"\")) & \n",
					"                    col(\"address5\").isNotNull() & (trim(col(\"address5\")) != \"\"),\n",
					"                    col(\"address5\")\n",
					"                ).otherwise(col(\"address4\"))\n",
					"            )\n",
					"            # Clear address5 if copied\n",
					"            df = df.withColumn(\"address5\", when(col(\"address4\") == col(\"address5\"), lit(None)).otherwise(col(\"address5\")))\n",
					"            \n",
					"        # Fix from address6 if still needed\n",
					"        if \"address6\" in df.columns:\n",
					"            df = df.withColumn(\n",
					"                \"address4\", \n",
					"                when(\n",
					"                    (col(\"address4\").isNull() | (trim(col(\"address4\")) == \"\")) & \n",
					"                    col(\"address6\").isNotNull() & (trim(col(\"address6\")) != \"\"),\n",
					"                    col(\"address6\")\n",
					"                ).otherwise(col(\"address4\"))\n",
					"            )\n",
					"            # Clear address6 if copied\n",
					"            df = df.withColumn(\"address6\", when(col(\"address4\") == col(\"address6\"), lit(None)).otherwise(col(\"address6\")))\n",
					"    \n",
					"    # 2. Remove duplicate postcodes from address fields if configured\n",
					"    if address_rules.get(\"remove_duplicate_postcodes\", True) and \"zip\" in df.columns:\n",
					"        address_fields = address_rules.get(\"address_fields\", [\"address\", \"address2\", \"address3\", \"address4\", \"address5\", \"address6\"])\n",
					"        for addr_col in address_fields:\n",
					"            if addr_col in df.columns:\n",
					"                df = df.withColumn(\n",
					"                    addr_col,\n",
					"                    when(\n",
					"                        col(\"zip\").isNotNull() & (trim(upper(col(addr_col))) == trim(upper(col(\"zip\")))),\n",
					"                        lit(None)\n",
					"                    ).otherwise(col(addr_col))\n",
					"                )\n",
					"    \n",
					"    # 3. Clean title field if configured\n",
					"    title_rules = validation_rules.get(\"title_validation\", {})\n",
					"    if title_rules.get(\"clean_special_chars\", True) and \"title\" in df.columns:\n",
					"        # Get title field constraints\n",
					"        field_constraints = validation_rules.get(\"field_constraints\", {}).get(\"title\", {})\n",
					"        max_length = field_constraints.get(\"max_length\", 10)\n",
					"        \n",
					"        required_fields = title_rules.get(\"required_fields_for_title_check\", [\"fname\", \"lname\", \"address\"])\n",
					"        if all(field in df.columns for field in required_fields):\n",
					"            # Create condition for all required fields present\n",
					"            all_required_present = None\n",
					"            for field in required_fields:\n",
					"                if all_required_present is None:\n",
					"                    all_required_present = (col(field).isNotNull() & (trim(col(field)) != \"\"))\n",
					"                else:\n",
					"                    all_required_present = all_required_present & (col(field).isNotNull() & (trim(col(field)) != \"\"))\n",
					"            \n",
					"            # Clean title when all required fields present\n",
					"            df = df.withColumn(\n",
					"                \"title\",\n",
					"                when(\n",
					"                    all_required_present,\n",
					"                    regexp_replace(col(\"title\"), \"[^a-zA-Z\\\\s]\", \"\")\n",
					"                ).otherwise(col(\"title\"))\n",
					"            )\n",
					"        \n",
					"        # Trim to max length regardless\n",
					"        df = df.withColumn(\n",
					"            \"title\",\n",
					"            when(\n",
					"                col(\"title\").isNotNull() & (length(col(\"title\")) > max_length),\n",
					"                substring(col(\"title\"), 1, max_length)\n",
					"            ).otherwise(col(\"title\"))\n",
					"        )\n",
					"    \n",
					"    #--------------------------------------------------------------------------------\n",
					"    # PHASE 2: FLAG RECORDS FOR REMOVAL\n",
					"    #--------------------------------------------------------------------------------\n",
					"    \n",
					"    # Initialize removal flags\n",
					"    df = df.withColumn(\"_remove_record\", lit(False))\n",
					"    df = df.withColumn(\"_removal_reason\", lit(None).cast(\"string\"))\n",
					"    \n",
					"    # Rule 1: Invalid title with missing required fields\n",
					"    title_rules = validation_rules.get(\"title_validation\", {})\n",
					"    if title_rules.get(\"remove_invalid_with_missing_fields\", True) and \"title\" in df.columns:\n",
					"        required_fields = title_rules.get(\"required_fields_for_title_check\", [\"fname\", \"lname\", \"address\"])\n",
					"        \n",
					"        # Check any required field is missing\n",
					"        any_required_missing = None\n",
					"        for field in required_fields:\n",
					"            if field in df.columns:\n",
					"                if any_required_missing is None:\n",
					"                    any_required_missing = (col(field).isNull() | (trim(col(field)) == \"\"))\n",
					"                else:\n",
					"                    any_required_missing = any_required_missing | (col(field).isNull() | (trim(col(field)) == \"\"))\n",
					"        \n",
					"        invalid_title_cond = col(\"title\").rlike(\"[^a-zA-Z\\\\s]\") & any_required_missing\n",
					"        \n",
					"        # Apply the rule\n",
					"        df = df.withColumn(\n",
					"            \"_remove_record\", \n",
					"            when(invalid_title_cond, lit(True)).otherwise(col(\"_remove_record\"))\n",
					"        )\n",
					"        df = df.withColumn(\n",
					"            \"_removal_reason\", \n",
					"            when(invalid_title_cond, lit(\"invalid_title_missing_fields\"))\n",
					"                .otherwise(col(\"_removal_reason\"))\n",
					"        )\n",
					"    \n",
					"    # Rule 2: Empty name fields\n",
					"    name_rules = validation_rules.get(\"name_validation\", {})\n",
					"    if name_rules.get(\"remove_empty_names\", True):\n",
					"        required_name_fields = name_rules.get(\"required_name_fields\", [\"fname\", \"lname\"])\n",
					"        \n",
					"        # Check any required name field is missing\n",
					"        empty_name_cond = None\n",
					"        for field in required_name_fields:\n",
					"            if field in df.columns:\n",
					"                if empty_name_cond is None:\n",
					"                    empty_name_cond = (col(field).isNull() | (trim(col(field)) == \"\"))\n",
					"                else:\n",
					"                    empty_name_cond = empty_name_cond | (col(field).isNull() | (trim(col(field)) == \"\"))\n",
					"        \n",
					"        # Apply the rule\n",
					"        if empty_name_cond is not None:\n",
					"            df = df.withColumn(\n",
					"                \"_remove_record\", \n",
					"                when(empty_name_cond, lit(True)).otherwise(col(\"_remove_record\"))\n",
					"            )\n",
					"            df = df.withColumn(\n",
					"                \"_removal_reason\", \n",
					"                when(empty_name_cond & (col(\"_removal_reason\").isNull()), \n",
					"                    lit(\"empty_name_fields\")).otherwise(col(\"_removal_reason\"))\n",
					"            )\n",
					"    \n",
					"    # Rule 3: Empty address with non-empty names\n",
					"    completeness_rules = validation_rules.get(\"completeness_validation\", {})\n",
					"    if completeness_rules.get(\"remove_empty_address_with_names\", True):\n",
					"        if all(field in df.columns for field in [\"fname\", \"lname\", \"address\"]):\n",
					"            empty_addr_cond = (\n",
					"                col(\"fname\").isNotNull() & (trim(col(\"fname\")) != \"\") &\n",
					"                col(\"lname\").isNotNull() & (trim(col(\"lname\")) != \"\") &\n",
					"                (col(\"address\").isNull() | (trim(col(\"address\")) == \"\"))\n",
					"            )\n",
					"            \n",
					"            # Apply the rule\n",
					"            df = df.withColumn(\n",
					"                \"_remove_record\", \n",
					"                when(empty_addr_cond, lit(True)).otherwise(col(\"_remove_record\"))\n",
					"            )\n",
					"            df = df.withColumn(\n",
					"                \"_removal_reason\", \n",
					"                when(empty_addr_cond & (col(\"_removal_reason\").isNull()), \n",
					"                    lit(\"empty_address\")).otherwise(col(\"_removal_reason\"))\n",
					"            )\n",
					"    \n",
					"    # Rule 4: Empty zip code\n",
					"    if completeness_rules.get(\"remove_empty_zip\", True) and \"zip\" in df.columns:\n",
					"        # Remove any record with empty zip\n",
					"        empty_zip_cond = (col(\"zip\").isNull() | (trim(col(\"zip\")) == \"\"))\n",
					"        \n",
					"        # Apply the rule\n",
					"        df = df.withColumn(\n",
					"            \"_remove_record\", \n",
					"            when(empty_zip_cond, lit(True)).otherwise(col(\"_remove_record\"))\n",
					"        )\n",
					"        df = df.withColumn(\n",
					"            \"_removal_reason\",\n",
					"            when(empty_zip_cond & (col(\"_removal_reason\").isNull()),\n",
					"            lit(\"empty_zip\")).otherwise(col(\"_removal_reason\"))\n",
					"        )\n",
					"    # Rule 5: Remove records with \"OTHER ELECTORS\" or variations in address\n",
					"    address_rules = validation_rules.get(\"address_validation\", {})\n",
					"    if address_rules.get(\"remove_other_electors\", True) and \"address\" in df.columns:\n",
					"         # Create condition to match \"OTHER ELECTORS\" in any case variation, with or without spaces\n",
					"         other_electors_pattern = \"(?i).*OTHER\\\\s*ELECTORS?.*\"  # Case-insensitive regex pattern\n",
					"         other_electors_cond = col(\"address\").rlike(other_electors_pattern)\n",
					"\n",
					"         # Apply the rule\n",
					"        df = df.withColumn(\n",
					"            \"_remove_record\", \n",
					"            when(other_electors_cond, lit(True)).otherwise(col(\"_remove_record\"))\n",
					"        )\n",
					"        df = df.withColumn(\n",
					"            \"_removal_reason\", \n",
					"            when(other_electors_cond & (col(\"_removal_reason\").isNull()), \n",
					"            lit(\"other_electors_address\")).otherwise(col(\"_removal_reason\"))\n",
					"        )        \n",
					"\n",
					"    \n",
					"    # Rule 6: Empty address4 after fixing\n",
					"    if address_rules.get(\"remove_empty_address4\", False) and \"address4\" in df.columns:\n",
					"        empty_addr4_cond = (col(\"address4\").isNull() | (trim(col(\"address4\")) == \"\"))\n",
					"        \n",
					"        # Apply the rule\n",
					"        df = df.withColumn(\n",
					"            \"_remove_record\", \n",
					"            when(empty_addr4_cond, lit(True)).otherwise(col(\"_remove_record\"))\n",
					"        )\n",
					"        df = df.withColumn(\n",
					"            \"_removal_reason\", \n",
					"            when(empty_addr4_cond & (col(\"_removal_reason\").isNull()), \n",
					"                lit(\"empty_address4\")).otherwise(col(\"_removal_reason\"))\n",
					"        )\n",
					"    \n",
					"    #--------------------------------------------------------------------------------\n",
					"    # PHASE 3: APPLY REMOVAL FLAG\n",
					"    #--------------------------------------------------------------------------------\n",
					"    \n",
					"    # Save summary of records to be removed\n",
					"    removal_summary = df.filter(col(\"_remove_record\")).groupBy(\"_removal_reason\").count().collect()\n",
					"    \n",
					"    # Calculate total removed\n",
					"    df_filtered = df.filter(~col(\"_remove_record\"))\n",
					"    removed_count = initial_count - df_filtered.count()\n",
					"    \n",
					"    # Log detailed removal reasons\n",
					"    reason_counts = {}\n",
					"    for row in removal_summary:\n",
					"        if row[\"_removal_reason\"]:\n",
					"            reason_counts[row[\"_removal_reason\"]] = row[\"count\"]\n",
					"    \n",
					"    log_message = [\n",
					"        f\"Validation removed {removed_count} of {initial_count} records ({(removed_count/initial_count)*100:.2f}%):\"\n",
					"    ]\n",
					"    \n",
					"    for reason, count in reason_counts.items():\n",
					"        log_message.append(f\"  - {reason}: {count} records\")\n",
					"    \n",
					"    logger.info(\"\\n\".join(log_message))\n",
					"    \n",
					"    # Drop temporary columns and return\n",
					"    return df_filtered.drop(\"_remove_record\", \"_removal_reason\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def transform_to_target_schema(df, column_mapping):\n",
					"    \"\"\"Transform the dataframe to match the target PostgreSQL schema\"\"\"\n",
					"    print(f\"Starting transformation with DataFrame containing {df.count()} rows\")\n",
					"    \n",
					"    try:\n",
					"        # Preserve hash_id if it exists\n",
					"        has_hash_id = \"hash_id\" in df.columns\n",
					"        hash_id_values = None\n",
					"        if has_hash_id:\n",
					"            # Save the hash_id values along with a key column for rejoining later\n",
					"            if \"poll_number\" in df.columns and df.filter(col(\"poll_number\").isNotNull()).count() > 0:\n",
					"                hash_id_values = df.select(\"poll_number\", \"hash_id\")\n",
					"            elif \"register_lett\" in df.columns and df.filter(col(\"register_lett\").isNotNull()).count() > 0:\n",
					"                hash_id_values = df.select(\"register_lett\", \"hash_id\")\n",
					"            else:\n",
					"                # Create a row ID for joining if no good key exists\n",
					"                df = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
					"                hash_id_values = df.select(\"row_id\", \"hash_id\")\n",
					"        \n",
					"        # First, handle special mappings that might not be in the standard mapping\n",
					"        # Specifically map LA_Code to rec_num if needed\n",
					"        if \"LA_Code\" in df.columns and \"rec_num\" not in df.columns:\n",
					"            print(\"Mapping LA_Code to rec_num\")\n",
					"            df = df.withColumnRenamed(\"LA_Code\", \"rec_num\")\n",
					"        \n",
					"        # Apply the standard process_dataframe function\n",
					"        transformed_df = process_dataframe(df)\n",
					"        \n",
					"        # NEW: Apply address field corrections\n",
					"        transformed_df = fix_address_fields(transformed_df)\n",
					"        \n",
					"        # Restore hash_id if it was present before\n",
					"        if has_hash_id and hash_id_values is not None:\n",
					"            if \"row_id\" in hash_id_values.columns:\n",
					"                # If we had to use row_id for joining\n",
					"                transformed_df = transformed_df.withColumn(\"row_id\", monotonically_increasing_id())\n",
					"                transformed_df = transformed_df.join(hash_id_values, on=\"row_id\").drop(\"row_id\")\n",
					"            elif \"poll_number\" in hash_id_values.columns and \"poll_number\" in transformed_df.columns:\n",
					"                transformed_df = transformed_df.join(hash_id_values, on=\"poll_number\", how=\"left\")\n",
					"            elif \"register_lett\" in hash_id_values.columns and \"register_lett\" in transformed_df.columns:\n",
					"                transformed_df = transformed_df.join(hash_id_values, on=\"register_lett\", how=\"left\")\n",
					"        \n",
					"        print(f\"Transformation complete. Result has {transformed_df.count()} rows and {len(transformed_df.columns)} columns\")\n",
					"        print(f\"Result columns: {transformed_df.columns}\")\n",
					"        \n",
					"        return transformed_df\n",
					"    except Exception as e:\n",
					"        print(f\"Error in transform_to_target_schema: {str(e)}\")\n",
					"        import traceback\n",
					"        print(traceback.format_exc())\n",
					"        \n",
					"        # Create a minimal valid DataFrame instead of returning None\n",
					"        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
					"        schema = StructType([\n",
					"            StructField(field[0], field[1], True) for field in TARGET_SCHEMA\n",
					"        ])\n",
					"        \n",
					"        # Return empty DataFrame with correct schema\n",
					"        return spark.createDataFrame([], schema)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def ensure_register_lett_exists(df):\n",
					"    \"\"\"Ensure register_lett column exists and is populated correctly\"\"\"\n",
					"    # Check if register_lett already exists\n",
					"    if \"register_lett\" in df.columns:\n",
					"        return df\n",
					"    \n",
					"    # If register_lett doesn't exist but poll_number does, create register_lett from poll_number\n",
					"    if \"poll_number\" in df.columns:\n",
					"        print(\"Creating register_lett from poll_number\")\n",
					"        df = df.withColumn(\"register_lett\", col(\"poll_number\"))\n",
					"        return df\n",
					"    \n",
					"    # If neither exists but Elector Number does, create both from Elector Number\n",
					"    if \"Elector Number\" in df.columns:\n",
					"        print(\"Creating register_lett from Elector Number\")\n",
					"        df = df.withColumn(\"register_lett\", col(\"Elector Number\"))\n",
					"        return df\n",
					"    \n",
					"    # If no suitable source column exists, create an empty register_lett\n",
					"    print(\"WARNING: No source column for register_lett found, creating empty column\")\n",
					"    df = df.withColumn(\"register_lett\", lit(None).cast(\"integer\"))\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Merge with Existing Data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def merge_with_existing_data(spark, new_data, output_path):\n",
					"    \"\"\"Merge new data with existing data using SQL-based approach with hash_id join\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    # Add debug logging\n",
					"    print(f\"New data has {new_data.count()} rows\")\n",
					"    \n",
					"    # Ensure register_lett exists in new_data\n",
					"    new_data = ensure_register_lett_exists(new_data)\n",
					"    \n",
					"    # Ensure hash_id exists\n",
					"    if \"hash_id\" not in new_data.columns:\n",
					"        new_data = generate_hash_id(new_data)\n",
					"    \n",
					"    # Check if existing data is available and has content\n",
					"    existing_data_empty = True\n",
					"    if mssparkutils.fs.exists(output_path):\n",
					"        try:\n",
					"            # Read existing data with schema from new_data\n",
					"            print(f\"Reading existing data from {output_path}\")\n",
					"            existing_schema = new_data.schema\n",
					"            \n",
					"            # Try to read with explicit schema inference\n",
					"            existing_data = spark.read.option(\"mergeSchema\", \"true\").schema(existing_schema).parquet(output_path)\n",
					"            existing_count = existing_data.count()\n",
					"            print(f\"Existing data has {existing_count} rows\")\n",
					"            \n",
					"            # If existing data is empty, treat as initial load\n",
					"            if existing_count == 0:\n",
					"                print(\"Existing data is empty - treating as initial load\")\n",
					"                existing_data_empty = True\n",
					"            else:\n",
					"                existing_data_empty = False\n",
					"                \n",
					"                # Ensure register_lett exists in existing_data\n",
					"                existing_data = ensure_register_lett_exists(existing_data)\n",
					"                \n",
					"                # Ensure hash_id exists in existing data\n",
					"                if \"hash_id\" not in existing_data.columns:\n",
					"                    existing_data = generate_hash_id(existing_data)\n",
					"        except Exception as e:\n",
					"            print(f\"Error reading existing data: {str(e)}\")\n",
					"            existing_data_empty = True\n",
					"    \n",
					"    # Initial load case - just return new data\n",
					"    if existing_data_empty:\n",
					"        print(\"Using new data only (initial load)\")\n",
					"        return new_data\n",
					"    \n",
					"    # Regular merge case - existing data has content\n",
					"    try:\n",
					"        # Register temporary views\n",
					"        new_data.createOrReplaceTempView(\"new_data\")\n",
					"        existing_data.createOrReplaceTempView(\"existing_data\")\n",
					"        \n",
					"        # Now refresh to avoid stale references\n",
					"        spark.sql(\"REFRESH TABLE existing_data\")\n",
					"        \n",
					"        # Use SQL approach with hash_id as the join key\n",
					"        merged_data = spark.sql(\"\"\"\n",
					"            -- Keep all new records that don't exist in the existing data\n",
					"            SELECT n.*\n",
					"            FROM new_data n\n",
					"            LEFT ANTI JOIN existing_data e\n",
					"            ON n.hash_id = e.hash_id\n",
					"            \n",
					"            UNION ALL\n",
					"            \n",
					"            -- Keep all existing records\n",
					"            SELECT \n",
					"                COALESCE(n.part_no, e.part_no) as part_no,\n",
					"                COALESCE(n.register_lett, e.register_lett) as register_lett,\n",
					"                COALESCE(n.poll_number, e.poll_number) as poll_number,\n",
					"                COALESCE(n.new_marker, e.new_marker) as new_marker,\n",
					"                SUBSTRING(COALESCE(n.title, e.title), 1, 10) as title,\n",
					"                COALESCE(n.lname, e.lname) as lname,\n",
					"                COALESCE(n.fname, e.fname) as fname,\n",
					"                COALESCE(n.dob, e.dob) as dob,\n",
					"                COALESCE(n.flags, e.flags) as flags,\n",
					"                COALESCE(n.address, e.address) as address,\n",
					"                COALESCE(n.address2, e.address2) as address2, \n",
					"                COALESCE(n.address3, e.address3) as address3,\n",
					"                COALESCE(n.address4, e.address4) as address4,\n",
					"                COALESCE(n.address5, e.address5) as address5,\n",
					"                COALESCE(n.address6, e.address6) as address6,\n",
					"                COALESCE(n.zip, e.zip) as zip,\n",
					"                COALESCE(n.date_selected1, e.date_selected1) as date_selected1,\n",
					"                COALESCE(n.date_selected2, e.date_selected2) as date_selected2,\n",
					"                COALESCE(n.date_selected3, e.date_selected3) as date_selected3,\n",
					"                COALESCE(n.rec_num, e.rec_num) as rec_num,\n",
					"                COALESCE(n.perm_disqual, e.perm_disqual) as perm_disqual,\n",
					"                COALESCE(n.source_id, e.source_id) as source_id,\n",
					"                COALESCE(n.postcode_start, e.postcode_start) as postcode_start,\n",
					"                e.hash_id as hash_id,\n",
					"                CASE \n",
					"                    WHEN n.creation_date > e.creation_date THEN n.creation_date\n",
					"                    ELSE e.creation_date\n",
					"                END as creation_date\n",
					"            FROM existing_data e\n",
					"            LEFT JOIN new_data n\n",
					"            ON e.hash_id = n.hash_id\n",
					"        \"\"\")\n",
					"        \n",
					"        # Count rows after merge\n",
					"        merged_count = merged_data.count()\n",
					"        print(f\"Merged data has {merged_count} rows\")\n",
					"        \n",
					"        # Ensure register_lett is synchronized with poll_number in merged result\n",
					"        merged_data = ensure_register_lett_exists(merged_data)\n",
					"        \n",
					"        return merged_data\n",
					"        \n",
					"    except Exception as e:\n",
					"        logging.error(f\"Error merging with existing data: {str(e)}\")\n",
					"        logging.warning(\"Returning only new data due to merge error\")\n",
					"        import traceback\n",
					"        logging.error(traceback.format_exc())\n",
					"        return new_data"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Output and Update Process Log"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def write_output_safely(df, output_path, description=\"data\"):\n",
					"    \"\"\"Write DataFrame to storage with proper error handling\"\"\"\n",
					"    logger = logging.getLogger()\n",
					"    \n",
					"    try:\n",
					"        # Ensure output directory exists\n",
					"        from notebookutils import mssparkutils\n",
					"        dir_path = \"/\".join(output_path.split(\"/\")[:-1])\n",
					"        \n",
					"        if not mssparkutils.fs.exists(dir_path):\n",
					"            logger.info(f\"Creating output directory: {dir_path}\")\n",
					"            mssparkutils.fs.mkdirs(dir_path)\n",
					"        \n",
					"        # Write data to a temporary path first\n",
					"        temp_path = f\"{dir_path}/temp_{int(time.time())}\"\n",
					"        df.write.mode(\"overwrite\").parquet(temp_path)\n",
					"        \n",
					"        # Move the temporary path to the final output path\n",
					"        if mssparkutils.fs.exists(temp_path):\n",
					"            if mssparkutils.fs.exists(output_path):\n",
					"                mssparkutils.fs.rm(output_path, True)\n",
					"            mssparkutils.fs.mv(temp_path, output_path)\n",
					"        \n",
					"        logger.info(f\"Successfully wrote {df.count()} rows of {description} to {output_path}\")\n",
					"        return True\n",
					"    \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error writing {description} to {output_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return False"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"def write_output_and_update_log(spark, merged_data, output_path, new_file_paths, process_log_path):\n",
					"    \"\"\"Write the merged data to output location and update the process log\"\"\"\n",
					"    logger = logging.getLogger()\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    try:\n",
					"        # Ensure output directory exists\n",
					"        dir_path = output_path\n",
					"        if '.' in output_path.split('/')[-1]:  # Check if last part looks like a filename\n",
					"            dir_path = '/'.join(output_path.split('/')[:-1])\n",
					"            \n",
					"        if not mssparkutils.fs.exists(dir_path):\n",
					"            logger.info(f\"Creating output directory: {dir_path}\")\n",
					"            mssparkutils.fs.mkdirs(dir_path)\n",
					"        \n",
					"        # Write the merged data\n",
					"        logger.info(f\"Writing {merged_data.count()} records to {output_path}\")\n",
					"        merged_data.write.mode(\"overwrite\").parquet(output_path)\n",
					"        logger.info(\"Successfully wrote data to output location\")\n",
					"        \n",
					"        # Rest of function remains the same...\n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error writing output or updating log: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return False"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def update_enhanced_process_log(spark, file_path, status, details=None):\n",
					"    \"\"\"Enhanced process log with more details about processing status\"\"\"\n",
					"    from datetime import datetime\n",
					"    from pyspark.sql import SparkSession\n",
					"    from pyspark.sql.types import StructType, StructField, StringType\n",
					"    from notebookutils import mssparkutils\n",
					"    import os\n",
					"    \n",
					"    log_path = f\"abfss://juror-etl@baubaisadfsaprod.dfs.core.windows.net/l2_process_logs/processed_files_log\"\n",
					"\n",
					"    # Ensure directory exists\n",
					"    log_dir = os.path.dirname(log_path)\n",
					"    if not mssparkutils.fs.exists(log_dir):\n",
					"        mssparkutils.fs.mkdirs(log_dir)\n",
					"\n",
					"    # Prepare new log entry\n",
					"    timestamp = datetime.now().isoformat()\n",
					"    record_count = details.get(\"record_count\", 0) if details else 0\n",
					"    new_records = details.get(\"new_records\", 0) if details else 0\n",
					"    new_log_line = f\"{file_path}|{status}|{timestamp}|{record_count}|{new_records}\"\n",
					"\n",
					"    # Read existing log entries if exists\n",
					"    try:\n",
					"        if mssparkutils.fs.exists(log_path):\n",
					"            existing_log_df = spark.read.text(log_path)\n",
					"            existing_log = [row.value for row in existing_log_df.collect()]\n",
					"        else:\n",
					"            existing_log = []\n",
					"    except Exception as e:\n",
					"        print(f\"Warning: Could not read existing log: {e}\")\n",
					"        existing_log = []\n",
					"\n",
					"    # Append new line\n",
					"    updated_log = existing_log + [new_log_line]\n",
					"\n",
					"    # Write updated log back (overwrite as a text file)\n",
					"    schema = StructType([StructField(\"value\", StringType(), True)])\n",
					"    updated_log_df = spark.createDataFrame([(line,) for line in updated_log], schema=schema)\n",
					"\n",
					"    updated_log_df.coalesce(1).write.mode(\"overwrite\").text(log_path)\n",
					"\n",
					"    return True\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def prepare_postgresql_data(df):\n",
					"    \"\"\"\n",
					"    Prepare data specifically for PostgreSQL loading\n",
					"    - Selects only required columns\n",
					"    - Applies flags field transformation (X overrules F)\n",
					"    - Ensures proper data types\n",
					"    - Replaces empty values with NULL\n",
					"    - Now also handles NAN values in problematic columns\n",
					"    \"\"\"\n",
					"    from pyspark.sql.functions import col, when, lit, trim, lower\n",
					"    \n",
					"    logger = logging.getLogger()\n",
					"    logger.info(f\"Preparing PostgreSQL data from {df.count()} records\")\n",
					"    \n",
					"    try:\n",
					"        # Ensure poll_number and register_lett are synchronized\n",
					"        df = ensure_register_poll_consistency(df)\n",
					"        \n",
					"        # NEW: Apply address corrections one more time before final export\n",
					"        df = fix_address_fields(df)\n",
					"        # Then apply comprehensive validation with config\n",
					"        df = comprehensive_address_validation(df, config_dict)\n",
					"        \n",
					"        # 1. Clean timestamp fields first to replace NaT with NULL\n",
					"        timestamp_columns = [\"date_selected1\", \"date_selected2\", \"date_selected3\"]\n",
					"        for col_name in timestamp_columns:\n",
					"            if col_name in df.columns:\n",
					"                # Replace \"NaT\", \"nat\", \"NAN\", \"nan\" with NULL in timestamp columns\n",
					"                df = df.withColumn(\n",
					"                    col_name,\n",
					"                    when(\n",
					"                        trim(lower(col(col_name))).isin(\"\", \"nat\", \"nan\", \"none\"),\n",
					"                        lit(None)\n",
					"                    ).otherwise(col(col_name))\n",
					"                )\n",
					"                logger.info(f\"Cleaned timestamp column: {col_name}\")\n",
					"        \n",
					"        # 2. Select only columns needed for PostgreSQL\n",
					"        pg_columns = [\"part_no\", \"register_lett\", \"poll_number\", \"new_marker\", \n",
					"                     \"title\", \"lname\", \"fname\", \"dob\", \"flags\", \"address\", \n",
					"                     \"address2\", \"address3\", \"address4\", \"address5\", \"address6\", \n",
					"                     \"zip\", \"date_selected1\", \"date_selected2\", \"date_selected3\", \n",
					"                     \"rec_num\", \"perm_disqual\", \"source_id\", \"postcode_start\", \"hash_id\", \"creation_date\"]\n",
					"        \n",
					"        pg_data = df.select([col(c) for c in pg_columns if c in df.columns])\n",
					"        \n",
					"        # 3. Process flags column for X/F rule\n",
					"        pg_data = pg_data.withColumn(\n",
					"            \"flags\",\n",
					"            when(\n",
					"                (col(\"flags\").isNull()) | \n",
					"                (col(\"flags\").cast(\"string\") == \"\") | \n",
					"                (col(\"flags\").cast(\"string\").isin(\"nan\", \"NaN\", \"None\")),\n",
					"                lit(None)\n",
					"            ).when(\n",
					"                col(\"flags\").cast(\"string\").rlike(\"(?i)X\"),  # contains X (case-insensitive)\n",
					"                \"X\"\n",
					"            ).when(\n",
					"                col(\"flags\").cast(\"string\").rlike(\"(?i)F\"),  # contains F but not X \n",
					"                \"F\"\n",
					"            ).otherwise(lit(None))\n",
					"        )\n",
					"        \n",
					"        # 4. NEW: Special handling for problematic columns with NAN values\n",
					"        problematic_columns = [\"new_marker\", \"address5\"]\n",
					"        for column in problematic_columns:\n",
					"            if column in pg_data.columns:\n",
					"                # More robust handling using lower() for case-insensitive comparison\n",
					"                pg_data = pg_data.withColumn(\n",
					"                    column,\n",
					"                    when(\n",
					"                        (col(column).isNull()) | \n",
					"                        (lower(col(column)).isin(\"nan\", \"null\")) |\n",
					"                        (trim(col(column)) == \"\"),\n",
					"                        lit(None)\n",
					"                    ).otherwise(col(column))\n",
					"                )\n",
					"                \n",
					"                # Verify the cleaning worked\n",
					"                remaining_nan = pg_data.filter(\n",
					"                    (col(column).isNotNull()) & \n",
					"                    (lower(col(column)).isin(\"nan\", \"null\"))\n",
					"                ).count()\n",
					"                \n",
					"                if remaining_nan > 0:\n",
					"                    logger.warning(f\"Still found {remaining_nan} records with NAN/NULL strings in {column}\")\n",
					"                else:\n",
					"                    logger.info(f\"Successfully cleaned {column}\")\n",
					"        \n",
					"        # 5. Replace empty values with NULL for all columns\n",
					"        for column in pg_data.columns:\n",
					"            pg_data = pg_data.withColumn(\n",
					"                column,\n",
					"                when(\n",
					"                    (col(column).isNull()) | \n",
					"                    (col(column).cast(\"string\") == \"\") | \n",
					"                    (lower(col(column)).isin(\"nan\", \"null\")),\n",
					"                    lit(None)\n",
					"                ).otherwise(col(column))\n",
					"            )\n",
					"        \n",
					"        # 6. Apply trim to all string columns to ensure no padding issues\n",
					"        string_columns = [\"part_no\", \"new_marker\", \"title\", \"lname\", \"fname\", \n",
					"                          \"flags\", \"address\", \"address2\", \"address3\", \"address4\", \n",
					"                          \"address5\", \"address6\", \"zip\", \"perm_disqual\", \n",
					"                          \"source_id\", \"postcode_start\", \"hash_id\", \"creation_date\"]\n",
					"        \n",
					"        for col_name in string_columns:\n",
					"            if col_name in pg_data.columns:\n",
					"                pg_data = pg_data.withColumn(col_name, trim(col(col_name)))\n",
					"        \n",
					"        # 7. Ensure proper data types for numeric fields\n",
					"        pg_data = pg_data.withColumn(\"register_lett\", col(\"register_lett\").cast(\"integer\"))\n",
					"        pg_data = pg_data.withColumn(\"poll_number\", col(\"poll_number\").cast(\"integer\"))\n",
					"        pg_data = pg_data.withColumn(\"rec_num\", col(\"rec_num\").cast(\"integer\"))\n",
					"        \n",
					"        # 8. Ensure timestamp columns are properly formatted and handle NaT\n",
					"        for date_col in timestamp_columns:\n",
					"            if date_col in pg_data.columns:\n",
					"                pg_data = pg_data.withColumn(date_col, col(date_col).cast(\"timestamp\"))\n",
					"        \n",
					"        logger.info(f\"PostgreSQL data preparation complete: {pg_data.count()} records\")\n",
					"        return pg_data\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error preparing PostgreSQL data: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        # Return original dataframe if transformation fails\n",
					"        return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Main Pipeline Function"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def main():\n",
					"    \"\"\"Main function with enhanced processing logic\"\"\"\n",
					"    # Initialize\n",
					"    print(\"Starting electoral data ETL process\")\n",
					"    print(f\"Current time: {datetime.now()}\")\n",
					"\n",
					"    # Configure logging and capture the logger\n",
					"    logger = configure_logging()\n",
					"    logger.info(\"Logging configured successfully\")\n",
					"\n",
					"    spark = create_spark_session()\n",
					"    config = load_config()\n",
					"    storage_account = config[\"storage_account\"]\n",
					"    process_log_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/l2_process_logs/processed_files_log.txt\"\n",
					"    \n",
					"    # Ensure all required directories exist\n",
					"    required_paths = [\n",
					"        config['input_path'],\n",
					"        config['output_path'],  # Now points to voters_transformed\n",
					"        \"/\".join(process_log_path.split(\"/\")[:-1]),\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_postgresql\"\n",
					"    ]\n",
					"    \n",
					"    for path in required_paths:\n",
					"        ensure_directory_exists(path)\n",
					"    \n",
					"    try:\n",
					"        # Get configuration and mapping\n",
					"        column_mapping = read_column_mapping(\n",
					"            spark, f\"{config['config_path']}/col_mapping_schema.json\"\n",
					"        )\n",
					"        \n",
					"        if not column_mapping:\n",
					"            logger.error(\"Failed to read column mapping. Aborting process.\")\n",
					"            return\n",
					"\n",
					"        # Identify new files to process\n",
					"        new_file_paths, all_file_paths = get_new_files(\n",
					"            spark, config[\"input_path\"], process_log_path\n",
					"        )\n",
					"\n",
					"        if not new_file_paths:\n",
					"            logger.info(\"No new files to process\")\n",
					"            return\n",
					"\n",
					"        logger.info(f\"Found {len(new_file_paths)} new files to process\")\n",
					"\n",
					"        # Process each new file with enhanced tracking\n",
					"        total_new_records = 0\n",
					"        successful_files = 0\n",
					"        failed_files = 0\n",
					"        no_contribution_files = 0\n",
					"        \n",
					"        for file_path in new_file_paths:\n",
					"            logger.info(f\"Processing file: {file_path}\")\n",
					"            success, new_records = process_input_file_with_tracking(spark, file_path, column_mapping, config)\n",
					"            \n",
					"            if success:\n",
					"                successful_files += 1\n",
					"                total_new_records += new_records\n",
					"                if new_records == 0:\n",
					"                    no_contribution_files += 1\n",
					"            else:\n",
					"                failed_files += 1\n",
					"        \n",
					"        logger.info(f\"Processing summary: {successful_files} successful, {failed_files} failed\")\n",
					"        logger.info(f\"Added {total_new_records} new records to voters_transformed\")\n",
					"        logger.info(f\"{no_contribution_files} files processed but added no new records\")\n",
					"        from pyspark.sql.utils import AnalysisException\n",
					"\n",
					"        # Now prepare the PostgreSQL-specific data from the entire transformed dataset\n",
					"        output_path = config[\"output_path\"]\n",
					"        try:\n",
					"            transformed_data = spark.read.format(\"delta\").load(output_path)\n",
					"        except AnalysisException as e:\n",
					"            logger.warning(f\"Delta table not available yet at {output_path} (likely first load or no successful writes).\")\n",
					"            logger.warning(\"Skipping PostgreSQL snapshot generation.\")\n",
					"            return\n",
					"        # Check if we have new files to process\n",
					"        if len(new_file_paths) > 0:\n",
					"            processed_dates = set()\n",
					"            for file_path in new_file_paths:\n",
					"                # Extract date from filename (e.g., 20231206_146-Northamptonshire.parquet)\n",
					"                date_match = re.search(r'(\\d{8})_', os.path.basename(file_path))\n",
					"                if date_match:\n",
					"                    processed_dates.add(date_match.group(1))\n",
					"            logger.info(f\"Processing data for dates: {sorted(list(processed_dates))}\")\n",
					"            # Filter transformed data to only include records from the dates we're processing\n",
					"            if processed_dates and \"creation_date\" in transformed_data.columns:\n",
					"                filter_conditions = []\n",
					"                for date_str in processed_dates:\n",
					"                    filter_conditions.append(col(\"creation_date\").startswith(date_str))\n",
					"                if filter_conditions:\n",
					"                    filter_expr = filter_conditions[0]\n",
					"                    for cond in filter_conditions[1:]:\n",
					"                        filter_expr = filter_expr | cond\n",
					"                    \n",
					"                    #Get new records\n",
					"                    new_data = transformed_data.filter(filter_expr)\n",
					"                    new_count = new_data.count()\n",
					"\n",
					"                    if new_count > 0:\n",
					"                        # Apply PostgreSQL-specific transformations\n",
					"                        postgresql_data = prepare_postgresql_data(new_data)\n",
					"                        # Write to PostgreSQL output using append mode and partitioning\n",
					"                        postgresql_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_postgresql\"\n",
					"                     # Ensure creation_date is suitable for partitioning (no slashes, etc.)\n",
					"                        if \"creation_date\" in postgresql_data.columns: \n",
					"                            # Ensure creation_date format is consistent for partitioning\n",
					"                            postgresql_data = postgresql_data.withColumn(\n",
					"                                \"creation_date_partition\",\n",
					"                                regexp_extract(col(\"creation_date\"), \"^(\\\\d{8})\", 1)\n",
					"                                )\n",
					"                            # Write partitioned data in append mode\n",
					"                            postgresql_data.write \\\n",
					"                                .partitionBy(\"creation_date_partition\") \\\n",
					"                                .mode(\"append\") \\\n",
					"                                .parquet(postgresql_path)\n",
					"                            logger.info(f\"Successfully wrote PostgreSQL data partitioned by date\")\n",
					"                        else:\n",
					"                            # Fallback to non-partitioned write if creation_date not available\n",
					"                            logger.warning(\"creation_date column not found, writing without partitioning\")\n",
					"                            postgresql_data.write.mode(\"append\").parquet(postgresql_path)\n",
					"                    else:\n",
					"                        logger.info(f\"No data found for the processed dates\")\n",
					"                else:\n",
					"                    logger.warning(\"No date filters created, skipping PostgreSQL update\")\n",
					"            else:\n",
					"                logger.warning(\"No processed dates identified or creation_date column missing\")\n",
					"        else:\n",
					"            logger.info(\"No new files processed, skipping PostgreSQL update\")\n",
					"\n",
					"        # Optimize the Delta table if we processed files\n",
					"        if total_new_records > 0:\n",
					"            try:\n",
					"                from delta.tables import DeltaTable\n",
					"                logger.info(\"Optimizing Delta table...\")\n",
					"\n",
					"                delta_table = DeltaTable.forPath(spark, config[\"output_path\"])\n",
					"                # Optimize (compaction)\n",
					"                delta_table.optimize().executeCompaction()\n",
					"                # Z-order by frequently queried columns\n",
					"                delta_table.optimize().executeZOrderBy(\"hash_id\")\n",
					"                # Vacuum (remove old files, retain 7 days history)\n",
					"                delta_table.vacuum(168)  # 7 days in hours\n",
					"\n",
					"                logger.info(\"Delta table optimization complete\")\n",
					"            except Exception as e:\n",
					"                logger.warning(f\"Delta table optimization failed: {str(e)}\")\n",
					"\n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error in incremental load process: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        raise\n",
					"\n",
					"\n",
					"if __name__ == \"__main__\":\n",
					"    main()"
				],
				"execution_count": null
			}
		]
	}
}