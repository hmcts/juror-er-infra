{
	"name": "L2_Er_Juror_Mergingfiles",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkstg",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "ca5e75d1-390a-42e7-aa30-ad3b101246d8"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/74dacd4f-a248-45bb-a2f0-af700dc4cf68/resourceGroups/baubais-data-factory-rg-stg/providers/Microsoft.Synapse/workspaces/baubais-synapse-stg/bigDataPools/sparkstg",
				"name": "sparkstg",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-stg.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkstg",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Electoral Roll Data ETL Process\n",
					"\n",
					"This notebook processes electoral roll data from multiple sources, standardizes columns, removes duplicates, hashes sensitive information, and prepares data for PostgreSQL.\n",
					"\n",
					"### Function Sequence\n",
					"\n",
					"### Setup Functions\n",
					"\n",
					"`create_spark_session()`: Initializes Spark session with necessary configurations\n",
					"\n",
					"`load_config()`: Returns configuration including storage paths and database settings\n",
					"\n",
					"\n",
					"Configuration Processing\n",
					"\n",
					"read_column_mapping(spark, config_path): Reads schema mapping configuration\n",
					"standardize_columns(df, column_mapping): Standardizes column names based on mapping\n",
					"\n",
					"\n",
					"Incremental Data Processing\n",
					"\n",
					"get_new_files(spark, input_path, process_log_path): Identifies files that need processing\n",
					"process_input_files(spark, file_paths, column_mapping): Reads and standardizes files\n",
					"\n",
					"\n",
					"Data Transformation\n",
					"\n",
					"merge_dataframes(dfs): Combines standardized DataFrames\n",
					"remove_duplicates_by_extraction_date(df): Keeps only the latest record for each elector\n",
					"apply_hashing(df): Hashes sensitive fields for privacy\n",
					"transform_to_target_schema(df, column_mapping): Maps to PostgreSQL schema\n",
					"\n",
					"\n",
					"Output Processing\n",
					"\n",
					"merge_with_existing_data(spark, new_data, output_path): Updates existing data with new rec\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession, Window\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"import json\n",
					"import logging\n",
					"from datetime import datetime\n",
					"import os\n",
					"import re"
				],
				"execution_count": 82
			},
			{
				"cell_type": "code",
				"source": [
					"def configure_logging():\n",
					"    \"\"\"Set up proper logging with rotating file handler\"\"\"\n",
					"    log_format = '%(asctime)s [%(levelname)s] %(message)s'\n",
					"    logging.basicConfig(level=logging.INFO, format=log_format)\n",
					"    \n",
					"    # Add a handler that writes to a log file\n",
					"    file_handler = logging.FileHandler('electoral_etl.log')\n",
					"    file_handler.setFormatter(logging.Formatter(log_format))\n",
					"    \n",
					"    # Get the root logger and add the file handler\n",
					"    root_logger = logging.getLogger()\n",
					"    root_logger.addHandler(file_handler)\n",
					"    \n",
					"    return root_logger"
				],
				"execution_count": 83
			},
			{
				"cell_type": "code",
				"source": [
					"def create_spark_session():\n",
					"    \"\"\"Create and return a Spark session\"\"\"\n",
					"    return SparkSession.builder \\\n",
					"        .appName(\"Electoral Data ETL\") \\\n",
					"        .getOrCreate()"
				],
				"execution_count": 84
			},
			{
				"cell_type": "code",
				"source": [
					"def normalize_storage_path(path):\n",
					"    \"\"\"Normalize storage path to ensure consistency\"\"\"\n",
					"    # Remove any trailing slashes\n",
					"    path = path.rstrip('/')\n",
					"    \n",
					"    # Check if path is missing container info\n",
					"    if not path.startswith(\"abfss://\"):\n",
					"        # Add default storage and container\n",
					"        path = f\"abfss://juror-etl@baubaisadfsastg.dfs.core.windows.net/{path}\"\n",
					"    \n",
					"    return path"
				],
				"execution_count": 85
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import StringType, IntegerType, TimestampType\n",
					"\n",
					"# Define global schema\n",
					"TARGET_SCHEMA = [\n",
					"    (\"part_no\", StringType()),\n",
					"    (\"register_lett\", IntegerType()),\n",
					"    (\"poll_number\", IntegerType()),\n",
					"    (\"new_marker\", StringType()),\n",
					"    (\"title\", StringType()),\n",
					"    (\"lname\", StringType()),\n",
					"    (\"fname\", StringType()),\n",
					"    (\"dob\", StringType()),\n",
					"    (\"flags\", StringType()),\n",
					"    (\"address\", StringType()),\n",
					"    (\"address2\", StringType()),\n",
					"    (\"address3\", StringType()),\n",
					"    (\"address4\", StringType()),\n",
					"    (\"address5\", StringType()),\n",
					"    (\"address6\", StringType()),\n",
					"    (\"zip\", StringType()),\n",
					"    (\"date_selected1\", TimestampType()),\n",
					"    (\"date_selected2\", TimestampType()),\n",
					"    (\"date_selected3\", TimestampType()),\n",
					"    (\"rec_num\", IntegerType()),\n",
					"    (\"perm_disqual\", StringType()),\n",
					"    (\"source_id\", StringType()),\n",
					"    (\"postcode_start\", StringType()),\n",
					"    (\"hash_id\", StringType()),\n",
					"    (\"creation_date\", StringType())\n",
					"]"
				],
				"execution_count": 86
			},
			{
				"cell_type": "code",
				"source": [
					"def load_config():\n",
					"    \"\"\"Load configuration including storage paths\"\"\"\n",
					"    storage_account = \"baubaisadfsastg\"\n",
					"    config = {\n",
					"        \"storage_account\": storage_account,\n",
					"        \"input_path\": f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/transformation\",\n",
					"        \"config_path\": f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/config/schema\",\n",
					"        \"output_path\": f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_temp\"\n",
					"    }\n",
					"    \n",
					"    # Normalize all paths\n",
					"    config[\"output_path\"] = normalize_storage_path(config[\"output_path\"])\n",
					"    config[\"input_path\"] = normalize_storage_path(config[\"input_path\"])\n",
					"    config[\"config_path\"] = normalize_storage_path(config[\"config_path\"])\n",
					"    \n",
					"    return config"
				],
				"execution_count": 87
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Read Configuration and Mapping"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Column Mapping"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def read_column_mapping(spark, config_path):\n",
					"    \"\"\"Read and parse the column mapping configuration\"\"\"\n",
					"    try:\n",
					"        # Read the JSON file\n",
					"        mapping_df = spark.read.text(config_path)\n",
					"        \n",
					"        # Combine all lines into a single string\n",
					"        json_str = mapping_df.agg(concat_ws(\"\", collect_list(\"value\"))).collect()[0][0]\n",
					"        \n",
					"        # Parse the JSON string\n",
					"        mapping_dict = json.loads(json_str)\n",
					"        \n",
					"        # Check if it has 'schema' structure\n",
					"        if \"schema\" in mapping_dict:\n",
					"            # Convert schema format to mappings format\n",
					"            mappings = {}\n",
					"            for item in mapping_dict[\"schema\"]:\n",
					"                # Use the 'name' field as the standard column name\n",
					"                # Initially set empty variations list\n",
					"                mappings[item[\"name\"]] = []\n",
					"            \n",
					"            return mappings\n",
					"        \n",
					"        # If looking for mappings specifically\n",
					"        elif \"mappings\" in mapping_dict:\n",
					"            return mapping_dict[\"mappings\"]\n",
					"        \n",
					"        # If neither structure is found, raise an error\n",
					"        else:\n",
					"            raise KeyError(\"Neither 'schema' nor 'mappings' found in mapping file\")\n",
					"            \n",
					"    except Exception as e:\n",
					"        try:\n",
					"            # Fallback to using wholeTextFiles\n",
					"            mapping_rdd = spark.sparkContext.wholeTextFiles(config_path)\n",
					"            json_str = mapping_rdd.values().first()\n",
					"            mapping_dict = json.loads(json_str)\n",
					"            \n",
					"            # Check the structure here too\n",
					"            if \"schema\" in mapping_dict:\n",
					"                mappings = {}\n",
					"                for item in mapping_dict[\"schema\"]:\n",
					"                    mappings[item[\"name\"]] = []\n",
					"                return mappings\n",
					"            elif \"mappings\" in mapping_dict:\n",
					"                return mapping_dict[\"mappings\"]\n",
					"            else:\n",
					"                raise KeyError(\"Neither 'schema' nor 'mappings' found in mapping file\")\n",
					"                \n",
					"        except Exception as e2:\n",
					"            raise Exception(f\"Failed to read mapping file. Primary error: {str(e)}, Fallback error: {str(e2)}\")"
				],
				"execution_count": 88
			},
			{
				"cell_type": "code",
				"source": [
					"def standardize_columns(df, column_mapping):\n",
					"    \"\"\"\n",
					"    Standardize column names based on mapping\n",
					"    \n",
					"    Args:\n",
					"        df: Input DataFrame\n",
					"        column_mapping: Dictionary mapping standard names to variations\n",
					"        \n",
					"    Returns:\n",
					"        DataFrame with standardized column names\n",
					"    \"\"\"\n",
					"    # Get current columns\n",
					"    current_columns = df.columns\n",
					"    \n",
					"    # Create a reverse mapping (from variations to standard names)\n",
					"    reverse_mapping = {}\n",
					"    for standard_name, variations in column_mapping.items():\n",
					"        # Add the standard name itself to the reverse mapping\n",
					"        reverse_mapping[standard_name.lower()] = standard_name\n",
					"        \n",
					"        # Add all variations\n",
					"        for variation in variations:\n",
					"            reverse_mapping[variation.lower()] = standard_name\n",
					"    \n",
					"    # Apply renaming\n",
					"    for col in current_columns:\n",
					"        # Check if column matches any standard name or variation (case-insensitive)\n",
					"        std_name = reverse_mapping.get(col.lower())\n",
					"        if std_name and col != std_name:\n",
					"            df = df.withColumnRenamed(col, std_name)\n",
					"    \n",
					"    return df\n",
					"def standardize_columns(df, column_mapping):\n",
					"    \"\"\"\n",
					"    Standardize column names based on mapping\n",
					"    \n",
					"    Args:\n",
					"        df: Input DataFrame\n",
					"        column_mapping: Dictionary mapping standard names to variations\n",
					"        \n",
					"    Returns:\n",
					"        DataFrame with standardized column names\n",
					"    \"\"\"\n",
					"    # Get current columns\n",
					"    current_columns = df.columns\n",
					"    \n",
					"    # Create a reverse mapping (from variations to standard names)\n",
					"    reverse_mapping = {}\n",
					"    for standard_name, variations in column_mapping.items():\n",
					"        # Add the standard name itself to the reverse mapping\n",
					"        reverse_mapping[standard_name.lower()] = standard_name\n",
					"        \n",
					"        # Add all variations\n",
					"        for variation in variations:\n",
					"            reverse_mapping[variation.lower()] = standard_name\n",
					"    \n",
					"    # Apply renaming\n",
					"    for col in current_columns:\n",
					"        # Check if column matches any standard name or variation (case-insensitive)\n",
					"        std_name = reverse_mapping.get(col.lower())\n",
					"        if std_name and col != std_name:\n",
					"            df = df.withColumnRenamed(col, std_name)\n",
					"    \n",
					"    return df\n",
					"\n",
					""
				],
				"execution_count": 89
			},
			{
				"cell_type": "code",
				"source": [
					"def read_schema_config(spark, config_path):\n",
					"    \"\"\"Read and parse the schema configuration\"\"\"\n",
					"    try:\n",
					"        schema_df = spark.read.text(config_path)\n",
					"        json_str = schema_df.agg(concat_ws(\"\", collect_list(\"value\"))).collect()[0][0]\n",
					"        schema_dict = json.loads(json_str)\n",
					"        return schema_dict[\"schema\"]\n",
					"    except Exception as e:\n",
					"        try:\n",
					"            schema_rdd = spark.sparkContext.wholeTextFiles(config_path)\n",
					"            json_str = schema_rdd.values().first()\n",
					"            schema_dict = json.loads(json_str)\n",
					"            return schema_dict[\"schema\"]\n",
					"        except Exception as e2:\n",
					"            raise Exception(f\"Failed to read schema file. Primary error: {str(e)}, Fallback error: {str(e2)}\")"
				],
				"execution_count": 90
			},
			{
				"cell_type": "code",
				"source": [
					"def ensure_consistent_schema(df, required_columns):\n",
					"    \"\"\"Ensure DataFrame has all required columns with correct data types\"\"\"\n",
					"    current_columns = set(df.columns)\n",
					"    \n",
					"    # Add missing columns\n",
					"    for col_name, col_type in required_columns:\n",
					"        if col_name not in current_columns:\n",
					"            df = df.withColumn(col_name, lit(None).cast(col_type))\n",
					"    \n",
					"    # Select only the required columns in the specified order\n",
					"    return df.select([col(c[0]).cast(c[1]) for c in required_columns])"
				],
				"execution_count": 91
			},
			{
				"cell_type": "code",
				"source": [
					"def get_new_files(spark, input_path, process_log_path):\n",
					"    \"\"\"Identify which files are new and need processing\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    # Get all files in the input directory\n",
					"    all_files = []\n",
					"    for item in mssparkutils.fs.ls(input_path):\n",
					"        if item.path.endswith(\".parquet\"):\n",
					"            all_files.append(item.path)\n",
					"    \n",
					"    # Read process log to get previously processed files\n",
					"    if mssparkutils.fs.exists(process_log_path):\n",
					"        processed_files = spark.read.parquet(process_log_path) \\\n",
					"                              .select(\"file_path\").rdd.flatMap(lambda x: x).collect()\n",
					"    else:\n",
					"        processed_files = []\n",
					"    \n",
					"    # Identify new files\n",
					"    new_files = [f for f in all_files if f not in processed_files]\n",
					"    \n",
					"    return new_files, all_files"
				],
				"execution_count": 92
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def process_input_files(spark, file_paths, column_mapping):\n",
					"    \"\"\"Read and standardize columns from input files\"\"\"\n",
					"    processed_dfs = []\n",
					"    \n",
					"    for file_path in file_paths:\n",
					"        try:\n",
					"            # Read the file\n",
					"            df = spark.read.parquet(file_path)\n",
					"            \n",
					"            # Standardize column names based on mapping\n",
					"            standardized_df = standardize_columns(df, column_mapping)\n",
					"            \n",
					"            # Add source metadata\n",
					"            standardized_df = standardized_df.withColumn(\"source_file\", lit(file_path))\n",
					"            standardized_df = standardized_df.withColumn(\"process_timestamp\", current_timestamp())\n",
					"            \n",
					"            processed_dfs.append(standardized_df)\n",
					"            \n",
					"        except Exception as e:\n",
					"            print(f\"Error processing {file_path}: {str(e)}\")\n",
					"    \n",
					"    return processed_dfs"
				],
				"execution_count": 93
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Merge Files with Standardized Schema"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def merge_dataframes(dfs):\n",
					"    \"\"\"Merge all dataframes with standardized schema, preserving original date information\"\"\"\n",
					"    if not dfs:\n",
					"        raise ValueError(\"No dataframes to merge\")\n",
					"        \n",
					"    # Merge using unionByName to handle schema variations\n",
					"    merged_df = dfs[0]\n",
					"    for df in dfs[1:]:\n",
					"        merged_df = merged_df.unionByName(df, allowMissingColumns=True)\n",
					"    \n",
					"    # Check which date column exists in the merged data\n",
					"    date_column = None\n",
					"    if \"CreationDate\" in merged_df.columns:\n",
					"        date_column = \"CreationDate\"\n",
					"    elif \"creation_date\" in merged_df.columns:\n",
					"        date_column = \"creation_date\"\n",
					"    \n",
					"    # Only perform date check if we found a date column\n",
					"    if date_column:\n",
					"        # Check if date values are too uniform\n",
					"        distinct_dates = merged_df.select(date_column).distinct().count()\n",
					"        logging.info(f\"Found {distinct_dates} distinct dates in column {date_column}\")\n",
					"        \n",
					"        if distinct_dates == 1:\n",
					"            logging.warning(f\"Only one distinct date found in {date_column} - this may indicate a problem!\")\n",
					"            \n",
					"            # Try to extract better date information from source_file if available\n",
					"            if \"source_file\" in merged_df.columns:\n",
					"                merged_df = merged_df.withColumn(\n",
					"                    \"extracted_date\",\n",
					"                    regexp_extract(col(\"source_file\"), r\"(\\d{8})_\", 1)\n",
					"                )\n",
					"                \n",
					"                # Use extracted date if available, otherwise keep existing\n",
					"                merged_df = merged_df.withColumn(\n",
					"                    date_column,  # Update whichever date column exists\n",
					"                    when(col(\"extracted_date\").isNotNull(), col(\"extracted_date\"))\n",
					"                    .otherwise(col(date_column))\n",
					"                )\n",
					"                \n",
					"                # Clean up temporary column\n",
					"                merged_df = merged_df.drop(\"extracted_date\")\n",
					"                \n",
					"                logging.info(f\"After date correction: {merged_df.select(date_column).distinct().count()} distinct dates\")\n",
					"    else:\n",
					"        logging.warning(\"No date column found in merged data!\")\n",
					"    \n",
					"    return merged_df"
				],
				"execution_count": 94
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Deduplicate Based on Latest Data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def deduplicate_by_la_code_and_date(df):\n",
					"    \"\"\"Keep only the most recent record for each LA_Code\"\"\"\n",
					"    # Check if we have multiple dates for same LA_Code\n",
					"    if \"LA_Code\" in df.columns and \"CreationDate\" in df.columns:\n",
					"        # Get unique LA_Codes\n",
					"        la_codes = df.select(\"LA_Code\").distinct()\n",
					"        la_count = la_codes.count()\n",
					"        \n",
					"        # Check how many CreationDates per LA_Code\n",
					"        date_counts = df.groupBy(\"LA_Code\").agg(\n",
					"            countDistinct(\"CreationDate\").alias(\"date_count\"),\n",
					"            max(\"CreationDate\").alias(\"latest_date\")\n",
					"        )\n",
					"        \n",
					"        # Log the findings\n",
					"        multi_date_las = date_counts.filter(col(\"date_count\") > 1).count()\n",
					"        logging.info(f\"Found {la_count} unique LA_Codes\")\n",
					"        logging.info(f\"{multi_date_las} LA_Codes have multiple CreationDates\")\n",
					"        \n",
					"        if multi_date_las > 0:\n",
					"            # Keep only the most recent record for each LA_Code\n",
					"            window_spec = Window.partitionBy(\"LA_Code\").orderBy(desc(\"CreationDate\"))\n",
					"            deduplicated = df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                             .filter(col(\"row_num\") == 1) \\\n",
					"                             .drop(\"row_num\")\n",
					"            \n",
					"            logging.info(f\"Kept {deduplicated.count()} records after LA_Code deduplication\")\n",
					"            return deduplicated\n",
					"        else:\n",
					"            logging.info(\"No LA_Code has multiple dates, skipping deduplication\")\n",
					"            return df\n",
					"    else:\n",
					"        logging.warning(\"Missing LA_Code or CreationDate columns, skipping deduplication\")\n",
					"        return df"
				],
				"execution_count": 95
			},
			{
				"cell_type": "code",
				"source": [
					"def optimise_deduplicate_data(df):\n",
					"    \"\"\"Extremely conservative deduplication to preserve row count\"\"\"\n",
					"    # Get current columns\n",
					"    actual_columns = df.columns\n",
					"    \n",
					"    print(f\"PRE-DEDUPLICATION COUNT: {df.count()} rows\")\n",
					"    \n",
					"    # We need to use a much more specific combination of columns to identify duplicates\n",
					"    # Only remove exact duplicates (rows that are identical across all columns)\n",
					"    try:\n",
					"        # Either use a more comprehensive set of ID columns\n",
					"        id_columns = []\n",
					"        \n",
					"        # Use a different approach - focus on all the key identifying fields together\n",
					"        key_fields = [\n",
					"            \"Elector Number\", \"LA_Code\", \"poll_number\", \"register_lett\", \"rec_num\",\n",
					"            \"Elector Surname\", \"Elector Forename\", \"Address1\", \"PostCode\"\n",
					"        ]\n",
					"        \n",
					"        # Only use columns that actually exist\n",
					"        id_columns = [col for col in key_fields if col in actual_columns]\n",
					"        \n",
					"        # If we have less than 3 identifying columns, use all non-metadata columns\n",
					"        if len(id_columns) < 3:\n",
					"            # Exclude metadata columns like timestamps, source_file, etc.\n",
					"            metadata_cols = [\"source_file\", \"process_timestamp\", \"CreationDate\", \"creation_date\"]\n",
					"            id_columns = [col for col in actual_columns if col not in metadata_cols]\n",
					"        \n",
					"        print(f\"Using these columns for deduplication: {id_columns}\")\n",
					"        \n",
					"        if len(id_columns) == 0:\n",
					"            print(\"No suitable columns for deduplication found - returning all data\")\n",
					"            return df\n",
					"            \n",
					"        # Use dropDuplicates instead of window functions for exact matching\n",
					"        deduplicated = df.dropDuplicates(id_columns)\n",
					"        \n",
					"        final_count = deduplicated.count()\n",
					"        print(f\"POST-DEDUPLICATION COUNT: {final_count} rows\")\n",
					"        print(f\"ROWS REMOVED: {df.count() - final_count}\")\n",
					"        \n",
					"        return deduplicated\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"ERROR IN DEDUPLICATION: {str(e)}\")\n",
					"        import traceback\n",
					"        traceback.print_exc()\n",
					"        print(\"Returning original DataFrame\")\n",
					"        return df"
				],
				"execution_count": 96
			},
			{
				"cell_type": "code",
				"source": [
					"def improved_deduplicate_data(df):\n",
					"    \"\"\"\n",
					"    Improved deduplication using window functions to keep most recent record\n",
					"    per set of identifying columns\n",
					"    \"\"\"\n",
					"    # Define identifying columns\n",
					"    id_columns = [\n",
					"        \"fname\", \"lname\", \"address\", \"poll_number\", \"rec_num\", \"hash_id\"\n",
					"    ]\n",
					"    \n",
					"    # Filter list to only include columns that exist in the dataframe\n",
					"    existing_id_columns = [col for col in id_columns if col in df.columns]\n",
					"    \n",
					"    if len(existing_id_columns) == 0:\n",
					"        print(\"No suitable columns for deduplication found - returning all data\")\n",
					"        return df\n",
					"    \n",
					"    print(f\"Deduplicating using columns: {existing_id_columns}\")\n",
					"    print(f\"Input record count: {df.count()}\")\n",
					"    \n",
					"    # Determine which date column to use for ordering\n",
					"    date_column = None\n",
					"    for possible_date_col in [\"creation_date\", \"CreationDate\", \"process_timestamp\"]:\n",
					"        if possible_date_col in df.columns:\n",
					"            date_column = possible_date_col\n",
					"            break\n",
					"    \n",
					"    if date_column:\n",
					"        print(f\"Using {date_column} for ordering records\")\n",
					"        # Create window spec partitioned by identifying columns with date ordering\n",
					"        window_spec = Window.partitionBy([col(x) for x in existing_id_columns]) \\\n",
					"                           .orderBy(col(date_column).desc())\n",
					"    else:\n",
					"        print(\"No date column found for ordering, using default ordering\")\n",
					"        # If no date column, just use window without ordering\n",
					"        window_spec = Window.partitionBy([col(x) for x in existing_id_columns])\n",
					"    \n",
					"    # Apply deduplication\n",
					"    deduplicated_df = df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                        .filter(col(\"row_num\") == 1) \\\n",
					"                        .drop(\"row_num\")\n",
					"    \n",
					"    output_count = deduplicated_df.count()\n",
					"    print(f\"Output record count: {output_count}\")\n",
					"    print(f\"Removed {df.count() - output_count} duplicate records\")\n",
					"    \n",
					"    return deduplicated_df"
				],
				"execution_count": 97
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Apply Hashing for Sensitive Data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def apply_hashing(df):\n",
					"    \"\"\"Apply improved hashing to sensitive fields to create a unique identifier\"\"\"\n",
					"    from pyspark.sql.functions import col, xxhash64, lit, concat_ws, coalesce\n",
					"    \n",
					"    # Create the hash_id from original values with more unique columns\n",
					"    hashed_df = df.withColumn(\n",
					"        \"hash_id\",\n",
					"        xxhash64(concat_ws(\",\",\n",
					"            coalesce(col(\"Elector Forename\"), lit(\"\")),\n",
					"            coalesce(col(\"Elector Surname\"), lit(\"\")),\n",
					"            coalesce(col(\"Elector DOB\"), lit(\"\")),\n",
					"            coalesce(col(\"Address1\"), lit(\"\")),\n",
					"            coalesce(col(\"Address2\"), lit(\"\")), \n",
					"            coalesce(col(\"Address3\"), lit(\"\")),\n",
					"            coalesce(col(\"PostCode\"), lit(\"\")),\n",
					"            coalesce(col(\"Elector Number\"), lit(\"\")),\n",
					"            coalesce(col(\"LA_Code\"), lit(\"\"))\n",
					"            # Excluding CreationDate to maintain consistency\n",
					"        ))\n",
					"    )\n",
					"    \n",
					"    # Keep the rest of the function as is\n",
					"    hashed_df = hashed_df.withColumn(\"original_forename\", col(\"Elector Forename\"))\n",
					"    hashed_df = hashed_df.withColumn(\"original_surname\", col(\"Elector Surname\"))\n",
					"    hashed_df = hashed_df.withColumn(\"original_dob\", col(\"Elector DOB\"))\n",
					"    hashed_df = hashed_df.withColumn(\"original_address1\", col(\"Address1\"))\n",
					"    \n",
					"    return hashed_df"
				],
				"execution_count": 98
			},
			{
				"cell_type": "code",
				"source": [
					"def apply_hashing_to_voters(df):\n",
					"    \"\"\"Apply improved hashing specifically for voters_temp table column structure\"\"\"\n",
					"    from pyspark.sql.functions import col, xxhash64, lit, concat_ws, coalesce, expr, substring, hex, when, isnan\n",
					"\n",
					"    # Create hash_id from original values with unique columns\n",
					"    # First check if required columns exist\n",
					"    required_cols = [\"fname\", \"lname\", \"poll_number\", \"address\", \"zip\"]\n",
					"    existing_cols = [c for c in required_cols if c in df.columns]\n",
					"    \n",
					"    if existing_cols:\n",
					"        try:\n",
					"            # Create hash ID with null-safe concatenation\n",
					"            hashed_df = df.withColumn(\n",
					"                \"hash_id\",\n",
					"                xxhash64(concat_ws(\",\",\n",
					"                    *[coalesce(col(c), lit(\"\")) for c in existing_cols]\n",
					"                ))\n",
					"            )\n",
					"        except Exception as e:\n",
					"            print(f\"Error creating hash_id: {str(e)}\")\n",
					"            # Fallback to a simpler hash method\n",
					"            hashed_df = df.withColumn(\"hash_id\", \n",
					"                                     xxhash64(concat_ws(\",\", lit(\"fixed_seed\"), lit(current_timestamp()))))\n",
					"    else:\n",
					"        # If no columns exist, create a random hash\n",
					"        print(\"No identifying columns found for hash_id, using random value\")\n",
					"        hashed_df = df.withColumn(\"hash_id\", xxhash64(lit(current_timestamp())))\n",
					"    \n",
					"    # Store original values for reference (only if columns exist)\n",
					"    for original_col in [\"fname\", \"lname\", \"dob\", \"address\"]:\n",
					"        if original_col in df.columns:\n",
					"            hashed_df = hashed_df.withColumn(f\"original_{original_col}\", col(original_col))\n",
					"    \n",
					"    # For DOB, handle nulls explicitly\n",
					"    if \"dob\" in df.columns:\n",
					"        try:\n",
					"            # Keep null values as null, only hash non-null values\n",
					"            # Use when() to handle the condition\n",
					"            hashed_df = hashed_df.withColumn(\"dob\", \n",
					"                when(\n",
					"                    col(\"dob\").isNull() | (col(\"dob\") == \"\"),\n",
					"                    None\n",
					"                ).otherwise(\n",
					"                    substring(hex(xxhash64(col(\"dob\"))), 1, 10)\n",
					"                )\n",
					"            )\n",
					"        except Exception as e:\n",
					"            print(f\"Error hashing DOB with special handling: {str(e)}\")\n",
					"            # Try an even simpler approach if the above fails\n",
					"            try:\n",
					"                hashed_df = hashed_df.withColumn(\"dob\", \n",
					"                    when(col(\"dob\").isNotNull() & (length(trim(col(\"dob\"))) > 0),\n",
					"                         hex(xxhash64(col(\"dob\")))\n",
					"                    ).otherwise(None)\n",
					"                )\n",
					"            except Exception as e2:\n",
					"                print(f\"Fallback DOB hashing also failed: {str(e2)}\")\n",
					"                # Keep original if all hashing attempts fail\n",
					"                pass\n",
					"    \n",
					"    # For text columns, similar null-aware handling\n",
					"    text_cols = [\"fname\", \"lname\", \"address\", \"address2\", \"address3\"]\n",
					"    for text_col in text_cols:\n",
					"        if text_col in df.columns:\n",
					"            try:\n",
					"                max_length = 35 if text_col.startswith(\"address\") else 20\n",
					"                hashed_df = hashed_df.withColumn(text_col, \n",
					"                    when(\n",
					"                        col(text_col).isNull() | (col(text_col) == \"\"),\n",
					"                        None\n",
					"                    ).otherwise(\n",
					"                        substring(hex(xxhash64(col(text_col))), 1, max_length)\n",
					"                    )\n",
					"                )\n",
					"            except Exception as e:\n",
					"                print(f\"Error hashing {text_col}: {str(e)}\")\n",
					"                # Keep original if hashing fails\n",
					"                pass\n",
					"    \n",
					"    return hashed_df"
				],
				"execution_count": 99
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Transform to Target Schema"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def transform_to_target_schema(df, column_mapping):\n",
					"    \"\"\"\n",
					"    Transform the dataframe to match the target PostgreSQL schema\n",
					"    \"\"\"\n",
					"    print(f\"Starting transformation with DataFrame containing {df.count()} rows\")\n",
					"    print(f\"Available columns: {df.columns}\")\n",
					"    \n",
					"    try:\n",
					"        # Check if hash_id column exists, create it if missing\n",
					"        if \"hash_id\" not in df.columns:\n",
					"            print(\"Creating hash_id column since it doesn't exist\")\n",
					"            df = df.withColumn(\n",
					"                \"hash_id\",\n",
					"                xxhash64(concat_ws(\",\",\n",
					"                    coalesce(col(\"Elector Forename\"), lit(\"\")),\n",
					"                    coalesce(col(\"Elector Surname\"), lit(\"\")),\n",
					"                    coalesce(col(\"Elector DOB\"), lit(\"\")),\n",
					"                    coalesce(col(\"Address1\"), lit(\"\"))\n",
					"                ))\n",
					"            )\n",
					"        \n",
					"        # Helper function to safely access columns\n",
					"        def safe_col(column_name):\n",
					"            if column_name in df.columns:\n",
					"                return col(column_name)\n",
					"            else:\n",
					"                print(f\"Warning: Column '{column_name}' not found in dataframe\")\n",
					"                return lit(None)\n",
					"        \n",
					"        # Create the target schema DataFrame\n",
					"        transformed_df = df.select(\n",
					"            # part_no - blank for now, will be generated by PostgreSQL\n",
					"            lit(None).cast(\"string\").alias(\"part_no\"),\n",
					"            \n",
					"            # register_lett - use the Elector Number directly\n",
					"            when(\n",
					"                col(\"Elector Number\").isNotNull(),\n",
					"                col(\"Elector Number\").cast(\"int\")\n",
					"            ).otherwise(lit(None)).alias(\"register_lett\"),\n",
					"            \n",
					"            # poll_number - also from Elector Number (same as register_lett)\n",
					"            when(\n",
					"                col(\"Elector Number\").isNotNull(),\n",
					"                col(\"Elector Number\").cast(\"int\")\n",
					"            ).otherwise(lit(None)).alias(\"poll_number\"),\n",
					"            \n",
					"            # Rest of the columns remain the same\n",
					"            safe_col(\"Elector Markers\").alias(\"new_marker\"),\n",
					"            safe_col(\"Elector Title\").alias(\"title\"),\n",
					"            safe_col(\"Elector Surname\").alias(\"lname\"),\n",
					"            safe_col(\"Elector Forename\").alias(\"fname\"),\n",
					"            safe_col(\"Elector DOB\").alias(\"dob\"),\n",
					"            when(\n",
					"                col(\"Flags/Markers\").isNotNull(), \n",
					"                col(\"Flags/Markers\")\n",
					"            ).otherwise(lit(None)).alias(\"flags\"),\n",
					"            safe_col(\"Address1\").alias(\"address\"),\n",
					"            safe_col(\"Address2\").alias(\"address2\"),\n",
					"            safe_col(\"Address3\").alias(\"address3\"),\n",
					"            safe_col(\"Address4\").alias(\"address4\"),\n",
					"            safe_col(\"Address5\").alias(\"address5\"),\n",
					"            safe_col(\"Address6\").alias(\"address6\"),\n",
					"            safe_col(\"PostCode\").alias(\"zip\"),\n",
					"            lit(None).cast(\"timestamp\").alias(\"date_selected1\"),\n",
					"            lit(None).cast(\"timestamp\").alias(\"date_selected2\"),\n",
					"            lit(None).cast(\"timestamp\").alias(\"date_selected3\"),\n",
					"            \n",
					"            # rec_num - also set to Elector Number for consistency\n",
					"            when(\n",
					"                col(\"LA_Code\").isNotNull(),\n",
					"                regexp_replace(col(\"LA_Code\"), \"[^0-9]\", \"\").cast(\"int\")\n",
					"            ).otherwise(lit(None)).alias(\"rec_num\"),\n",
					"            \n",
					"            lit(None).cast(\"string\").alias(\"perm_disqual\"),\n",
					"            lit(None).cast(\"string\").alias(\"source_id\"),\n",
					"            when(\n",
					"                col(\"PostCode\").isNotNull(),\n",
					"                split(trim(col(\"PostCode\")), \" \")[0]\n",
					"            ).otherwise(lit(None)).alias(\"postcode_start\"),\n",
					"            when(\n",
					"                col(\"hash_id\").isNotNull(),\n",
					"                col(\"hash_id\")\n",
					"            ).otherwise(lit(None)).alias(\"hash_id\"),\n",
					"            when(\n",
					"                col(\"CreationDate\").isNotNull(),\n",
					"                col(\"CreationDate\")\n",
					"            ).otherwise(date_format(current_timestamp(), \"yyyyMMdd\")).alias(\"creation_date\")\n",
					"        )\n",
					"        \n",
					"        print(f\"Transformation complete. Result has {transformed_df.count()} rows and {len(transformed_df.columns)} columns\")\n",
					"        print(f\"Result columns: {transformed_df.columns}\")\n",
					"        \n",
					"        return transformed_df\n",
					"    except Exception as e:\n",
					"        # Error handling code\n",
					"        print(f\"Error in transform_to_target_schema: {str(e)}\")\n",
					"        import traceback\n",
					"        print(traceback.format_exc())\n",
					"        \n",
					"        \n",
					"         # Create a minimal valid DataFrame instead of returning None\n",
					"        print(\"Creating minimal valid DataFrame to avoid pipeline failure\")\n",
					"        columns = [\n",
					"            \"part_no\", \"register_lett\", \"poll_number\",\n",
					"            \"new_marker\", \"title\", \"lname\", \"fname\", \"dob\", \"flags\",\n",
					"            \"address\", \"address2\", \"address3\", \"address4\", \"address5\", \n",
					"            \"address6\", \"zip\", \"date_selected1\", \"date_selected2\", \n",
					"            \"date_selected3\", \"rec_num\", \"perm_disqual\", \"source_id\",\n",
					"            \"postcode_start\", \"hash_id\", \"creation_date\"\n",
					"        ]\n",
					"        \n",
					"        # Create empty DataFrame with the right structure\n",
					"        schema = StructType([StructField(c, StringType(), True) for c in columns])\n",
					"        empty_df = spark.createDataFrame([], schema)\n",
					"        \n",
					"        return empty_df"
				],
				"execution_count": 100
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Merge with Existing Data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def merge_with_existing_data(spark, new_data, output_path):\n",
					"    \"\"\"Merge new data with existing data using SQL-based approach for better reliability\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    # Register temporary views\n",
					"    new_data.createOrReplaceTempView(\"new_data\")\n",
					"    \n",
					"    # Check if existing data is available\n",
					"    if mssparkutils.fs.exists(output_path):\n",
					"        try:\n",
					"            # Explicitly refresh to avoid stale references\n",
					"            spark.sql(\"REFRESH TABLE existing_data\")\n",
					"            existing_data = spark.read.parquet(output_path)\n",
					"            existing_data.createOrReplaceTempView(\"existing_data\")\n",
					"            \n",
					"            # Use SQL approach which has better optimisation\n",
					"            merged_data = spark.sql(\"\"\"\n",
					"                -- Keep all new records that don't exist in the existing data\n",
					"                SELECT n.*\n",
					"                FROM new_data n\n",
					"                LEFT ANTI JOIN existing_data e\n",
					"                ON n.poll_number = e.poll_number AND n.register_lett = e.register_lett\n",
					"                \n",
					"                UNION ALL\n",
					"                \n",
					"                -- For existing records, either keep them or update with new values if available\n",
					"                SELECT \n",
					"                    COALESCE(n.part_no, e.part_no) as part_no,\n",
					"                    COALESCE(n.register_lett, e.register_lett) as register_lett,\n",
					"                    COALESCE(n.poll_number, e.poll_number) as poll_number,\n",
					"                    COALESCE(n.new_marker, e.new_marker) as new_marker,\n",
					"                    COALESCE(n.title, e.title) as title,\n",
					"                    COALESCE(n.lname, e.lname) as lname,\n",
					"                    COALESCE(n.fname, e.fname) as fname,\n",
					"                    COALESCE(n.dob, e.dob) as dob,\n",
					"                    COALESCE(n.flags, e.flags) as flags,\n",
					"                    COALESCE(n.address, e.address) as address,\n",
					"                    COALESCE(n.address2, e.address2) as address2, \n",
					"                    COALESCE(n.address3, e.address3) as address3,\n",
					"                    COALESCE(n.address4, e.address4) as address4,\n",
					"                    COALESCE(n.address5, e.address5) as address5,\n",
					"                    COALESCE(n.address6, e.address6) as address6,\n",
					"                    COALESCE(n.zip, e.zip) as zip,\n",
					"                    COALESCE(n.date_selected1, e.date_selected1) as date_selected1,\n",
					"                    COALESCE(n.date_selected2, e.date_selected2) as date_selected2,\n",
					"                    COALESCE(n.date_selected3, e.date_selected3) as date_selected3,\n",
					"                    COALESCE(n.rec_num, e.rec_num) as rec_num,\n",
					"                    COALESCE(n.perm_disqual, e.perm_disqual) as perm_disqual,\n",
					"                    COALESCE(n.source_id, e.source_id) as source_id,\n",
					"                    COALESCE(n.postcode_start, e.postcode_start) as postcode_start,\n",
					"                    COALESCE(n.hash_id, e.hash_id) as hash_id,\n",
					"                    -- Use the newer creation date\n",
					"                    CASE \n",
					"                        WHEN n.creation_date > e.creation_date THEN n.creation_date\n",
					"                        ELSE e.creation_date\n",
					"                    END as creation_date\n",
					"                FROM existing_data e\n",
					"                LEFT JOIN new_data n\n",
					"                ON e.poll_number = n.poll_number AND e.register_lett = n.register_lett\n",
					"                WHERE n.poll_number IS NULL OR n.creation_date > e.creation_date\n",
					"            \"\"\")\n",
					"            \n",
					"            return merged_data\n",
					"            \n",
					"        except Exception as e:\n",
					"            logging.error(f\"Error merging with existing data: {str(e)}\")\n",
					"            logging.warning(\"Returning only new data due to merge error\")\n",
					"            return new_data\n",
					"    else:\n",
					"        # No existing data, just return the new data\n",
					"        return new_data"
				],
				"execution_count": 101
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Output and Update Process Log"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def write_output_safely(df, output_path, description=\"data\"):\n",
					"    \"\"\"Write DataFrame to storage with proper error handling\"\"\"\n",
					"    # Get the logger\n",
					"    logger = logging.getLogger()\n",
					"    \n",
					"    try:\n",
					"        # Ensure output directory exists\n",
					"        from notebookutils import mssparkutils\n",
					"        dir_path = \"/\".join(output_path.split(\"/\")[:-1])\n",
					"        \n",
					"        if not mssparkutils.fs.exists(dir_path):\n",
					"            logger.info(f\"Creating output directory: {dir_path}\")\n",
					"            mssparkutils.fs.mkdirs(dir_path)\n",
					"        \n",
					"        # Write data with caching disabled to avoid stale references\n",
					"        df.write.mode(\"overwrite\").option(\"cacheMetadata\", \"false\").parquet(output_path)\n",
					"        logger.info(f\"Successfully wrote {df.count()} rows of {description} to {output_path}\")\n",
					"        return True\n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error writing {description} to {output_path}: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return False"
				],
				"execution_count": 102
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"def write_output_and_update_log(spark, merged_data, output_path, new_file_paths, process_log_path):\n",
					"    \"\"\"Write the merged data to output location and update the process log\"\"\"\n",
					"    logger = logging.getLogger()\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    try:\n",
					"        # Ensure output directory exists\n",
					"        dir_path = output_path\n",
					"        if '.' in output_path.split('/')[-1]:  # Check if last part looks like a filename\n",
					"            dir_path = '/'.join(output_path.split('/')[:-1])\n",
					"            \n",
					"        if not mssparkutils.fs.exists(dir_path):\n",
					"            logger.info(f\"Creating output directory: {dir_path}\")\n",
					"            mssparkutils.fs.mkdirs(dir_path)\n",
					"        \n",
					"        # Write the merged data\n",
					"        logger.info(f\"Writing {merged_data.count()} records to {output_path}\")\n",
					"        merged_data.write.mode(\"overwrite\").parquet(output_path)\n",
					"        logger.info(\"Successfully wrote data to output location\")\n",
					"        \n",
					"        # Rest of function remains the same...\n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error writing output or updating log: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return False"
				],
				"execution_count": 103
			},
			{
				"cell_type": "code",
				"source": [
					"def update_process_log(spark, new_file_paths, process_log_path):\n",
					"    \"\"\"Update the processing log with newly processed files\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    from pyspark.sql.types import StringType, TimestampType, StructType, StructField\n",
					"    \n",
					"    try:\n",
					"        # Create schema with explicit types\n",
					"        log_schema = StructType([\n",
					"            StructField(\"file_path\", StringType(), False),\n",
					"            StructField(\"process_timestamp\", TimestampType(), False)\n",
					"        ])\n",
					"        \n",
					"        # Get current timestamp, properly formatted\n",
					"        timestamp_now = datetime.now()\n",
					"        \n",
					"        # Create new log entries with explicit timestamp\n",
					"        new_log_entries = [(file_path, timestamp_now) for file_path in new_file_paths]\n",
					"        \n",
					"        # Create DataFrame with explicit schema\n",
					"        new_log_df = spark.createDataFrame(new_log_entries, log_schema)\n",
					"        \n",
					"        # Update existing log if it exists\n",
					"        if mssparkutils.fs.exists(process_log_path):\n",
					"            try:\n",
					"                existing_log = spark.read.parquet(process_log_path)\n",
					"                # Ensure the timestamp field has the correct type\n",
					"                existing_log = existing_log.withColumn(\"process_timestamp\", \n",
					"                                                     col(\"process_timestamp\").cast(\"timestamp\"))\n",
					"                updated_log = existing_log.union(new_log_df)\n",
					"            except Exception as e:\n",
					"                print(f\"Error reading existing log: {str(e)}. Creating new log.\")\n",
					"                updated_log = new_log_df\n",
					"        else:\n",
					"            updated_log = new_log_df\n",
					"            \n",
					"        # Write updated log\n",
					"        updated_log.write.mode(\"overwrite\").parquet(process_log_path)\n",
					"        \n",
					"        return True\n",
					"    except Exception as e:\n",
					"        print(f\"Error updating process log: {str(e)}\")\n",
					"        import traceback\n",
					"        print(traceback.format_exc())\n",
					"        return False"
				],
				"execution_count": 104
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Main Pipeline Function"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def main():\n",
					"    \"\"\"Main function to run the incremental load process\"\"\"\n",
					"    # Initialize\n",
					"    print(\"Starting electoral data ETL process\")\n",
					"    print(f\"Current time: {datetime.now()}\")\n",
					"\n",
					"    # Configure logging and capture the logger\n",
					"    logger = configure_logging()\n",
					"    print(\"Logging configured successfully\")\n",
					"    logger.info(\"Logging configured successfully\")\n",
					"\n",
					"    spark = create_spark_session()\n",
					"    config = load_config()\n",
					"    storage_account = config[\"storage_account\"]\n",
					"    process_log_path = f\"{config['output_path']}_process_log\"\n",
					"\n",
					"    print(f\"Looking for files in: {config['input_path']}\")\n",
					"\n",
					"    try:\n",
					"        # 1. Get configuration and mapping\n",
					"        column_mapping = read_column_mapping(\n",
					"            spark, f\"{config['config_path']}/col_mapping_schema.json\"\n",
					"        )\n",
					"\n",
					"        # 2. Identify new files\n",
					"        new_file_paths, all_file_paths = get_new_files(\n",
					"            spark, config[\"input_path\"], process_log_path\n",
					"        )\n",
					"\n",
					"        if not new_file_paths:\n",
					"            logger.info(\"No new files to process\")\n",
					"            return\n",
					"\n",
					"        logger.info(f\"Found {len(new_file_paths)} new files to process\")\n",
					"\n",
					"        # 3. Process new files\n",
					"        processed_dfs = process_input_files(spark, new_file_paths, column_mapping)\n",
					"\n",
					"        if not processed_dfs or len(processed_dfs) == 0:\n",
					"            logger.error(\"No data to process after reading files\")\n",
					"            return\n",
					"\n",
					"        logger.info(f\"Successfully processed {len(processed_dfs)} DataFrames\")\n",
					"\n",
					"        # 4. Merge new data\n",
					"        merged_new_data = merge_dataframes(processed_dfs)\n",
					"        if merged_new_data is None:\n",
					"            logger.error(\"Merged DataFrame is None. Check merge_dataframes function\")\n",
					"            return\n",
					"\n",
					"        logger.info(f\"Merged data contains {merged_new_data.count()} rows\")\n",
					"\n",
					"        # 5. Deduplicate new data\n",
					"        deduplicated_data = optimise_deduplicate_data(merged_new_data)\n",
					"        if deduplicated_data is None:\n",
					"            logger.error(\"Deduplicated DataFrame is None\")\n",
					"            return\n",
					"\n",
					"        logger.info(f\"After deduplication: {deduplicated_data.count()} rows\")\n",
					"\n",
					"        # 6. Apply hashing\n",
					"        hashed_data = apply_hashing(merged_new_data)\n",
					"        if hashed_data is None:\n",
					"            logger.error(\"Hashed DataFrame is None\")\n",
					"            return\n",
					"        logger.info(f\"After applying improved hashing to {hashed_data.count()} rows\")\n",
					"\n",
					"        # Apply new deduplication\n",
					"        deduplicated_data = improved_deduplicate_data(hashed_data)\n",
					"        if deduplicated_data is None:\n",
					"            logger.error(\"Deduplicated DataFrame is None\")\n",
					"            return\n",
					"\n",
					"        logger.info(f\"After improved deduplication: {deduplicated_data.count()} rows\")\n",
					"\n",
					"        # 7. Transform to target schema\n",
					"        transformed_data = transform_to_target_schema(deduplicated_data, column_mapping)\n",
					"        if transformed_data is None:\n",
					"            logger.error(\"Transformed DataFrame is None\")\n",
					"            return\n",
					"        # Check that transformed_data is not empty\n",
					"        if transformed_data.count() == 0:\n",
					"            logger.warning(\"Transformed DataFrame is empty - nothing to hash\")\n",
					"            return\n",
					"        try: \n",
					"            # Apply hashing to sensitive columns\n",
					"            transformed_data = apply_hashing_to_voters(transformed_data)\n",
					"            logger.info(\"Applied hashing to sensitive columns\")\n",
					"        except Exception as hash_error:\n",
					"            logger.error(f\"Error applying hashing: {str(hash_error)}\")\n",
					"            import traceback\n",
					"            logger.error(traceback.format_exc())\n",
					"            # Continue with unhashed data rather than failing completely\n",
					"            logger.warning(\"Continuing with unhashed data\")\n",
					"          # Apply final deduplication on transformed data with new hash_id\n",
					"        try:\n",
					"            final_deduplicated = improved_deduplicate_data(transformed_data)\n",
					"            if final_deduplicated is None:\n",
					"                logger.error(\"Final deduplication failed\")\n",
					"                # Use transformed_data as fallback\n",
					"                final_deduplicated = transformed_data\n",
					"                logger.warning(\"Using non-deduplicated data as fallback\")\n",
					"        except Exception as dedup_error:\n",
					"            logger.error(f\"Error in final deduplication: {str(dedup_error)}\")\n",
					"            # Use transformed_data as fallback\n",
					"            final_deduplicated = transformed_data\n",
					"            logger.warning(\"Using non-deduplicated data as fallback\")\n",
					"\n",
					"        logger.info(f\"After final deduplication: {final_deduplicated.count()} rows\")\n",
					"\n",
					"        # Ensure schema consistency\n",
					"        final_deduplicated = ensure_consistent_schema(final_deduplicated, TARGET_SCHEMA)\n",
					"        logger.info(f\"Schema standardized with {len(TARGET_SCHEMA)} columns\")\n",
					"\n",
					"        transformed_data = ensure_consistent_schema(transformed_data, TARGET_SCHEMA)\n",
					"        logger.info(f\"Transformed data contains {transformed_data.count()} rows\")\n",
					"\n",
					"        # 8. Merge with existing data\n",
					"        final_data = merge_with_existing_data(\n",
					"            spark, final_deduplicated, config[\"output_path\"]\n",
					"        )\n",
					"        if final_data is None:\n",
					"            logger.error(\"Final DataFrame is None\")\n",
					"            return\n",
					"\n",
					"        merged_new_data = merge_dataframes(processed_dfs)\n",
					"        if merged_new_data is None:\n",
					"            logger.error(\"Merged DataFrame is None. Check merge_dataframes function\")\n",
					"            return\n",
					"\n",
					"        logger.info(f\"Merged data contains {merged_new_data.count()} rows\")\n",
					"        deduplicated_by_la = deduplicate_by_la_code_and_date(merged_new_data)\n",
					"        logger.info(f\"After LA_Code deduplication: {deduplicated_by_la.count()} rows\")\n",
					"\n",
					"        # 9. Write output to both locations\n",
					"        logger.info(f\"Writing {final_data.count()} records to {config['output_path']}\")\n",
					"\n",
					"        # Make sure directories exist\n",
					"        for path in [\n",
					"            config[\"output_path\"],\n",
					"            f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_deduplicated\",\n",
					"        ]:\n",
					"            dir_path = path\n",
					"            from notebookutils import mssparkutils\n",
					"\n",
					"            if not mssparkutils.fs.exists(dir_path):\n",
					"                logger.info(f\"Creating directory: {dir_path}\")\n",
					"                mssparkutils.fs.mkdirs(dir_path)\n",
					"\n",
					"        if final_data.count() == 0:\n",
					"            logger.warning(\"Final data is empty, nothing to write\")\n",
					"            return\n",
					"\n",
					"        # Update the process log\n",
					"        success1 = write_output_safely(\n",
					"            final_data, config[\"output_path\"], \"transformed data\"\n",
					"        )\n",
					"\n",
					"        success1 = write_output_safely(\n",
					"            final_data, config[\"output_path\"], \"transformed data\"\n",
					"        )\n",
					"        success2 = False  # Initialize the variable\n",
					"\n",
					"        if success1:\n",
					"            success2 = write_output_safely(\n",
					"                final_data,\n",
					"                f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_deduplicated\",\n",
					"                \"deduplicated data\",\n",
					"            )\n",
					"\n",
					"        if success1 and success2:\n",
					"            logger.info(f\"Successfully processed {len(new_file_paths)} files\")\n",
					"            logger.info(f\"Total records in output: {final_data.count()}\")\n",
					"\n",
					"            # Update the process log\n",
					"            log_success = update_process_log(spark, new_file_paths, process_log_path)\n",
					"            if log_success:\n",
					"                logger.info(f\"Updated process log with {len(new_file_paths)} files\")\n",
					"            else:\n",
					"                logger.warning(\n",
					"                    \"Failed to update process log, but data was written successfully\"\n",
					"                )\n",
					"        else:\n",
					"            logger.error(\"Failed to write output to one or more destinations\")\n",
					"\n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error in incremental load process: {str(e)}\")\n",
					"        import traceback\n",
					"\n",
					"        logger.error(traceback.format_exc())\n",
					"        raise\n",
					"\n",
					"\n",
					"if __name__ == \"__main__\":\n",
					"    main()\n",
					""
				],
				"execution_count": 105
			}
		]
	}
}