{
	"name": "L2_Er_Juror_TransformationToSchema",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "2c4c166e-e07f-41c0-ac5f-650dac4027d3"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import logging\n",
					"import os\n",
					"import json\n",
					"from datetime import datetime\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import (\n",
					"    col, lit, when, upper, trim, split, element_at, size, \n",
					"    concat_ws, regexp_replace, regexp_extract, length, \n",
					"    collect_list, greatest\n",
					")\n",
					"from pyspark.sql.types import StringType, StructType, StructField\n",
					"from notebookutils import mssparkutils\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# Configure logging\n",
					"logging.basicConfig(\n",
					"    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
					")\n",
					"\n",
					"def create_spark_session():\n",
					"    \"\"\"Create and return a Spark session\"\"\"\n",
					"    return SparkSession.builder.appName(\"Electoral Data Transformation\").getOrCreate()\n",
					"\n",
					"def read_file(spark, file_path):\n",
					"    \"\"\"Read file as DataFrame with resilient options\"\"\"\n",
					"    try:\n",
					"        if file_path.lower().endswith(\".csv\"):\n",
					"            logging.info(f\"Reading as CSV file: {file_path}\")\n",
					"            return (\n",
					"                spark.read.option(\"header\", \"true\")\n",
					"                .option(\"inferSchema\", \"false\")\n",
					"                .option(\"mode\", \"PERMISSIVE\")\n",
					"                .option(\"encoding\", \"UTF-8\")\n",
					"                .csv(file_path)\n",
					"            )\n",
					"        elif file_path.lower().endswith((\".xlsx\", \".xls\")):\n",
					"            logging.info(f\"Reading as Excel file: {file_path}\")\n",
					"            return read_excel_file(spark, file_path)\n",
					"        else:\n",
					"            raise ValueError(f\"Unsupported file format: {file_path}\")\n",
					"    except Exception as e:\n",
					"        error_msg = f\"Error reading file {file_path}: {str(e)}\"\n",
					"        logging.error(error_msg)\n",
					"        raise type(e)(error_msg) from e\n",
					"\n",
					"def read_excel_file(spark, file_path):\n",
					"    \"\"\"Read Excel file with multiple fallback mechanisms\"\"\"\n",
					"    errors = []\n",
					"\n",
					"    # Attempt 1: Basic reading\n",
					"    try:\n",
					"        return (\n",
					"            spark.read.format(\"com.crealytics.spark.excel\")\n",
					"            .option(\"header\", \"true\")\n",
					"            .option(\"inferSchema\", \"false\")\n",
					"            .option(\"treatEmptyValuesAsNulls\", \"true\")\n",
					"            .load(file_path)\n",
					"        )\n",
					"    except Exception as e:\n",
					"        errors.append(f\"Basic reading failed: {str(e)}\")\n",
					"\n",
					"    # Attempt 2: With Sheet1 specification\n",
					"    try:\n",
					"        return (\n",
					"            spark.read.format(\"com.crealytics.spark.excel\")\n",
					"            .option(\"header\", \"true\")\n",
					"            .option(\"inferSchema\", \"false\")\n",
					"            .option(\"treatEmptyValuesAsNulls\", \"true\")\n",
					"            .option(\"sheetName\", \"Sheet1\")\n",
					"            .load(file_path)\n",
					"        )\n",
					"    except Exception as e:\n",
					"        errors.append(f\"Sheet1 reading failed: {str(e)}\")\n",
					"\n",
					"    # Attempt 3: With sheet index\n",
					"    try:\n",
					"        return (\n",
					"            spark.read.format(\"com.crealytics.spark.excel\")\n",
					"            .option(\"header\", \"true\")\n",
					"            .option(\"inferSchema\", \"false\")\n",
					"            .option(\"treatEmptyValuesAsNulls\", \"true\")\n",
					"            .option(\"sheetIndex\", 0)\n",
					"            .load(file_path)\n",
					"        )\n",
					"    except Exception as e:\n",
					"        errors.append(f\"Sheet index reading failed: {str(e)}\")\n",
					"\n",
					"    # If all attempts failed, raise an informative error\n",
					"    raise Exception(\n",
					"        f\"Failed to read Excel file {file_path}. Errors: {'; '.join(errors)}\"\n",
					"    )\n",
					"\n",
					"def standardize_column_names(df, reverse_mapping):\n",
					"    \"\"\"Standardize column names using the mapping\"\"\"\n",
					"    if not reverse_mapping:\n",
					"        logging.warning(\"Empty reverse mapping provided - cannot standardize columns\")\n",
					"        return df\n",
					"\n",
					"    renamed_columns = []\n",
					"\n",
					"    for col in df.columns:\n",
					"        std_name = reverse_mapping.get(col.lower())\n",
					"        if std_name:\n",
					"            df = df.withColumnRenamed(col, std_name)\n",
					"            renamed_columns.append(f\"{col} -> {std_name}\")\n",
					"\n",
					"    # Check if Flags/Markers exists after standardization, add if missing\n",
					"    if \"Flags/Markers\" not in df.columns:\n",
					"        logging.info(\"Adding missing 'Flags/Markers' column after standardization\")\n",
					"        df = df.withColumn(\"Flags/Markers\", lit(None).cast(StringType()))\n",
					"\n",
					"    if renamed_columns:\n",
					"        logging.info(\n",
					"            f\"Renamed {len(renamed_columns)} columns: {', '.join(renamed_columns[:5])}...\"\n",
					"        )\n",
					"    else:\n",
					"        logging.warning(\"No columns were renamed - check your mapping configuration\")\n",
					"\n",
					"    return df\n",
					"\n",
					"def ensure_required_fields(df):\n",
					"    \"\"\"Ensure all required fields exist with proper data types\"\"\"\n",
					"    # Add Elector Title if it doesn't exist\n",
					"    if \"Elector Title\" not in df.columns and \"Title\" not in df.columns:\n",
					"        logging.info(\"Adding missing 'Elector Title' column\")\n",
					"        df = df.withColumn(\"Elector Title\", lit(None).cast(StringType()))\n",
					"    elif \"Title\" in df.columns and \"Elector Title\" not in df.columns:\n",
					"        logging.info(\"Renaming 'Title' to 'Elector Title'\")\n",
					"        df = df.withColumnRenamed(\"Title\", \"Elector Title\")\n",
					"\n",
					"    # Ensure other essential columns exist\n",
					"    required_columns = {\n",
					"        \"Elector Surname\": StringType(),\n",
					"        \"Elector Forename\": StringType(),\n",
					"        \"Elector DOB\": StringType(),\n",
					"        \"Address1\": StringType(),\n",
					"        \"PostCode\": StringType(),\n",
					"    }\n",
					"\n",
					"    for col_name, data_type in required_columns.items():\n",
					"        if col_name not in df.columns:\n",
					"            logging.info(f\"Adding missing '{col_name}' column\")\n",
					"            df = df.withColumn(col_name, lit(None).cast(data_type))\n",
					"\n",
					"    return df\n",
					"\n",
					"def combine_name_components(df):\n",
					"    \"\"\"Combine initials and middle names into Elector Forename\"\"\"\n",
					"    # Check if we have initials column\n",
					"    has_initials = False\n",
					"    initials_col = None\n",
					"\n",
					"    for col_name in [\"Initial\", \"Initials\"]:\n",
					"        if col_name in df.columns:\n",
					"            has_initials = True\n",
					"            initials_col = col_name\n",
					"            break\n",
					"\n",
					"    # Check if we have middle name column\n",
					"    has_middle_name = False\n",
					"    middle_name_col = None\n",
					"\n",
					"    for col_name in [\n",
					"        \"ElectorMiddleName\",\n",
					"        \"Elector Middlename\",\n",
					"        \"MiddleName\",\n",
					"        \"Middle Name\",\n",
					"    ]:\n",
					"        if col_name in df.columns:\n",
					"            has_middle_name = True\n",
					"            middle_name_col = col_name\n",
					"            break\n",
					"\n",
					"    # Combine components if needed\n",
					"    if \"Elector Forename\" in df.columns:\n",
					"        if has_initials:\n",
					"            # Add initials to forename (when both exist)\n",
					"            df = df.withColumn(\n",
					"                \"Elector Forename\",\n",
					"                when(\n",
					"                    (col(\"Elector Forename\").isNotNull())\n",
					"                    & (col(initials_col).isNotNull())\n",
					"                    & (length(trim(col(initials_col))) > 0),\n",
					"                    concat_ws(\" \", col(\"Elector Forename\"), col(initials_col)),\n",
					"                ).otherwise(col(\"Elector Forename\")),\n",
					"            )\n",
					"\n",
					"            # Use initials as forename when forename is null\n",
					"            df = df.withColumn(\n",
					"                \"Elector Forename\",\n",
					"                when(\n",
					"                    (col(\"Elector Forename\").isNull())\n",
					"                    & (col(initials_col).isNotNull())\n",
					"                    & (length(trim(col(initials_col))) > 0),\n",
					"                    col(initials_col),\n",
					"                ).otherwise(col(\"Elector Forename\")),\n",
					"            )\n",
					"\n",
					"        if has_middle_name:\n",
					"            # Add middle name to forename (when both exist)\n",
					"            df = df.withColumn(\n",
					"                \"Elector Forename\",\n",
					"                when(\n",
					"                    (col(\"Elector Forename\").isNotNull())\n",
					"                    & (col(middle_name_col).isNotNull())\n",
					"                    & (length(trim(col(middle_name_col))) > 0),\n",
					"                    concat_ws(\" \", col(\"Elector Forename\"), col(middle_name_col)),\n",
					"                ).otherwise(col(\"Elector Forename\")),\n",
					"            )\n",
					"\n",
					"            # Use middle name as forename when forename is null\n",
					"            df = df.withColumn(\n",
					"                \"Elector Forename\",\n",
					"                when(\n",
					"                    (col(\"Elector Forename\").isNull())\n",
					"                    & (col(middle_name_col).isNotNull())\n",
					"                    & (length(trim(col(middle_name_col))) > 0),\n",
					"                    col(middle_name_col),\n",
					"                ).otherwise(col(\"Elector Forename\")),\n",
					"            )\n",
					"\n",
					"    # Upper case and trim the final result\n",
					"    if \"Elector Forename\" in df.columns:\n",
					"        df = df.withColumn(\"Elector Forename\", upper(trim(col(\"Elector Forename\"))))\n",
					"\n",
					"    return df\n",
					"\n",
					"def enhanced_split_elector_name(df):\n",
					"    \"\"\"Split elector name with improved handling of multiple forenames\"\"\"\n",
					"    if \"Elector Name\" in df.columns:\n",
					"        logging.info(\"Found 'Elector Name' column, performing enhanced name splitting\")\n",
					"\n",
					"        # First, create a backup of the original name\n",
					"        df = df.withColumn(\"Original_Name\", col(\"Elector Name\"))\n",
					"\n",
					"        # Split the name into parts\n",
					"        df = df.withColumn(\"name_parts\", split(col(\"Elector Name\"), \" \"))\n",
					"\n",
					"        # Extract surname (first part in UK electoral register format)\n",
					"        df = df.withColumn(\n",
					"            \"Elector Surname\",\n",
					"            when(\n",
					"                size(col(\"name_parts\")) > lit(0),\n",
					"                upper(trim(element_at(col(\"name_parts\"), lit(1)))),\n",
					"            ).otherwise(lit(None)),\n",
					"        )\n",
					"\n",
					"        # Extract primary forename (first name after surname)\n",
					"        df = df.withColumn(\n",
					"            \"Elector Forename\",\n",
					"            when(\n",
					"                size(col(\"name_parts\")) > lit(1),\n",
					"                upper(trim(element_at(col(\"name_parts\"), lit(2)))),\n",
					"            ).otherwise(lit(None)),\n",
					"        )\n",
					"\n",
					"        # Extract middle names (everything after the first forename)\n",
					"        df = df.withColumn(\n",
					"            \"Elector Middlename\",\n",
					"            when(\n",
					"                size(col(\"name_parts\")) > lit(2),\n",
					"                upper(\n",
					"                    trim(\n",
					"                        concat_ws(\n",
					"                            \" \",\n",
					"                            slice(\n",
					"                                col(\"name_parts\"),\n",
					"                                lit(3),\n",
					"                                greatest(lit(1), size(col(\"name_parts\")) - lit(2)),\n",
					"                            ),\n",
					"                        )\n",
					"                    )\n",
					"                ),\n",
					"            ).otherwise(lit(None)),\n",
					"        )\n",
					"\n",
					"        # Drop temporary column\n",
					"        df = df.drop(\"name_parts\")\n",
					"\n",
					"        logging.info(\"Successfully performed enhanced name splitting\")\n",
					"\n",
					"    return df\n",
					"\n",
					"def extract_numeric_la_code(df):\n",
					"    \"\"\"Extract only the numeric part from LA_Code\"\"\"\n",
					"    if \"LA_Code\" in df.columns:\n",
					"        logging.info(\"Extracting numeric part from LA_Code\")\n",
					"\n",
					"        # Extract only numeric characters from LA_Code\n",
					"        df = df.withColumn(\"LA_Code\", regexp_extract(col(\"LA_Code\"), \"([0-9]+)\", 1))\n",
					"\n",
					"        # If extraction results in empty string, set to null\n",
					"        df = df.withColumn(\n",
					"            \"LA_Code\", when(col(\"LA_Code\") == \"\", None).otherwise(col(\"LA_Code\"))\n",
					"        )\n",
					"\n",
					"        logging.info(\"LA_Code numeric extraction complete\")\n",
					"\n",
					"    return df\n",
					"\n",
					"def clean_special_characters(df):\n",
					"    \"\"\"Clean special characters from address and postcode fields\"\"\"\n",
					"    logging.info(\"Cleaning special characters from address and postcode fields\")\n",
					"\n",
					"    # List of address and postcode columns to clean\n",
					"    address_cols = [\n",
					"        \"Address1\",\n",
					"        \"Address2\",\n",
					"        \"Address3\",\n",
					"        \"Address4\",\n",
					"        \"Address5\",\n",
					"        \"Address6\",\n",
					"        \"PostCode\",\n",
					"    ]\n",
					"\n",
					"    for col_name in address_cols:\n",
					"        if col_name in df.columns:\n",
					"            # Replace non-ASCII characters with appropriate substitutions\n",
					"            df = df.withColumn(\n",
					"                col_name,\n",
					"                regexp_replace(\n",
					"                    # First replace � and similar replacement characters\n",
					"                    regexp_replace(col(col_name), \"�\", \"\"),\n",
					"                    # Then replace other problematic characters\n",
					"                    \"[^\\x00-\\x7f]\",\n",
					"                    \"\",\n",
					"                ),\n",
					"            )\n",
					"\n",
					"            # Trim any resulting double spaces\n",
					"            df = df.withColumn(col_name, trim(regexp_replace(col(col_name), \" +\", \" \")))\n",
					"\n",
					"            # For postcodes specifically, remove all non-alphanumeric characters except spaces\n",
					"            if col_name == \"PostCode\":\n",
					"                df = df.withColumn(\n",
					"                    col_name, regexp_replace(col(col_name), \"[^A-Z0-9 ]\", \"\")\n",
					"                )\n",
					"\n",
					"                # Ensure proper postcode format (if possible)\n",
					"                df = df.withColumn(\n",
					"                    col_name,\n",
					"                    when(\n",
					"                        # If it looks like a valid UK postcode pattern after cleaning\n",
					"                        col(col_name).rlike(\n",
					"                            \"^[A-Z]{1,2}[0-9][0-9A-Z]? ?[0-9][A-Z]{2}$\"\n",
					"                        ),\n",
					"                        # Ensure proper spacing (one space between outward and inward parts)\n",
					"                        regexp_replace(\n",
					"                            col(col_name),\n",
					"                            \"^([A-Z]{1,2}[0-9][0-9A-Z]?)[ ]*([0-9][A-Z]{2})$\",\n",
					"                            \"$1 $2\",\n",
					"                        ),\n",
					"                    ).otherwise(col(col_name)),\n",
					"                )\n",
					"\n",
					"    return df\n",
					"\n",
					"def fix_numeric_fields(df):\n",
					"    \"\"\"Fix numeric fields that might be interpreted as dates\"\"\"\n",
					"    numeric_columns = [\n",
					"        \"Elector Number\",\n",
					"        \"Poll number\",\n",
					"        \"poll_number\",\n",
					"        \"RollNo\",\n",
					"        \"Eno\",\n",
					"        \"ElectorNumber\",\n",
					"    ]\n",
					"\n",
					"    for col_name in numeric_columns:\n",
					"        if col_name in df.columns:\n",
					"            logging.info(f\"Fixing numeric format for column: {col_name}\")\n",
					"\n",
					"            # Check column data type first\n",
					"            col_type = df.schema[col_name].dataType\n",
					"            if isinstance(col_type, StringType):\n",
					"                # Only process string columns\n",
					"                df = df.withColumn(\n",
					"                    f\"{col_name}_temp\",\n",
					"                    when(\n",
					"                        # Check if value contains date separators\n",
					"                        regexp_extract(col(col_name), \"(-|/)\", 1) != \"\",\n",
					"                        # If it's a date format, extract only numeric part\n",
					"                        regexp_replace(col(col_name), \"[^0-9]\", \"\"),\n",
					"                    )\n",
					"                    .when(\n",
					"                        # For pure numeric strings, keep as is\n",
					"                        col(col_name).rlike(\"^[0-9]+$\"),\n",
					"                        col(col_name),\n",
					"                    )\n",
					"                    .otherwise(\n",
					"                        # For other formats, try to extract numeric part\n",
					"                        regexp_replace(col(col_name), \"[^0-9]\", \"\")\n",
					"                    ),\n",
					"                )\n",
					"\n",
					"                # Cast to integer if possible\n",
					"                df = df.withColumn(\n",
					"                    f\"{col_name}_temp\",\n",
					"                    when(\n",
					"                        col(f\"{col_name}_temp\").rlike(\"^[0-9]+$\"),\n",
					"                        col(f\"{col_name}_temp\").cast(\"int\"),\n",
					"                    ).otherwise(col(f\"{col_name}_temp\")),\n",
					"                )\n",
					"\n",
					"                # Replace original column with fixed version\n",
					"                df = df.drop(col_name).withColumnRenamed(f\"{col_name}_temp\", col_name)\n",
					"\n",
					"    return df\n",
					"\n",
					"def process_name_components(df):\n",
					"    \"\"\"Process name components separately\"\"\"\n",
					"    name_columns = [\n",
					"        \"Elector Title\",\n",
					"        \"Elector Forename\",\n",
					"        \"Elector Surname\",\n",
					"        \"Elector Middlename\",\n",
					"        \"Suffix\",\n",
					"        \"Initials\",\n",
					"    ]\n",
					"\n",
					"    for col_name in name_columns:\n",
					"        if col_name in df.columns:\n",
					"            df = df.withColumn(\n",
					"                col_name,\n",
					"                when(col(col_name).isNotNull(), upper(trim(col(col_name)))).otherwise(\n",
					"                    None\n",
					"                ),\n",
					"            )\n",
					"\n",
					"    return df\n",
					"\n",
					"def get_folder_parts(file_path):\n",
					"    \"\"\"Extract folder information for naming with improved parsing\"\"\"\n",
					"    parts = file_path.split(\"/\")\n",
					"\n",
					"    try:\n",
					"        la_index = parts.index(\"LA_Data\")\n",
					"        if la_index > 0:\n",
					"            date_folder = parts[la_index - 1]\n",
					"            if len(parts) > la_index + 1:\n",
					"                # Get LA folder name (third folder)\n",
					"                third_folder = parts[la_index + 1]\n",
					"                return date_folder, third_folder\n",
					"    except ValueError:\n",
					"        pass\n",
					"\n",
					"    # Alternative parsing method if LA_Data not found\n",
					"    # Assuming format is something like storage/files/20240215_Birmingham.csv\n",
					"    file_name = parts[-1]\n",
					"    if \"_\" in file_name:\n",
					"        parts = file_name.split(\"_\", 1)\n",
					"        if len(parts) == 2:\n",
					"            date_part = parts[0]\n",
					"            la_part = parts[1].split(\".\")[0]  # Remove extension\n",
					"            return date_part, la_part\n",
					"\n",
					"    # If all else fails, use current date and filename\n",
					"    current_date = datetime.now().strftime(\"%Y%m%d\")\n",
					"    filename_without_ext = os.path.splitext(os.path.basename(file_path))[0]\n",
					"    return current_date, filename_without_ext\n",
					"\n",
					"def filter_records_with_empty_fields(df):\n",
					"    \"\"\"Filter out records where critical fields are empty\"\"\"\n",
					"    logging.info(f\"Original record count: {df.count()}\")\n",
					"\n",
					"    # Define critical fields that should not be empty\n",
					"    critical_fields = [\n",
					"        \"Elector Number\",\n",
					"        \"Elector Surname\",\n",
					"        \"Elector Forename\",\n",
					"        \"LA_Code\",\n",
					"    ]\n",
					"\n",
					"    # Filter out rows where critical fields are null or empty string\n",
					"    for field in critical_fields:\n",
					"        if field in df.columns:\n",
					"            initial_count = df.count()\n",
					"            df = df.filter((col(field).isNotNull()) & (length(trim(col(field))) > 0))\n",
					"            removed = initial_count - df.count()\n",
					"            logging.info(f\"Removed {removed} rows with empty {field}\")\n",
					"\n",
					"    logging.info(f\"Remaining record count after filtering: {df.count()}\")\n",
					"    return df\n",
					"\n",
					"def transform_data(df, creation_date, la_code=None):\n",
					"    \"\"\"Transform data by cleaning text columns and handling names\"\"\"\n",
					"    # Clean text columns\n",
					"    for col_name in df.columns:\n",
					"        if df.schema[col_name].dataType == StringType():\n",
					"            df = df.withColumn(col_name, upper(trim(df[col_name])))\n",
					"\n",
					"    # Ensure all required fields exist\n",
					"    df = ensure_required_fields(df)\n",
					"\n",
					"    # Clean special characters in address and postcode fields\n",
					"    df = clean_special_characters(df)\n",
					"\n",
					"    # Fix numeric fields\n",
					"    df = fix_numeric_fields(df)\n",
					"\n",
					"    # Process name fields\n",
					"    df = enhanced_split_elector_name(df)\n",
					"    df = process_name_components(df)\n",
					"\n",
					"    # Combine name components (initials and middle names into forename)\n",
					"    df = combine_name_components(df)\n",
					"\n",
					"    # Add metadata\n",
					"    if creation_date:\n",
					"        df = df.withColumn(\"CreationDate\", lit(creation_date))\n",
					"\n",
					"    if la_code:\n",
					"        # Extract only numeric part from la_code\n",
					"        numeric_la_code = \"\".join(c for c in la_code if c.isdigit())\n",
					"        df = df.withColumn(\"LA_Code\", lit(numeric_la_code))\n",
					"\n",
					"    # Extract numeric part from LA_Code if it exists\n",
					"    df = extract_numeric_la_code(df)\n",
					"\n",
					"    # Filter out records with empty critical fields\n",
					"    df = filter_records_with_empty_fields(df)\n",
					"\n",
					"    return df\n",
					"\n",
					"def filter_to_required_columns(df, column_mapping=None, keep_unmapped=False):\n",
					"    \"\"\"Filter columns based on mapping configuration\"\"\"\n",
					"    if not column_mapping:\n",
					"        logging.warning(\"No column mapping provided, returning original DataFrame\")\n",
					"        return df\n",
					"        \n",
					"    # Get all standard column names from the mapping\n",
					"    mapped_columns = list(column_mapping.keys())\n",
					"    \n",
					"    # Add essential metadata columns\n",
					"    essential_columns = [\"LA_Code\", \"CreationDate\"]\n",
					"    required_columns = mapped_columns + essential_columns\n",
					"    \n",
					"    if keep_unmapped:\n",
					"        # Keep all columns but reorder to put mapped columns first\n",
					"        existing_mapped = [col for col in required_columns if col in df.columns]"
				],
				"execution_count": null
			}
		]
	}
}