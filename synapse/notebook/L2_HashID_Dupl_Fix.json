{
	"name": "L2_HashID_Dupl_Fix",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkprod",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "65f9928a-ae32-4be0-80da-531b267f986c"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5ca62022-6aa2-4cee-aaa7-e7536c8d566c/resourceGroups/baubais-data-factory-rg-prod/providers/Microsoft.Synapse/workspaces/baubais-synapse-prod/bigDataPools/sparkprod",
				"name": "sparkprod",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkprod",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col, when, concat, count, lit\n",
					"from pyspark.sql.window import Window\n",
					"import pyspark.sql.functions as F\n",
					"import time  # Add this import for the timestamp\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col, when, concat, count, lit\n",
					"from pyspark.sql.window import Window\n",
					"import pyspark.sql.functions as F\n",
					"import time\n",
					"\n",
					"def fix_duplicate_hash_ids():\n",
					"    \"\"\"\n",
					"    Fixes duplicate hash_ids in voters_postgresql parquet files by appending\n",
					"    poll_number to hash_id when duplicates with different poll numbers are detected.\n",
					"    \"\"\"\n",
					"    # Initialize Spark session\n",
					"    spark = SparkSession.builder \\\n",
					"        .appName(\"Fix Duplicate Hash IDs\") \\\n",
					"        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
					"        .getOrCreate()\n",
					"    \n",
					"    # Define paths\n",
					"    storage_account = \"baubaisadfsaprod\"\n",
					"    input_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_postgresql\"\n",
					"    output_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_postgresql_fixed\"\n",
					"    \n",
					"    # Ensure output directory exists\n",
					"    from notebookutils import mssparkutils\n",
					"    if not mssparkutils.fs.exists(output_path):\n",
					"        mssparkutils.fs.mkdirs(output_path)\n",
					"    \n",
					"    print(f\"Reading data from: {input_path}\")\n",
					"    \n",
					"    # Read the data\n",
					"    df = spark.read.parquet(input_path)\n",
					"    \n",
					"    # Store the count right away to avoid issues later\n",
					"    total_record_count = df.count()\n",
					"    print(f\"Read {total_record_count} records\")\n",
					"    \n",
					"    # Identify duplicate hash_ids with different poll_numbers\n",
					"    window_spec = Window.partitionBy(\"hash_id\")\n",
					"    df_with_counts = df.withColumn(\n",
					"        \"hash_count\", F.count(\"hash_id\").over(window_spec)\n",
					"    )\n",
					"    \n",
					"    # Add flag to identify if records have different poll_numbers for same hash_id\n",
					"    window_poll_distinct = Window.partitionBy(\"hash_id\")\n",
					"    df_with_distinct_counts = df_with_counts.withColumn(\n",
					"        \"distinct_poll_numbers\", F.size(F.collect_set(\"poll_number\").over(window_poll_distinct))\n",
					"    )\n",
					"    \n",
					"    # Mark records that need fixing (those with same hash_id but different poll_numbers)\n",
					"    df_with_flag = df_with_distinct_counts.withColumn(\n",
					"        \"needs_fix\", \n",
					"        (col(\"hash_count\") > 1) & (col(\"distinct_poll_numbers\") > 1)\n",
					"    )\n",
					"    \n",
					"    # Count how many records need to be fixed\n",
					"    fix_count = df_with_flag.filter(col(\"needs_fix\")).count()\n",
					"    print(f\"Found {fix_count} records that need hash_id fixes\")\n",
					"    \n",
					"    # Fix the hash_ids by appending poll_number for records that need it\n",
					"    df_fixed = df_with_flag.withColumn(\n",
					"        \"hash_id\",\n",
					"        when(\n",
					"            col(\"needs_fix\"), \n",
					"            concat(col(\"hash_id\"), lit(\"_\"), col(\"poll_number\"))\n",
					"        ).otherwise(col(\"hash_id\"))\n",
					"    )\n",
					"    \n",
					"    # Drop the temporary columns\n",
					"    df_fixed = df_fixed.drop(\"hash_count\", \"distinct_poll_numbers\", \"needs_fix\")\n",
					"    \n",
					"    # Verify the fix worked - there should be no more duplicate hash_ids with different poll_numbers\n",
					"    final_check = df_fixed.groupBy(\"hash_id\").agg(\n",
					"        F.count(\"*\").alias(\"count\"),\n",
					"        F.countDistinct(\"poll_number\").alias(\"distinct_poll\")\n",
					"    ).filter(col(\"count\") > 1).filter(col(\"distinct_poll\") > 1)\n",
					"    \n",
					"    remaining_issues = final_check.count()\n",
					"    if remaining_issues > 0:\n",
					"        print(f\"WARNING: There are still {remaining_issues} hash_ids with multiple distinct poll numbers.\")\n",
					"        print(\"Sample of remaining issues:\")\n",
					"        final_check.show(5, truncate=False)\n",
					"    else:\n",
					"        print(\"✅ All duplicate hash_ids with different poll_numbers have been fixed!\")\n",
					"    \n",
					"    # Write the fixed data\n",
					"    print(f\"Writing fixed data to: {output_path}\")\n",
					"    \n",
					"    # If the data has a creation_date_partition column, use it for partitioning\n",
					"    if \"creation_date_partition\" in df_fixed.columns:\n",
					"        df_fixed.write \\\n",
					"            .partitionBy(\"creation_date_partition\") \\\n",
					"            .mode(\"overwrite\") \\\n",
					"            .parquet(output_path)\n",
					"        print(f\"Successfully wrote fixed data partitioned by creation_date_partition\")\n",
					"    else:\n",
					"        df_fixed.write \\\n",
					"            .mode(\"overwrite\") \\\n",
					"            .parquet(output_path)\n",
					"        print(f\"Successfully wrote fixed data (not partitioned)\")\n",
					"    \n",
					"    # Optional: If you want to replace the original data, you can rename directories\n",
					"    if mssparkutils.fs.exists(output_path):\n",
					"        backup_path = f\"{input_path}_backup_{int(time.time())}\"\n",
					"        print(f\"Backing up original data to: {backup_path}\")\n",
					"        mssparkutils.fs.mv(input_path, backup_path)\n",
					"        print(f\"Moving fixed data to original location: {input_path}\")\n",
					"        mssparkutils.fs.mv(output_path, input_path)\n",
					"        print(\"✅ Fixed data is now in the original location\")\n",
					"    \n",
					"    # Print summary using the stored count, not recomputing df.count()\n",
					"    print(\"\\nFix Summary:\")\n",
					"    print(f\"Total records processed: {total_record_count}\")\n",
					"    print(f\"Records with hash_id fixed: {fix_count}\")\n",
					"    print(f\"Remaining issues: {remaining_issues}\")\n",
					"    \n",
					"    return {\n",
					"        \"total_records\": total_record_count,\n",
					"        \"records_fixed\": fix_count,\n",
					"        \"remaining_issues\": remaining_issues\n",
					"    }\n",
					"\n",
					"# Run the fix function\n",
					"fix_duplicate_hash_ids()"
				],
				"execution_count": 4
			}
		]
	}
}