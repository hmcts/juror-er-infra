{
	"name": "L2_Test_CheckCreationDateExists",
	"properties": {
		"folder": {
			"name": "Testing/L2"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkstg",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "09e93eec-22fd-4218-ab53-f28df8420740"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/74dacd4f-a248-45bb-a2f0-af700dc4cf68/resourceGroups/baubais-data-factory-rg-stg/providers/Microsoft.Synapse/workspaces/baubais-synapse-stg/bigDataPools/sparkstg",
				"name": "sparkstg",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-stg.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkstg",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"\n",
					"def create_spark_session():\n",
					"    \"\"\"Create and return a Spark session\"\"\"\n",
					"    return SparkSession.builder \\\n",
					"        .appName(\"Check CreationDate\") \\\n",
					"        .getOrCreate()\n",
					"\n",
					"def check_files_for_creation_date(spark, transform_folder):\n",
					"    \"\"\"\n",
					"    Check all parquet files in the transform folder to see if they have a CreationDate column\n",
					"    \"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    print(f\"Checking files in {transform_folder} for CreationDate column\")\n",
					"    \n",
					"    # Find all parquet files\n",
					"    all_files = []\n",
					"    try:\n",
					"        print(\"Listing files in transformation folder...\")\n",
					"        items = mssparkutils.fs.ls(transform_folder)\n",
					"        for item in items:\n",
					"            if item.path.endswith(\".parquet\"):\n",
					"                all_files.append(item.path)\n",
					"            elif item.isDir:\n",
					"                # Check subdirectories\n",
					"                try:\n",
					"                    subitems = mssparkutils.fs.ls(item.path)\n",
					"                    for subitem in subitems:\n",
					"                        if subitem.path.endswith(\".parquet\"):\n",
					"                            all_files.append(subitem.path)\n",
					"                except Exception as e:\n",
					"                    print(f\"Error listing subfolder {item.path}: {str(e)}\")\n",
					"    except Exception as e:\n",
					"        print(f\"Error listing files: {str(e)}\")\n",
					"    \n",
					"    print(f\"Found {len(all_files)} parquet files to check\")\n",
					"    \n",
					"    # Check each file\n",
					"    results = []\n",
					"    files_with_creation_date = 0\n",
					"    files_without_creation_date = 0\n",
					"    \n",
					"    for file_path in all_files:\n",
					"        try:\n",
					"            # Read the schema only\n",
					"            df = spark.read.parquet(file_path)\n",
					"            columns = df.columns\n",
					"            \n",
					"            # Check if CreationDate exists\n",
					"            has_creation_date = \"CreationDate\" in columns\n",
					"            \n",
					"            # Get row count\n",
					"            row_count = df.count()\n",
					"            \n",
					"            if has_creation_date:\n",
					"                files_with_creation_date += 1\n",
					"            else:\n",
					"                files_without_creation_date += 1\n",
					"            \n",
					"            # Add to results\n",
					"            results.append({\n",
					"                \"file_path\": file_path,\n",
					"                \"has_creation_date\": has_creation_date,\n",
					"                \"row_count\": row_count,\n",
					"                \"total_columns\": len(columns),\n",
					"                \"columns\": \", \".join(columns)\n",
					"            })\n",
					"            \n",
					"            print(f\"Checked {file_path}: {'✓' if has_creation_date else '✗'} CreationDate, {row_count} rows, {len(columns)} columns\")\n",
					"            \n",
					"        except Exception as e:\n",
					"            print(f\"Error checking {file_path}: {str(e)}\")\n",
					"            results.append({\n",
					"                \"file_path\": file_path,\n",
					"                \"has_creation_date\": False,\n",
					"                \"row_count\": 0,\n",
					"                \"total_columns\": 0,\n",
					"                \"columns\": f\"ERROR: {str(e)}\"\n",
					"            })\n",
					"    \n",
					"    # Create summary\n",
					"    print(\"\\n===== SUMMARY =====\")\n",
					"    print(f\"Total files: {len(all_files)}\")\n",
					"    print(f\"Files WITH CreationDate: {files_with_creation_date}\")\n",
					"    print(f\"Files WITHOUT CreationDate: {files_without_creation_date}\")\n",
					"    \n",
					"    # Create a report dataframe\n",
					"    report_df = spark.createDataFrame(results)\n",
					"    \n",
					"    print(\"\\n===== FILES WITHOUT CREATIONDATE =====\")\n",
					"    missing_df = report_df.filter(~col(\"has_creation_date\"))\n",
					"    missing_df.select(\"file_path\", \"row_count\", \"total_columns\").show(truncate=False)\n",
					"    \n",
					"    return report_df\n",
					"\n",
					"if __name__ == \"__main__\":\n",
					"    # Paths\n",
					"    storage_account = \"baubaisadfsastg\"\n",
					"    transform_folder = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/transformation\"\n",
					"    \n",
					"    # Create Spark session\n",
					"    spark = create_spark_session()\n",
					"    \n",
					"    # Check files\n",
					"    report_df = check_files_for_creation_date(spark, transform_folder)\n",
					"    \n",
					"    # Save report if needed\n",
					"    output_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/file_check_report\"\n",
					"    report_df.write.mode(\"overwrite\").json(output_path)\n",
					"    print(f\"\\nDetailed report saved to {output_path}\")"
				],
				"execution_count": null
			}
		]
	}
}