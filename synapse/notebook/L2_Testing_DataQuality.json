{
	"name": "L2_Testing_DataQuality",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkstg",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "cb3f9f42-68b2-45d2-9d1d-0fc50c22b450"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/74dacd4f-a248-45bb-a2f0-af700dc4cf68/resourceGroups/baubais-data-factory-rg-stg/providers/Microsoft.Synapse/workspaces/baubais-synapse-stg/bigDataPools/sparkstg",
				"name": "sparkstg",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-stg.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkstg",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# Import necessary libraries\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col, lit, desc, when, coalesce, length\n",
					"from pyspark.sql.types import *\n",
					"import logging\n",
					"from notebookutils import mssparkutils"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"# Configure logging\n",
					"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"logger = logging.getLogger()\n",
					"\n",
					"# Create Spark session\n",
					"def create_spark_session():\n",
					"    return SparkSession.builder \\\n",
					"        .appName(\"Electoral Data Diagnostics\") \\\n",
					"        .getOrCreate()\n",
					"\n",
					"spark = create_spark_session()"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"source": [
					"# Path configuration - update these to match your actual paths\n",
					"storage_account = \"baubaisadfsastg\"\n",
					"input_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/transformation\"\n",
					"output_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_temp\"\n",
					"deduplicated_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_deduplicated\""
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# Improved data loading function with better error handling\n",
					"def safe_load_data(path, format_type=\"parquet\"):\n",
					"    \"\"\"Safely load data with proper error handling\"\"\"\n",
					"    try:\n",
					"        # Check if path exists\n",
					"        if not mssparkutils.fs.exists(path):\n",
					"            print(f\"Path does not exist: {path}\")\n",
					"            return None\n",
					"            \n",
					"        # List files in directory to check content\n",
					"        files = mssparkutils.fs.ls(path)\n",
					"        if len(files) == 0:\n",
					"            print(f\"No files found in: {path}\")\n",
					"            return None\n",
					"            \n",
					"        # Check for specific file types\n",
					"        if format_type == \"parquet\":\n",
					"            parquet_files = [f for f in files if f.name.endswith(\".parquet\") or f.name.endswith(\".snappy.parquet\")]\n",
					"            if len(parquet_files) == 0:\n",
					"                print(f\"No parquet files found in: {path}\")\n",
					"                # Try listing subdirectories\n",
					"                for f in files:\n",
					"                    if f.isDir:\n",
					"                        print(f\"Found directory: {f.path}, checking for parquet files...\")\n",
					"                        subfiles = mssparkutils.fs.ls(f.path)\n",
					"                        subparquet = [sf for sf in subfiles if sf.name.endswith(\".parquet\") or sf.name.endswith(\".snappy.parquet\")]\n",
					"                        if len(subparquet) > 0:\n",
					"                            print(f\"Found {len(subparquet)} parquet files in {f.path}\")\n",
					"                            return spark.read.parquet(f.path)\n",
					"                return None\n",
					"        \n",
					"        # Load the data\n",
					"        if format_type == \"parquet\":\n",
					"            return spark.read.parquet(path)\n",
					"        elif format_type == \"csv\":\n",
					"            return spark.read.option(\"header\", \"true\").csv(path)\n",
					"        else:\n",
					"            print(f\"Unsupported format type: {format_type}\")\n",
					"            return None\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"Error loading data from {path}: {str(e)}\")\n",
					"        import traceback\n",
					"        traceback.print_exc()\n",
					"        return None\n",
					"\n",
					"# Diagnostic functions remain the same as in the previous code...\n",
					"def analyze_data_quality(df, stage_name=\"initial\"):\n",
					"    # Same as before...\n",
					"    if df is None:\n",
					"        print(f\"\\n--- DATA QUALITY ANALYSIS ({stage_name}) ---\")\n",
					"        print(\"No data available for analysis\")\n",
					"        return\n",
					"    \n",
					"    print(f\"\\n--- DATA QUALITY ANALYSIS ({stage_name}) ---\")\n",
					"    print(f\"Total rows: {df.count()}\")\n",
					"    \n",
					"    # Check for null values in key fields\n",
					"    key_fields = ['Elector Number', 'LA_Code', 'Elector Surname', 'Elector Forename', 'Address1', 'PostCode',\n",
					"                 'poll_number', 'register_lett', 'lname', 'fname', 'address']\n",
					"    for field in key_fields:\n",
					"        if field in df.columns:\n",
					"            null_count = df.filter(col(field).isNull()).count()\n",
					"            print(f\"Null values in {field}: {null_count}\")\n",
					"    \n",
					"    # Rest of the function...\n",
					"\n",
					"# Other diagnostic functions remain the same...\n",
					"\n",
					"# Main diagnostic execution\n",
					"print(\"Starting diagnostic analysis...\")\n",
					"\n",
					"# List all paths to make sure we're looking in the right places\n",
					"print(\"\\n--- LISTING AVAILABLE PATHS ---\")\n",
					"print(f\"Input path: {input_path}\")\n",
					"try:\n",
					"    input_files = mssparkutils.fs.ls(input_path)\n",
					"    print(f\"Files found: {len(input_files)}\")\n",
					"    for f in input_files[:5]:  # Show first 5 files\n",
					"        print(f\"  - {f.name} ({'directory' if f.isDir else 'file'}) {f.size} bytes\")\n",
					"    if len(input_files) > 5:\n",
					"        print(f\"  ... and {len(input_files) - 5} more\")\n",
					"except Exception as e:\n",
					"    print(f\"Error listing input path: {str(e)}\")\n",
					"\n",
					"print(f\"\\nOutput path: {output_path}\")\n",
					"try:\n",
					"    output_files = mssparkutils.fs.ls(output_path)\n",
					"    print(f\"Files found: {len(output_files)}\")\n",
					"    for f in output_files[:5]:\n",
					"        print(f\"  - {f.name} ({'directory' if f.isDir else 'file'}) {f.size} bytes\")\n",
					"    if len(output_files) > 5:\n",
					"        print(f\"  ... and {len(output_files) - 5} more\")\n",
					"except Exception as e:\n",
					"    print(f\"Error listing output path: {str(e)}\")\n",
					"\n",
					"# Try to load data from different locations\n",
					"print(\"\\n--- ATTEMPTING TO LOAD DATA ---\")\n",
					"\n",
					"# Try to load transformed data\n",
					"transformed_df = safe_load_data(input_path)\n",
					"if transformed_df is not None:\n",
					"    print(f\"Successfully loaded transformed data: {transformed_df.count()} rows\")\n",
					"    print(f\"Columns: {transformed_df.columns}\")\n",
					"    \n",
					"    print(\"\\n======= TRANSFORMED DATA DIAGNOSTICS =======\")\n",
					"    analyze_data_quality(transformed_df, \"transformed\")\n",
					"    # Call other diagnostic functions...\n",
					"\n",
					"# Try additional paths if the main ones fail\n",
					"if transformed_df is None:\n",
					"    print(\"\\nTrying alternative paths for transformed data...\")\n",
					"    alt_paths = [\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/transformation/*\",\n",
					"        f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/mapping\"\n",
					"    ]\n",
					"    \n",
					"    for alt_path in alt_paths:\n",
					"        print(f\"Trying: {alt_path}\")\n",
					"        transformed_df = safe_load_data(alt_path)\n",
					"        if transformed_df is not None:\n",
					"            print(f\"Successfully loaded from alternative path: {transformed_df.count()} rows\")\n",
					"            break\n",
					"\n",
					"# Try to load final output data\n",
					"final_df = safe_load_data(output_path)\n",
					"if final_df is not None:\n",
					"    print(f\"Successfully loaded final data: {final_df.count()} rows\")\n",
					"    \n",
					"    print(\"\\n======= FINAL OUTPUT DATA DIAGNOSTICS =======\")\n",
					"    analyze_data_quality(final_df, \"final\")\n",
					"    # Call other diagnostic functions...\n",
					"\n",
					"# Load deduplicated data if available\n",
					"dedup_df = safe_load_data(deduplicated_path)\n",
					"if dedup_df is not None:\n",
					"    print(f\"Successfully loaded deduplicated data: {dedup_df.count()} rows\")\n",
					"    \n",
					"    print(\"\\n======= DEDUPLICATED DATA DIAGNOSTICS =======\")\n",
					"    analyze_data_quality(dedup_df, \"deduplicated\")\n",
					"    # Call other diagnostic functions...\n",
					"\n",
					"print(\"\\n--- DIAGNOSTICS COMPLETE ---\")"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"display(final_df)"
				],
				"execution_count": 10
			}
		]
	}
}