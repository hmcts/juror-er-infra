{
	"name": "L2_Testing_Mergingfiles",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkstg",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "fa1aeb50-203c-4630-9123-cfe6f205087d"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/74dacd4f-a248-45bb-a2f0-af700dc4cf68/resourceGroups/baubais-data-factory-rg-stg/providers/Microsoft.Synapse/workspaces/baubais-synapse-stg/bigDataPools/sparkstg",
				"name": "sparkstg",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-stg.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkstg",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Electoral Roll Data ETL Process\n",
					"\n",
					"This notebook processes electoral roll data from multiple sources, standardizes columns, removes duplicates, hashes sensitive information, and prepares data for PostgreSQL.\n",
					"\n",
					"### Function Sequence\n",
					"\n",
					"### Setup Functions\n",
					"\n",
					"`create_spark_session()`: Initializes Spark session with necessary configurations\n",
					"\n",
					"`load_config()`: Returns configuration including storage paths and database settings\n",
					"\n",
					"\n",
					"Configuration Processing\n",
					"\n",
					"read_column_mapping(spark, config_path): Reads schema mapping configuration\n",
					"standardize_columns(df, column_mapping): Standardizes column names based on mapping\n",
					"\n",
					"\n",
					"Incremental Data Processing\n",
					"\n",
					"get_new_files(spark, input_path, process_log_path): Identifies files that need processing\n",
					"process_input_files(spark, file_paths, column_mapping): Reads and standardizes files\n",
					"\n",
					"\n",
					"Data Transformation\n",
					"\n",
					"merge_dataframes(dfs): Combines standardized DataFrames\n",
					"remove_duplicates_by_extraction_date(df): Keeps only the latest record for each elector\n",
					"apply_hashing(df): Hashes sensitive fields for privacy\n",
					"transform_to_target_schema(df, column_mapping): Maps to PostgreSQL schema\n",
					"\n",
					"\n",
					"Output Processing\n",
					"\n",
					"merge_with_existing_data(spark, new_data, output_path): Updates existing data with new rec\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession, Window\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"import json\n",
					"import logging\n",
					"from datetime import datetime\n",
					"import os\n",
					"import re"
				],
				"execution_count": 62
			},
			{
				"cell_type": "code",
				"source": [
					"def configure_logging():\n",
					"    \"\"\"Set up proper logging with rotating file handler\"\"\"\n",
					"    log_format = '%(asctime)s [%(levelname)s] %(message)s'\n",
					"    logging.basicConfig(level=logging.INFO, format=log_format)\n",
					"    \n",
					"    # Add a handler that writes to a log file\n",
					"    file_handler = logging.FileHandler('electoral_etl.log')\n",
					"    file_handler.setFormatter(logging.Formatter(log_format))\n",
					"    \n",
					"    # Get the root logger and add the file handler\n",
					"    root_logger = logging.getLogger()\n",
					"    root_logger.addHandler(file_handler)\n",
					"    \n",
					"    return root_logger"
				],
				"execution_count": 63
			},
			{
				"cell_type": "code",
				"source": [
					"def create_spark_session():\n",
					"    \"\"\"Create and return a Spark session\"\"\"\n",
					"    return SparkSession.builder \\\n",
					"        .appName(\"Electoral Data ETL\") \\\n",
					"        .getOrCreate()"
				],
				"execution_count": 64
			},
			{
				"cell_type": "code",
				"source": [
					"def load_config():\n",
					"    \"\"\"Load configuration including storage paths\"\"\"\n",
					"    storage_account = \"baubaisadfsastg\"\n",
					"    return {\n",
					"        \"storage_account\": storage_account,\n",
					"        \"input_path\": f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/transformation\",\n",
					"        \"config_path\": f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/config/schema\",\n",
					"        \"output_path\": f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_temp\"\n",
					"    }"
				],
				"execution_count": 65
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Read Configuration and Mapping"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Column Mapping"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def read_column_mapping(spark, config_path):\n",
					"    \"\"\"Read and parse the column mapping configuration\"\"\"\n",
					"    try:\n",
					"        # Read the JSON file\n",
					"        mapping_df = spark.read.text(config_path)\n",
					"        \n",
					"        # Combine all lines into a single string\n",
					"        json_str = mapping_df.agg(concat_ws(\"\", collect_list(\"value\"))).collect()[0][0]\n",
					"        \n",
					"        # Parse the JSON string\n",
					"        mapping_dict = json.loads(json_str)\n",
					"        \n",
					"        # Check if it has 'schema' structure\n",
					"        if \"schema\" in mapping_dict:\n",
					"            # Convert schema format to mappings format\n",
					"            mappings = {}\n",
					"            for item in mapping_dict[\"schema\"]:\n",
					"                # Use the 'name' field as the standard column name\n",
					"                # Initially set empty variations list\n",
					"                mappings[item[\"name\"]] = []\n",
					"            \n",
					"            return mappings\n",
					"        \n",
					"        # If looking for mappings specifically\n",
					"        elif \"mappings\" in mapping_dict:\n",
					"            return mapping_dict[\"mappings\"]\n",
					"        \n",
					"        # If neither structure is found, raise an error\n",
					"        else:\n",
					"            raise KeyError(\"Neither 'schema' nor 'mappings' found in mapping file\")\n",
					"            \n",
					"    except Exception as e:\n",
					"        try:\n",
					"            # Fallback to using wholeTextFiles\n",
					"            mapping_rdd = spark.sparkContext.wholeTextFiles(config_path)\n",
					"            json_str = mapping_rdd.values().first()\n",
					"            mapping_dict = json.loads(json_str)\n",
					"            \n",
					"            # Check the structure here too\n",
					"            if \"schema\" in mapping_dict:\n",
					"                mappings = {}\n",
					"                for item in mapping_dict[\"schema\"]:\n",
					"                    mappings[item[\"name\"]] = []\n",
					"                return mappings\n",
					"            elif \"mappings\" in mapping_dict:\n",
					"                return mapping_dict[\"mappings\"]\n",
					"            else:\n",
					"                raise KeyError(\"Neither 'schema' nor 'mappings' found in mapping file\")\n",
					"                \n",
					"        except Exception as e2:\n",
					"            raise Exception(f\"Failed to read mapping file. Primary error: {str(e)}, Fallback error: {str(e2)}\")"
				],
				"execution_count": 66
			},
			{
				"cell_type": "code",
				"source": [
					"def standardize_columns(df, column_mapping):\n",
					"    \"\"\"\n",
					"    Standardize column names based on mapping\n",
					"    \n",
					"    Args:\n",
					"        df: Input DataFrame\n",
					"        column_mapping: Dictionary mapping standard names to variations\n",
					"        \n",
					"    Returns:\n",
					"        DataFrame with standardized column names\n",
					"    \"\"\"\n",
					"    # Get current columns\n",
					"    current_columns = df.columns\n",
					"    \n",
					"    # Create a reverse mapping (from variations to standard names)\n",
					"    reverse_mapping = {}\n",
					"    for standard_name, variations in column_mapping.items():\n",
					"        # Add the standard name itself to the reverse mapping\n",
					"        reverse_mapping[standard_name.lower()] = standard_name\n",
					"        \n",
					"        # Add all variations\n",
					"        for variation in variations:\n",
					"            reverse_mapping[variation.lower()] = standard_name\n",
					"    \n",
					"    # Apply renaming\n",
					"    for col in current_columns:\n",
					"        # Check if column matches any standard name or variation (case-insensitive)\n",
					"        std_name = reverse_mapping.get(col.lower())\n",
					"        if std_name and col != std_name:\n",
					"            df = df.withColumnRenamed(col, std_name)\n",
					"    \n",
					"    return df\n",
					"def standardize_columns(df, column_mapping):\n",
					"    \"\"\"\n",
					"    Standardize column names based on mapping\n",
					"    \n",
					"    Args:\n",
					"        df: Input DataFrame\n",
					"        column_mapping: Dictionary mapping standard names to variations\n",
					"        \n",
					"    Returns:\n",
					"        DataFrame with standardized column names\n",
					"    \"\"\"\n",
					"    # Get current columns\n",
					"    current_columns = df.columns\n",
					"    \n",
					"    # Create a reverse mapping (from variations to standard names)\n",
					"    reverse_mapping = {}\n",
					"    for standard_name, variations in column_mapping.items():\n",
					"        # Add the standard name itself to the reverse mapping\n",
					"        reverse_mapping[standard_name.lower()] = standard_name\n",
					"        \n",
					"        # Add all variations\n",
					"        for variation in variations:\n",
					"            reverse_mapping[variation.lower()] = standard_name\n",
					"    \n",
					"    # Apply renaming\n",
					"    for col in current_columns:\n",
					"        # Check if column matches any standard name or variation (case-insensitive)\n",
					"        std_name = reverse_mapping.get(col.lower())\n",
					"        if std_name and col != std_name:\n",
					"            df = df.withColumnRenamed(col, std_name)\n",
					"    \n",
					"    return df\n",
					"\n",
					"def read_schema_config(spark, config_path):\n",
					"    \"\"\"Read and parse the schema configuration\"\"\"\n",
					"    try:\n",
					"        schema_df = spark.read.text(config_path)\n",
					"        json_str = schema_df.agg(concat_ws(\"\", collect_list(\"value\"))).collect()[0][0]\n",
					"        schema_dict = json.loads(json_str)\n",
					"        return schema_dict[\"schema\"]\n",
					"    except Exception as e:\n",
					"        try:\n",
					"            schema_rdd = spark.sparkContext.wholeTextFiles(config_path)\n",
					"            json_str = schema_rdd.values().first()\n",
					"            schema_dict = json.loads(json_str)\n",
					"            return schema_dict[\"schema\"]\n",
					"        except Exception as e2:\n",
					"            raise Exception(f\"Failed to read schema file. Primary error: {str(e)}, Fallback error: {str(e2)}\")"
				],
				"execution_count": 67
			},
			{
				"cell_type": "code",
				"source": [
					"def get_new_files(spark, input_path, process_log_path):\n",
					"    \"\"\"Identify which files are new and need processing\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    # Get all files in the input directory\n",
					"    all_files = []\n",
					"    for item in mssparkutils.fs.ls(input_path):\n",
					"        if item.path.endswith(\".parquet\"):\n",
					"            all_files.append(item.path)\n",
					"    \n",
					"    # Read process log to get previously processed files\n",
					"    if mssparkutils.fs.exists(process_log_path):\n",
					"        processed_files = spark.read.parquet(process_log_path) \\\n",
					"                              .select(\"file_path\").rdd.flatMap(lambda x: x).collect()\n",
					"    else:\n",
					"        processed_files = []\n",
					"    \n",
					"    # Identify new files\n",
					"    new_files = [f for f in all_files if f not in processed_files]\n",
					"    \n",
					"    return new_files, all_files"
				],
				"execution_count": 68
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def process_input_files(spark, file_paths, column_mapping):\n",
					"    \"\"\"Read and standardize columns from input files\"\"\"\n",
					"    processed_dfs = []\n",
					"    \n",
					"    for file_path in file_paths:\n",
					"        try:\n",
					"            # Read the file\n",
					"            df = spark.read.parquet(file_path)\n",
					"            \n",
					"            # Standardize column names based on mapping\n",
					"            standardized_df = standardize_columns(df, column_mapping)\n",
					"            \n",
					"            # Add source metadata\n",
					"            standardized_df = standardized_df.withColumn(\"source_file\", lit(file_path))\n",
					"            standardized_df = standardized_df.withColumn(\"process_timestamp\", current_timestamp())\n",
					"            \n",
					"            processed_dfs.append(standardized_df)\n",
					"            \n",
					"        except Exception as e:\n",
					"            print(f\"Error processing {file_path}: {str(e)}\")\n",
					"    \n",
					"    return processed_dfs"
				],
				"execution_count": 69
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Merge Files with Standardized Schema"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def merge_dataframes(dfs):\n",
					"    \"\"\"Merge all dataframes with standardized schema, preserving original CreationDate\"\"\"\n",
					"    if not dfs:\n",
					"        raise ValueError(\"No dataframes to merge\")\n",
					"        \n",
					"    # Merge using unionByName to handle schema variations\n",
					"    merged_df = dfs[0]\n",
					"    for df in dfs[1:]:\n",
					"        merged_df = merged_df.unionByName(df, allowMissingColumns=True)\n",
					"    \n",
					"    # Ensure CreationDate isn't being overwritten\n",
					"    # Check if CreationDate exists but is all the same value\n",
					"    distinct_dates = merged_df.select(\"CreationDate\").distinct().count()\n",
					"    if distinct_dates == 1:\n",
					"        # If only one date exists, it might have been overwritten\n",
					"        logging.warning(\"Only one distinct CreationDate found in merged data - this may indicate a problem!\")\n",
					"        \n",
					"        # Check if original date information is available in other columns or file metadata\n",
					"        if \"source_file\" in merged_df.columns:\n",
					"            # Extract date from source_file path if possible\n",
					"            merged_df = merged_df.withColumn(\n",
					"                \"extracted_date\",\n",
					"                regexp_extract(col(\"source_file\"), r\"(\\d{8})_\", 1)\n",
					"            )\n",
					"            \n",
					"            # Use extracted date if available, otherwise keep existing\n",
					"            merged_df = merged_df.withColumn(\n",
					"                \"CreationDate\",\n",
					"                when(col(\"extracted_date\").isNotNull(), col(\"extracted_date\"))\n",
					"                .otherwise(col(\"CreationDate\"))\n",
					"            )\n",
					"            \n",
					"            # Clean up temporary column\n",
					"            merged_df = merged_df.drop(\"extracted_date\")\n",
					"            \n",
					"            logging.info(f\"After date correction: {merged_df.select('CreationDate').distinct().count()} distinct dates\")\n",
					"    \n",
					"    return merged_df"
				],
				"execution_count": 70
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Deduplicate Based on Latest Data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def remove_duplicates_by_extraction_date(df):\n",
					"    \"\"\"Keep only the latest record for each unique individual\"\"\"\n",
					"    # Use your window partitioning approach with enhanced identity matching\n",
					"    window_spec = Window.partitionBy(\n",
					"        \"Elector Number\", \n",
					"        \"LA_Code\",\n",
					"        \"Elector Forename\",\n",
					"        \"Elector Surname\"\n",
					"    ).orderBy(desc(\"CreationDate\"))\n",
					"    \n",
					"    deduplicated = df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                     .filter(col(\"row_num\") == 1) \\\n",
					"                     .drop(\"row_num\")\n",
					"                     \n",
					"    logging.info(f\"Deduplication: {df.count()} rows before, {deduplicated.count()} rows after\")\n",
					"    return deduplicated"
				],
				"execution_count": 71
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Apply Hashing for Sensitive Data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def apply_hashing_to_voters(df):\n",
					"    \"\"\"Apply hashing specifically for voters_temp table column structure\"\"\"\n",
					"    # Create hash_id from original values\n",
					"    hashed_df = df.withColumn(\n",
					"        \"hash_id\",\n",
					"        xxhash64(concat_ws(\",\",\n",
					"            coalesce(col(\"fname\"), lit(\"\")),\n",
					"            coalesce(col(\"lname\"), lit(\"\")),\n",
					"            coalesce(col(\"dob\"), lit(\"\")),\n",
					"            coalesce(col(\"address\"), lit(\"\")),\n",
					"            coalesce(col(\"creation_date\"), lit(\"\"))\n",
					"        ))\n",
					"    )\n",
					"    \n",
					"    # Create backup columns with original values\n",
					"    hashed_df = hashed_df.withColumn(\"original_fname\", col(\"fname\"))\n",
					"    hashed_df = hashed_df.withColumn(\"original_lname\", col(\"lname\"))\n",
					"    hashed_df = hashed_df.withColumn(\"original_dob\", col(\"dob\"))\n",
					"    hashed_df = hashed_df.withColumn(\"original_address\", col(\"address\"))\n",
					"    \n",
					"    # Hash the sensitive fields\n",
					"    hashed_df = hashed_df.withColumn(\"fname\", xxhash64(col(\"fname\")))\n",
					"    hashed_df = hashed_df.withColumn(\"lname\", xxhash64(col(\"lname\")))\n",
					"    hashed_df = hashed_df.withColumn(\"dob\", xxhash64(col(\"dob\")))\n",
					"    hashed_df = hashed_df.withColumn(\"address\", xxhash64(col(\"address\")))\n",
					"    \n",
					"    return hashed_df"
				],
				"execution_count": 72
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Transform to Target Schema"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def transform_to_target_schema(df, column_mapping):\n",
					"    \"\"\"\n",
					"    Transform the dataframe to match the target PostgreSQL schema\n",
					"    \n",
					"    Args:\n",
					"        df: Input DataFrame with standardized column names\n",
					"        column_mapping: Dictionary mapping standard names to variations\n",
					"        \n",
					"    Returns:\n",
					"        DataFrame with target schema for PostgreSQL\n",
					"    \"\"\"\n",
					"    print(f\"Starting transformation with DataFrame containing {df.count()} rows\")\n",
					"    print(f\"Available columns: {df.columns}\")\n",
					"    \n",
					"    try:\n",
					"        # Helper function to safely access columns\n",
					"        def safe_col(column_name):\n",
					"            if column_name in df.columns:\n",
					"                return col(column_name)\n",
					"            else:\n",
					"                print(f\"Warning: Column '{column_name}' not found in dataframe\")\n",
					"                return lit(None)\n",
					"        \n",
					"        # Create the target schema DataFrame\n",
					"        transformed_df = df.select(\n",
					"            # loc_code - blank (will be populated by another process)\n",
					"            lit(None).cast(\"string\").alias(\"loc_code\"),\n",
					"            \n",
					"            # part_no - blank for now, will be generated by PostgreSQL\n",
					"            lit(None).cast(\"string\").alias(\"part_no\"),\n",
					"            \n",
					"            # register_lett - use the Elector Number directly\n",
					"            when(\n",
					"                col(\"Elector Number\").isNotNull(),\n",
					"                col(\"Elector Number\").cast(\"int\")\n",
					"            ).otherwise(lit(None)).alias(\"register_lett\"),\n",
					"            \n",
					"            # poll_number - also from Elector Number (same as register_lett)\n",
					"            when(\n",
					"                col(\"Elector Number\").isNotNull(),\n",
					"                col(\"Elector Number\").cast(\"int\")\n",
					"            ).otherwise(lit(None)).alias(\"poll_number\"),\n",
					"            \n",
					"            # Rest of the columns remain the same\n",
					"            safe_col(\"Elector Markers\").alias(\"new_marker\"),\n",
					"            safe_col(\"Elector Title\").alias(\"title\"),\n",
					"            safe_col(\"Elector Surname\").alias(\"lname\"),\n",
					"            safe_col(\"Elector Forename\").alias(\"fname\"),\n",
					"            safe_col(\"Elector DOB\").alias(\"dob\"),\n",
					"            when(\n",
					"                col(\"Flags/Markers\").isNotNull(), \n",
					"                col(\"Flags/Markers\")\n",
					"            ).otherwise(lit(None)).alias(\"flags\"),\n",
					"            safe_col(\"Address1\").alias(\"address\"),\n",
					"            safe_col(\"Address2\").alias(\"address2\"),\n",
					"            safe_col(\"Address3\").alias(\"address3\"),\n",
					"            safe_col(\"Address4\").alias(\"address4\"),\n",
					"            safe_col(\"Address5\").alias(\"address5\"),\n",
					"            safe_col(\"Address6\").alias(\"address6\"),\n",
					"            safe_col(\"PostCode\").alias(\"zip\"),\n",
					"            lit(None).cast(\"timestamp\").alias(\"date_selected1\"),\n",
					"            lit(None).cast(\"timestamp\").alias(\"date_selected2\"),\n",
					"            lit(None).cast(\"timestamp\").alias(\"date_selected3\"),\n",
					"            \n",
					"            # rec_num - also set to Elector Number for consistency\n",
					"            when(\n",
					"                col(\"LA_Code\").isNotNull(),\n",
					"                regexp_replace(col(\"LA_Code\"), \"[^0-9]\", \"\").cast(\"int\")\n",
					"            ).otherwise(lit(None)).alias(\"rec_num\"),\n",
					"            \n",
					"            lit(None).cast(\"string\").alias(\"perm_disqual\"),\n",
					"            lit(None).cast(\"string\").alias(\"source_id\"),\n",
					"            when(\n",
					"                col(\"PostCode\").isNotNull(),\n",
					"                split(trim(col(\"PostCode\")), \" \")[0]\n",
					"            ).otherwise(lit(None)).alias(\"postcode_start\"),\n",
					"            when(\n",
					"                col(\"hash_id\").isNotNull(),\n",
					"                col(\"hash_id\")\n",
					"            ).otherwise(lit(None)).alias(\"hash_id\"),\n",
					"            when(\n",
					"                col(\"CreationDate\").isNotNull(),\n",
					"                col(\"CreationDate\")\n",
					"            ).otherwise(date_format(current_timestamp(), \"yyyyMMdd\")).alias(\"creation_date\")\n",
					"        )\n",
					"        \n",
					"        print(f\"Transformation complete. Result has {transformed_df.count()} rows and {len(transformed_df.columns)} columns\")\n",
					"        print(f\"Result columns: {transformed_df.columns}\")\n",
					"        \n",
					"        return transformed_df\n",
					"        \n",
					"    except Exception as e:\n",
					"        # Error handling as before\n",
					"        print(f\"Error in transform_to_target_schema: {str(e)}\")\n",
					"        import traceback\n",
					"        print(traceback.format_exc())\n",
					"        \n",
					"         # Create a minimal valid DataFrame instead of returning None\n",
					"        print(\"Creating minimal valid DataFrame to avoid pipeline failure\")\n",
					"        columns = [\n",
					"            \"loc_code\", \"part_no\", \"register_lett\", \"poll_number\",\n",
					"            \"new_marker\", \"title\", \"lname\", \"fname\", \"dob\", \"flags\",\n",
					"            \"address\", \"address2\", \"address3\", \"address4\", \"address5\", \n",
					"            \"address6\", \"zip\", \"date_selected1\", \"date_selected2\", \n",
					"            \"date_selected3\", \"rec_num\", \"perm_disqual\", \"source_id\",\n",
					"            \"postcode_start\", \"hash_id\", \"creation_date\"\n",
					"        ]\n",
					"        \n",
					"        # Create empty DataFrame with the right structure\n",
					"        schema = StructType([StructField(c, StringType(), True) for c in columns])\n",
					"        empty_df = spark.createDataFrame([], schema)\n",
					"        \n",
					"        return empty_df"
				],
				"execution_count": 73
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Merge with Existing Data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def merge_with_existing_data(spark, new_data, output_path):\n",
					"    \"\"\"Merge new data with existing data using proper incremental logic\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    # First, ensure the output directory exists\n",
					"    try:\n",
					"        if not mssparkutils.fs.exists(output_path):\n",
					"            # Extract the directory path\n",
					"            dir_path = output_path\n",
					"            if not dir_path.endswith('/'):\n",
					"                # If path points to a file, get the directory\n",
					"                dir_path = '/'.join(dir_path.split('/')[:-1])\n",
					"                \n",
					"            print(f\"Output path {output_path} does not exist yet. Creating directory.\")\n",
					"            mssparkutils.fs.mkdirs(dir_path)\n",
					"            return new_data  # No existing data to merge with\n",
					"    except Exception as e:\n",
					"        print(f\"Error while checking/creating output path: {str(e)}\")\n",
					"        print(\"Continuing with only new data\")\n",
					"        return new_data\n",
					"        \n",
					"    # If we get here, the directory exists, now check if there's data to read\n",
					"    try:\n",
					"        # Try to list files in the directory to see if there's actual data\n",
					"        files = mssparkutils.fs.ls(output_path)\n",
					"        parquet_files = [f for f in files if f.path.endswith('.parquet')]\n",
					"        \n",
					"        if len(parquet_files) > 0:\n",
					"            # There are parquet files, try to read them\n",
					"            try:\n",
					"                existing_data = spark.read.parquet(output_path)\n",
					"                \n",
					"                # Create a view of the new data\n",
					"                new_data.createOrReplaceTempView(\"new_data\")\n",
					"                existing_data.createOrReplaceTempView(\"existing_data\")\n",
					"        \n",
					"                # Perform a merge operation to update existing records and add new ones\n",
					"                merged_data = spark.sql(\"\"\"\n",
					"                    -- Part 1: Get all new records that don't exist in the current data\n",
					"                    SELECT n.* \n",
					"                    FROM new_data n\n",
					"                    LEFT ANTI JOIN existing_data e\n",
					"                    ON n.poll_number = e.poll_number AND n.register_lett = e.register_lett\n",
					"                    \n",
					"                    UNION ALL\n",
					"                    \n",
					"                    -- Part 2: Get all existing records, updating them with new data if available\n",
					"                    SELECT \n",
					"                        COALESCE(n.loc_code, e.loc_code) as loc_code,\n",
					"                        COALESCE(n.part_no, e.part_no) as part_no,\n",
					"                        COALESCE(n.register_lett, e.register_lett) as register_lett,\n",
					"                        COALESCE(n.poll_number, e.poll_number) as poll_number,\n",
					"                        COALESCE(n.new_marker, e.new_marker) as new_marker,\n",
					"                        COALESCE(n.title, e.title) as title,\n",
					"                        COALESCE(n.lname, e.lname) as lname,\n",
					"                        COALESCE(n.fname, e.fname) as fname,\n",
					"                        COALESCE(n.dob, e.dob) as dob,\n",
					"                        COALESCE(n.flags, e.flags) as flags,\n",
					"                        COALESCE(n.address, e.address) as address,\n",
					"                        COALESCE(n.address2, e.address2) as address2,\n",
					"                        COALESCE(n.address3, e.address3) as address3,\n",
					"                        COALESCE(n.address4, e.address4) as address4,\n",
					"                        COALESCE(n.address5, e.address5) as address5,\n",
					"                        COALESCE(n.address6, e.address6) as address6,\n",
					"                        COALESCE(n.zip, e.zip) as zip,\n",
					"                        COALESCE(n.date_selected1, e.date_selected1) as date_selected1,\n",
					"                        COALESCE(n.date_selected2, e.date_selected2) as date_selected2,\n",
					"                        COALESCE(n.date_selected3, e.date_selected3) as date_selected3,\n",
					"                        COALESCE(n.rec_num, e.rec_num) as rec_num,\n",
					"                        COALESCE(n.perm_disqual, e.perm_disqual) as perm_disqual,\n",
					"                        COALESCE(n.source_id, e.source_id) as source_id,\n",
					"                        COALESCE(n.postcode_start, e.postcode_start) as postcode_start,\n",
					"                        COALESCE(n.hash_id, e.hash_id) as hash_id,\n",
					"                        COALESCE(n.creation_date, e.creation_date) as creation_date\n",
					"                    FROM existing_data e\n",
					"                    LEFT JOIN new_data n\n",
					"                    ON e.poll_number = n.poll_number AND e.register_lett = n.register_lett\n",
					"                    WHERE n.poll_number IS NULL OR n.creation_date > e.creation_date\n",
					"                \"\"\")\n",
					"                \n",
					"            except Exception as e:\n",
					"                print(f\"Error reading existing parquet files: {str(e)}\")\n",
					"                return new_data\n",
					"        else:\n",
					"            print(f\"No existing parquet files found in {output_path}. First-time run.\")\n",
					"            return new_data\n",
					"            \n",
					"    except Exception as e:\n",
					"        print(f\"Error listing files in output directory: {str(e)}\")\n",
					"    return new_data"
				],
				"execution_count": 74
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Output and Update Process Log"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"def write_output_and_update_log(spark, merged_data, output_path, new_file_paths, process_log_path):\n",
					"    \"\"\"Write the merged data to output location and update the process log\"\"\"\n",
					"    logger = logging.getLogger()\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    try:\n",
					"        # Ensure output directory exists\n",
					"        dir_path = output_path\n",
					"        if '.' in output_path.split('/')[-1]:  # Check if last part looks like a filename\n",
					"            dir_path = '/'.join(output_path.split('/')[:-1])\n",
					"            \n",
					"        if not mssparkutils.fs.exists(dir_path):\n",
					"            logger.info(f\"Creating output directory: {dir_path}\")\n",
					"            mssparkutils.fs.mkdirs(dir_path)\n",
					"        \n",
					"        # Write the merged data\n",
					"        logger.info(f\"Writing {merged_data.count()} records to {output_path}\")\n",
					"        merged_data.write.mode(\"overwrite\").parquet(output_path)\n",
					"        logger.info(\"Successfully wrote data to output location\")\n",
					"        \n",
					"        # Rest of function remains the same...\n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error writing output or updating log: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return False"
				],
				"execution_count": 75
			},
			{
				"cell_type": "code",
				"source": [
					"def update_process_log(spark, new_file_paths, process_log_path):\n",
					"    \"\"\"Update the processing log with newly processed files\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    from pyspark.sql.types import StringType, TimestampType, StructType, StructField\n",
					"    \n",
					"    try:\n",
					"        # Create schema with explicit types\n",
					"        log_schema = StructType([\n",
					"            StructField(\"file_path\", StringType(), False),\n",
					"            StructField(\"process_timestamp\", TimestampType(), False)\n",
					"        ])\n",
					"        \n",
					"        # Get current timestamp, properly formatted\n",
					"        timestamp_now = datetime.now()\n",
					"        \n",
					"        # Create new log entries with explicit timestamp\n",
					"        new_log_entries = [(file_path, timestamp_now) for file_path in new_file_paths]\n",
					"        \n",
					"        # Create DataFrame with explicit schema\n",
					"        new_log_df = spark.createDataFrame(new_log_entries, log_schema)\n",
					"        \n",
					"        # Update existing log if it exists\n",
					"        if mssparkutils.fs.exists(process_log_path):\n",
					"            try:\n",
					"                existing_log = spark.read.parquet(process_log_path)\n",
					"                # Ensure the timestamp field has the correct type\n",
					"                existing_log = existing_log.withColumn(\"process_timestamp\", \n",
					"                                                     col(\"process_timestamp\").cast(\"timestamp\"))\n",
					"                updated_log = existing_log.union(new_log_df)\n",
					"            except Exception as e:\n",
					"                print(f\"Error reading existing log: {str(e)}. Creating new log.\")\n",
					"                updated_log = new_log_df\n",
					"        else:\n",
					"            updated_log = new_log_df\n",
					"            \n",
					"        # Write updated log\n",
					"        updated_log.write.mode(\"overwrite\").parquet(process_log_path)\n",
					"        \n",
					"        return True\n",
					"    except Exception as e:\n",
					"        print(f\"Error updating process log: {str(e)}\")\n",
					"        import traceback\n",
					"        print(traceback.format_exc())\n",
					"        return False"
				],
				"execution_count": 76
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Main Pipeline Function"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def main():\n",
					"    \"\"\"Main function to run the incremental load process\"\"\"\n",
					"    # Initialize\n",
					"    print(\"Starting electoral data ETL process\")\n",
					"    print(f\"Current time: {datetime.now()}\")\n",
					"    \n",
					"    # Configure logging and capture the logger\n",
					"    logger = configure_logging()\n",
					"    print(\"Logging configured successfully\")\n",
					"    logger.info(\"Logging configured successfully\")\n",
					"    \n",
					"    spark = create_spark_session()\n",
					"    config = load_config()\n",
					"    storage_account = config[\"storage_account\"]\n",
					"    process_log_path = f\"{config['output_path']}_process_log\"\n",
					"    \n",
					"    print(f\"Looking for files in: {config['input_path']}\")\n",
					"    \n",
					"    try:\n",
					"        # 1. Get configuration and mapping\n",
					"        column_mapping = read_column_mapping(spark, f\"{config['config_path']}/col_mapping_schema.json\")\n",
					"        \n",
					"        # 2. Identify new files\n",
					"        new_file_paths, all_file_paths = get_new_files(spark, config[\"input_path\"], process_log_path)\n",
					"        \n",
					"        if not new_file_paths:\n",
					"            logger.info(\"No new files to process\")\n",
					"            return\n",
					"        \n",
					"        logger.info(f\"Found {len(new_file_paths)} new files to process\")\n",
					"        \n",
					"        # 3. Process new files\n",
					"        processed_dfs = process_input_files(spark, new_file_paths, column_mapping)\n",
					"        \n",
					"        if not processed_dfs or len(processed_dfs) == 0:\n",
					"            logger.error(\"No data to process after reading files\")\n",
					"            return\n",
					"        \n",
					"        logger.info(f\"Successfully processed {len(processed_dfs)} DataFrames\")\n",
					"        \n",
					"        # 4. Merge new data - ADD ERROR CHECKING HERE\n",
					"        merged_new_data = merge_dataframes(processed_dfs)\n",
					"        if merged_new_data is None:\n",
					"            logger.error(\"Merged DataFrame is None. Check merge_dataframes function\")\n",
					"            return\n",
					"        \n",
					"        logger.info(f\"Merged data contains {merged_new_data.count()} rows\")\n",
					"        \n",
					"        # 5. Deduplicate new data - ADD ERROR CHECKING HERE\n",
					"        deduplicated_data = remove_duplicates_by_extraction_date(merged_new_data)\n",
					"        if deduplicated_data is None:\n",
					"            logger.error(\"Deduplicated DataFrame is None. Check remove_duplicates function\")\n",
					"            return\n",
					"        \n",
					"        logger.info(f\"After deduplication: {deduplicated_data.count()} rows\")\n",
					"        \n",
					"        # 6. Apply hashing - ADD ERROR CHECKING HERE\n",
					"        hashed_data = apply_hashing(deduplicated_data)\n",
					"        if hashed_data is None:\n",
					"            logger.error(\"Hashed DataFrame is None. Check apply_hashing function\")\n",
					"            return\n",
					"        \n",
					"        # 7. Transform to target schema - ADD ERROR CHECKING HERE\n",
					"        transformed_data = transform_to_target_schema(hashed_data, column_mapping)\n",
					"        if transformed_data is None:\n",
					"            logger.error(\"Transformed DataFrame is None. Check transform_to_target_schema function\")\n",
					"            return\n",
					"        \n",
					"        logger.info(f\"Transformed data contains {transformed_data.count()} rows\")\n",
					"        \n",
					"        # 8. Merge with existing data - ADD ERROR CHECKING HERE\n",
					"        final_data = merge_with_existing_data(spark, transformed_data, config[\"output_path\"])\n",
					"        if final_data is None:\n",
					"            logger.error(\"Final DataFrame is None. Check merge_with_existing_data function\")\n",
					"            return\n",
					"\n",
					"        # reapply hashing after merging to ensure hash reflects current data\n",
					"        final_data = final_data.withColumn(\n",
					"            \"hash_id\",\n",
					"            xxhash64(concat_ws(\",\", \n",
					"                coalesce(col(\"fname\"), lit(\"\")),  # Using the target schema column names\n",
					"                coalesce(col(\"lname\"), lit(\"\")),\n",
					"                coalesce(col(\"dob\"), lit(\"\")),\n",
					"                coalesce(col(\"address\"), lit(\"\"))                \n",
					"            ))\n",
					"        )\n",
					"        \n",
					"        # After deduplication has been applied\n",
					"        deduplicated_data = remove_duplicates_by_extraction_date(merged_new_data)\n",
					"        \n",
					"        \n",
					"        # Transform to target schema if needed\n",
					"        transformed_data = transform_to_target_schema(deduplicated_data, column_mapping)\n",
					"        \n",
					"        # Write final deduplicated dataset to a clearly named location\n",
					"        final_output_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_deduplicated\"\n",
					"        transformed_data.write.mode(\"overwrite\").parquet(final_output_path)\n",
					"        \n",
					"        # For a large dataset (40M rows), let Spark determine partitioning\n",
					"        transformed_data.write.mode(\"overwrite\").parquet(final_output_path)\n",
					"        \n",
					"        # Log details for documentation\n",
					"        logging.info(f\"Created deduplicated dataset with {transformed_data.count()} rows at {final_output_path}\")\n",
					"        logging.info(f\"Dataset partitioned across multiple files for performance\")\n",
					"        \n",
					"        # 9. Write output and update log\n",
					"        logger.info(f\"Writing {final_data.count()} records to {config['output_path']}\")\n",
					"        \n",
					"        # Call the write_output_and_update_log function\n",
					"        success = write_output_and_update_log(\n",
					"            spark,\n",
					"            final_data,\n",
					"            config[\"output_path\"],\n",
					"            new_file_paths,\n",
					"            process_log_path\n",
					"        )\n",
					"        \n",
					"        if success:\n",
					"            logger.info(f\"Successfully processed {len(new_file_paths)} files\")\n",
					"            logger.info(f\"Total records in output: {final_data.count()}\")\n",
					"        else:\n",
					"            logger.error(\"Failed to write output or update log\")\n",
					"            \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error in incremental load process: {str(e)}\")\n",
					"        # Print stack trace for better debugging\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        raise\n",
					"\n",
					"if __name__ == \"__main__\":\n",
					"    main()"
				],
				"execution_count": 77
			}
		]
	}
}