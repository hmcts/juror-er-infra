{
	"name": "L2_Testing_Mergingfiles",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkstg",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b598ef77-a125-4552-9049-8fdc7b18d93d"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/74dacd4f-a248-45bb-a2f0-af700dc4cf68/resourceGroups/baubais-data-factory-rg-stg/providers/Microsoft.Synapse/workspaces/baubais-synapse-stg/bigDataPools/sparkstg",
				"name": "sparkstg",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-stg.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkstg",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Implementation Sequence\n",
					"\n",
					"Start with Setup Functions\n",
					"\n",
					"configure_logging()\n",
					"load_config()\n",
					"create_spark_session() \n",
					"\n",
					"Column Mapping Functions\n",
					"\n",
					"normalize_column_name()\n",
					"get_standard_column_map()\n",
					"Replace or modify your existing read_column_mapping() function\n",
					"\n",
					"\n",
					"Data Processing Core Functions\n",
					"\n",
					"standardize_columns() (implement this to replace process_df_with_mapping())\n",
					"process_parquet_files()\n",
					"\n",
					"\n",
					"Data Transformation Functions\n",
					"\n",
					"merge_dataframes()\n",
					"remove_duplicates() (implement this to handle deduplication)\n",
					"transform_to_target_schema()\n",
					"\n",
					"\n",
					"Validation and Quality Control\n",
					"\n",
					"validate_output_schema()\n",
					"\n",
					"\n",
					"Main Function\n",
					"\n",
					"Rewrite your main function to use all the new components"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession, Window\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"import json\n",
					"import logging\n",
					"from datetime import datetime\n",
					"import os\n",
					"import re"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def configure_logging():\n",
					"    \"\"\"Set up proper logging with rotating file handler\"\"\"\n",
					"    log_format = '%(asctime)s [%(levelname)s] %(message)s'\n",
					"    logging.basicConfig(level=logging.INFO, format=log_format)\n",
					"    \n",
					"    # Add a handler that writes to a log file\n",
					"    file_handler = logging.FileHandler('electoral_etl.log')\n",
					"    file_handler.setFormatter(logging.Formatter(log_format))\n",
					"    \n",
					"    # Get the root logger and add the file handler\n",
					"    root_logger = logging.getLogger()\n",
					"    root_logger.addHandler(file_handler)\n",
					"    \n",
					"    return root_logger"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def load_config():\n",
					"    \"\"\"Load configuration from environment or config file\"\"\"\n",
					"    return {\n",
					"        \"storage_account\": \"baubaisadfsastg\",\n",
					"        \"input_paths\": {\n",
					"            \"transform_folder\": \"abfss://juror-etl@baubaisadfsastg.dfs.core.windows.net/transformation\",\n",
					"            \"config_folder\": \"abfss://juror-etl@baubaisadfsastg.dfs.core.windows.net/config/schema\"\n",
					"        },\n",
					"        \"output_paths\": {\n",
					"            \"voters_temp\": \"abfss://juror-etl@baubaisadfsastg.dfs.core.windows.net/voters_temp\"\n",
					"        }\n",
					"    }"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def create_spark_session():\n",
					"    \"\"\"Create and return a Spark session\"\"\"\n",
					"    return SparkSession.builder \\\n",
					"        .appName(\"Electoral Data ETL\") \\\n",
					"        .getOrCreate()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Column Mapping"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def normalize_column_name(name):\n",
					"    \"\"\"Normalize column name for comparison (lowercase, remove spaces)\"\"\"\n",
					"    if name is None:\n",
					"        return None\n",
					"    return re.sub(r'[^a-zA-Z0-9]', '', name.lower())\n",
					"\n",
					"def get_standard_column_map(column_mapping):\n",
					"    \"\"\"Create a normalized mapping for easier column matching\"\"\"\n",
					"    normalized_map = {}\n",
					"    \n",
					"    # Map standard names\n",
					"    for standard_name, variations in column_mapping.items():\n",
					"        normalized_std = normalize_column_name(standard_name)\n",
					"        normalized_map[normalized_std] = standard_name\n",
					"        \n",
					"        # Map all variations\n",
					"        for variation in variations:\n",
					"            normalized_var = normalize_column_name(variation)\n",
					"            normalized_map[normalized_var] = standard_name\n",
					"    \n",
					"    return normalized_map"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def read_column_mapping(spark, config_path):\n",
					"    \"\"\"Read and parse the column mapping configuration\"\"\"\n",
					"    try:\n",
					"        # Read the JSON file\n",
					"        mapping_df = spark.read.text(config_path)\n",
					"        \n",
					"        # Combine all lines into a single string\n",
					"        json_str = mapping_df.agg(concat_ws(\"\", collect_list(\"value\"))).collect()[0][0]\n",
					"        \n",
					"        # Parse the JSON string\n",
					"        mapping_dict = json.loads(json_str)\n",
					"        return mapping_dict[\"mappings\"]\n",
					"    except Exception as e:\n",
					"        print(f\"Error reading mapping file: {str(e)}\")\n",
					"        try:\n",
					"            # Fallback to using wholeTextFiles\n",
					"            mapping_rdd = spark.sparkContext.wholeTextFiles(config_path)\n",
					"            json_str = mapping_rdd.values().first()\n",
					"            mapping_dict = json.loads(json_str)\n",
					"            return mapping_dict[\"mappings\"]\n",
					"        except Exception as e2:\n",
					"            raise Exception(f\"Failed to read mapping file. Primary error: {str(e)}, Fallback error: {str(e2)}\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Data Processing Core Functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def normalize_column_name(name):\n",
					"    \"\"\"Normalize column name for comparison (lowercase, remove spaces)\"\"\"\n",
					"    if name is None:\n",
					"        return None\n",
					"    return re.sub(r'[^a-zA-Z0-9]', '', name.lower())\n",
					"\n",
					"def get_standard_column_map(column_mapping):\n",
					"    \"\"\"Create a normalized mapping for easier column matching\"\"\"\n",
					"    normalized_map = {}\n",
					"    \n",
					"    # Map standard names\n",
					"    for standard_name, variations in column_mapping.items():\n",
					"        normalized_std = normalize_column_name(standard_name)\n",
					"        normalized_map[normalized_std] = standard_name\n",
					"        \n",
					"        # Map all variations\n",
					"        for variation in variations:\n",
					"            normalized_var = normalize_column_name(variation)\n",
					"            normalized_map[normalized_var] = standard_name\n",
					"    \n",
					"    return normalized_map"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def process_parquet_files(spark, folder_path, column_mapping, logger):\n",
					"    \"\"\"Process all parquet files with better progress tracking\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    # Get normalized column mapping\n",
					"    norm_mapping = get_standard_column_map(column_mapping)\n",
					"    \n",
					"    # Get all parquet files recursively\n",
					"    all_files = []\n",
					"    for item in mssparkutils.fs.ls(folder_path):\n",
					"        if item.path.endswith(\".parquet\"):\n",
					"            all_files.append(item.path)\n",
					"        elif item.isDir:\n",
					"            # Recursive search in subdirectories\n",
					"            all_files.extend([f.path for f in mssparkutils.fs.ls(item.path) \n",
					"                             if f.path.endswith(\".parquet\")])\n",
					"    \n",
					"    total_files = len(all_files)\n",
					"    logger.info(f\"Found {total_files} parquet files to process\")\n",
					"    \n",
					"    # Process files in parallel when possible\n",
					"    standardized_dfs = []\n",
					"    for i, file_path in enumerate(all_files, 1):\n",
					"        try:\n",
					"            logger.info(f\"Processing file {i}/{total_files}: {file_path}\")\n",
					"            df = spark.read.parquet(file_path)\n",
					"            \n",
					"            # Map columns according to the standardized mapping\n",
					"            mapped_df = standardize_columns(df, norm_mapping)\n",
					"            \n",
					"            if mapped_df.columns:\n",
					"                standardized_dfs.append(mapped_df)\n",
					"                logger.info(f\"Successfully mapped {len(mapped_df.columns)} columns\")\n",
					"            else:\n",
					"                logger.warning(f\"No mappable columns found in {file_path}\")\n",
					"                \n",
					"        except Exception as e:\n",
					"            logger.error(f\"Error processing {file_path}: {str(e)}\")\n",
					"    \n",
					"    return standardized_dfs"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Data Transformation Functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def merge_dataframes(dfs, partition_columns=None):\n",
					"    \"\"\"Merge dataframes with optional partitioning for better performance\"\"\"\n",
					"    if not dfs:\n",
					"        raise ValueError(\"No dataframes to merge\")\n",
					"    \n",
					"    # Determine common schema\n",
					"    merged_df = dfs[0]\n",
					"    for df in dfs[1:]:\n",
					"        merged_df = merged_df.unionByName(df, allowMissingColumns=True)\n",
					"    \n",
					"    # Optimize storage if partition columns are specified\n",
					"    if partition_columns and all(col in merged_df.columns for col in partition_columns):\n",
					"        return merged_df.repartition(*partition_columns)\n",
					"    \n",
					"    # Default behavior - repartition to a reasonable number based on data size\n",
					"    count = merged_df.count()\n",
					"    num_partitions = max(1, min(200, count // 100000))  # Adjust based on your cluster\n",
					"    return merged_df.repartition(num_partitions)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"remove_duplicates() (implement this to handle deduplication)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"transform_to_target_schema() (extract this from your existing code)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Validation"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def validate_output_schema(df, required_columns):\n",
					"    \"\"\"Validate that the output dataframe meets requirements\"\"\"\n",
					"    missing_columns = [col for col in required_columns if col not in df.columns]\n",
					"    \n",
					"    if missing_columns:\n",
					"        raise ValueError(f\"Output schema missing required columns: {missing_columns}\")\n",
					"    \n",
					"    # Check for null values in key fields\n",
					"    for col in [\"poll_number\", \"lname\", \"fname\"]:\n",
					"        if col in df.columns:\n",
					"            null_count = df.filter(df[col].isNull()).count()\n",
					"            if null_count > 0:\n",
					"                logging.warning(f\"Found {null_count} records with null values in {col}\")\n",
					"    \n",
					"    # Return True if validation passes\n",
					"    return True"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def main():\n",
					"    \"\"\"Main ETL process with better structure\"\"\"\n",
					"    # Setup\n",
					"    logger = configure_logging()\n",
					"    config = load_config()\n",
					"    spark = create_spark_session()\n",
					"    \n",
					"    try:\n",
					"        # Read configuration\n",
					"        logger.info(\"Reading column mapping configuration\")\n",
					"        column_mapping = read_column_mapping(\n",
					"            spark, \n",
					"            f\"{config['input_paths']['config_folder']}/col_mapping.json\"\n",
					"        )\n",
					"        \n",
					"        # Process files\n",
					"        logger.info(\"Processing source files\")\n",
					"        standardized_dfs = process_parquet_files(\n",
					"            spark, \n",
					"            config['input_paths']['transform_folder'],\n",
					"            column_mapping,\n",
					"            logger\n",
					"        )\n",
					"        \n",
					"        # Merge data\n",
					"        logger.info(\"Merging standardized dataframes\")\n",
					"        merged_df = merge_dataframes(standardized_dfs, [\"postcode_start\"])\n",
					"        \n",
					"        # Handle duplicates\n",
					"        logger.info(\"Removing duplicates\")\n",
					"        deduplicated_df = remove_duplicates(merged_df)\n",
					"        \n",
					"        # Transform to target schema\n",
					"        logger.info(\"Transforming to target schema\")\n",
					"        target_df = transform_to_target_schema(deduplicated_df)\n",
					"        \n",
					"        # Validate output\n",
					"        logger.info(\"Validating output schema\")\n",
					"        required_columns = [\"poll_number\", \"lname\", \"fname\", \"zip\", \"hash_id\"]\n",
					"        validate_output_schema(target_df, required_columns)\n",
					"        \n",
					"        # Write output\n",
					"        output_path = config['output_paths']['voters_temp']\n",
					"        logger.info(f\"Writing {target_df.count()} records to {output_path}\")\n",
					"        \n",
					"        # Use partitioning for optimal storage and query performance\n",
					"        target_df.write.partitionBy(\"postcode_start\").mode(\"overwrite\").parquet(output_path)\n",
					"        \n",
					"        logger.info(\"ETL process completed successfully\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"ETL process failed: {str(e)}\")\n",
					"        raise\n",
					"    finally:\n",
					"        # Log final statistics\n",
					"        logger.info(\"ETL process finished\")\n",
					"if __name__ == \"__main__\":\n",
					"        main()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"def get_all_mapped_columns(column_mapping):\n",
					"    \"\"\"Get a set of all the column names in the mapping\"\"\"\n",
					"    all_cols = set()\n",
					"    \n",
					"    # Add standard column names\n",
					"    for standard_name in column_mapping.keys():\n",
					"        all_cols.add(standard_name)\n",
					"    \n",
					"    # Add all variations\n",
					"    for variations in column_mapping.values():\n",
					"        for variation in variations:\n",
					"            all_cols.add(variation)\n",
					"    \n",
					"    # Add additional required columns that might not be in the mapping\n",
					"    additional_cols = [\n",
					"        \"CreationDate\", \"LA_Code\", \"Flasg/Markers\", \"Date Of Attainment\",\n",
					"        \"Elector Number Suffix\"\n",
					"    ]\n",
					"    for col in additional_cols:\n",
					"        all_cols.add(col)\n",
					"    \n",
					"    return all_cols\n",
					"\n",
					"def merge_transform_to_voters_temp(spark, transform_folder, config_folder, output_path):\n",
					"    \"\"\"\n",
					"    Merge all files in the transformation folder into a single voters_temp parquet file\n",
					"    using the column mapping schema\n",
					"    \"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    print(f\"Merging files from {transform_folder} to single voters_temp parquet\")\n",
					"    \n",
					"    # Read the column mapping schema\n",
					"    mapping_file = f\"{config_folder}/col_mapping.json\"\n",
					"    try:\n",
					"        column_mapping = read_column_mapping(spark, mapping_file)\n",
					"        print(f\"Successfully read column mapping with {len(column_mapping)} mappings\")\n",
					"        \n",
					"        # Get all mapped columns\n",
					"        mapped_columns = get_all_mapped_columns(column_mapping)\n",
					"        print(f\"Found {len(mapped_columns)} mapped columns\")\n",
					"    except Exception as e:\n",
					"        print(f\"Error reading column mapping: {str(e)}\")\n",
					"        raise e\n",
					"    \n",
					"    # Read all parquet files from the transformation folder\n",
					"    try:\n",
					"        print(\"Listing files in transformation folder...\")\n",
					"        all_files = []\n",
					"        for item in mssparkutils.fs.ls(transform_folder):\n",
					"            if item.path.endswith(\".parquet\"):\n",
					"                all_files.append(item.path)\n",
					"                print(f\"Found parquet file: {item.path}\")\n",
					"            elif item.isDir:\n",
					"                # Check for parquet files in subdirectories\n",
					"                try:\n",
					"                    subitems = mssparkutils.fs.ls(item.path)\n",
					"                    for subitem in subitems:\n",
					"                        if subitem.path.endswith(\".parquet\"):\n",
					"                            all_files.append(subitem.path)\n",
					"                            print(f\"Found parquet file in subfolder: {subitem.path}\")\n",
					"                except Exception as sub_e:\n",
					"                    print(f\"Error listing subfolder {item.path}: {str(sub_e)}\")\n",
					"        \n",
					"        print(f\"Found {len(all_files)} parquet files to process\")\n",
					"        \n",
					"        if len(all_files) == 0:\n",
					"            raise ValueError(\"No parquet files found in the transformation folder\")\n",
					"        \n",
					"        # Read and process each file\n",
					"        dfs = []\n",
					"        for file_path in all_files:\n",
					"            try:\n",
					"                # Read the parquet file\n",
					"                df = spark.read.parquet(file_path)\n",
					"                \n",
					"                # Keep all columns - don't filter based on mapping\n",
					"                # Instead, we'll make sure to include required columns for the target schema\n",
					"                dfs.append(df)\n",
					"                \n",
					"                # Print which mapped columns are present\n",
					"                current_cols = set(df.columns)\n",
					"                mapped_present = [col for col in current_cols if col.lower() in [c.lower() for c in mapped_columns]]\n",
					"                print(f\"Read {df.count()} records from {file_path}\")\n",
					"                print(f\"File has {len(mapped_present)}/{len(current_cols)} columns that match the mapping\")\n",
					"            except Exception as e:\n",
					"                print(f\"Error processing file {file_path}: {str(e)}\")\n",
					"        \n",
					"        if not dfs:\n",
					"            raise ValueError(\"No valid data found in any parquet file\")\n",
					"        \n",
					"        # Merge all dataframes\n",
					"        try:\n",
					"            print(\"Attempting to merge dataframes with schema flexibility...\")\n",
					"            all_data = dfs[0]\n",
					"            for df in dfs[1:]:\n",
					"                try:\n",
					"                    all_data = all_data.unionByName(df, allowMissingColumns=True)\n",
					"                except Exception as union_e:\n",
					"                    print(f\"Error during union: {str(union_e)}\")\n",
					"                    print(\"Falling back to simpler approach\")\n",
					"                    \n",
					"                    # Get just the common columns\n",
					"                    common_cols = list(set(all_data.columns).intersection(set(df.columns)))\n",
					"                    if common_cols:\n",
					"                        print(f\"Using {len(common_cols)} common columns\")\n",
					"                        try:\n",
					"                            all_data = all_data.select(*common_cols).union(df.select(*common_cols))\n",
					"                        except Exception as simpler_e:\n",
					"                            print(f\"Even simpler union failed: {str(simpler_e)}\")\n",
					"                            print(\"Skipping this dataframe\")\n",
					"                    else:\n",
					"                        print(\"No common columns found, skipping this dataframe\")\n",
					"            \n",
					"            print(f\"Merged {all_data.count()} records from all files\")\n",
					"            \n",
					"        except Exception as merge_e:\n",
					"            print(f\"Error during merge: {str(merge_e)}\")\n",
					"            \n",
					"            # Last resort: try merging with the bare minimum columns\n",
					"            print(\"Trying one last approach with minimum columns...\")\n",
					"            min_cols = [\"Elector Number\", \"Elector Surname\", \"Elector Forename\", \"PostCode\"]\n",
					"            valid_dfs = []\n",
					"            \n",
					"            for df in dfs:\n",
					"                # Check if dataframe has the minimum required columns\n",
					"                missing_cols = [col for col in min_cols if col not in df.columns]\n",
					"                if not missing_cols:\n",
					"                    valid_dfs.append(df.select(*min_cols))\n",
					"                else:\n",
					"                    print(f\"Dataframe missing required columns: {missing_cols}\")\n",
					"            \n",
					"            if valid_dfs:\n",
					"                all_data = valid_dfs[0]\n",
					"                for df in valid_dfs[1:]:\n",
					"                    all_data = all_data.union(df)\n",
					"                print(f\"Created minimal merged dataset with {all_data.count()} records\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"Error processing parquet files: {str(e)}\")\n",
					"        raise e\n",
					"    \n",
					"    # Show the original schema\n",
					"    print(\"Original schema after merging:\")\n",
					"    all_data.printSchema()\n",
					"    \n",
					"    # Handle duplicates\n",
					"    print(\"Handling duplicates...\")\n",
					"    \n",
					"    # First check if CreationDate exists, if not add it with current date\n",
					"    if \"CreationDate\" not in all_data.columns:\n",
					"        print(\"Warning: 'CreationDate' column not found, adding current date\")\n",
					"        all_data = all_data.withColumn(\"CreationDate\", lit(datetime.now().strftime(\"%Y-%m-%d\")))\n",
					"    \n",
					"    # Now check for Elector Number\n",
					"    if \"Elector Number\" in all_data.columns:\n",
					"        try:\n",
					"            # Try with CreationDate\n",
					"            window_spec = Window.partitionBy(\"Elector Number\").orderBy(desc(\"CreationDate\"))\n",
					"            deduplicated_data = all_data.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                                .filter(col(\"row_num\") == 1) \\\n",
					"                                .drop(\"row_num\")\n",
					"            print(f\"After deduplication: {deduplicated_data.count()} records\")\n",
					"        except Exception as e:\n",
					"            print(f\"Error during deduplication with CreationDate: {str(e)}\")\n",
					"            print(\"Falling back to simpler deduplication\")\n",
					"            \n",
					"            # Fallback to simple deduplication without ordering\n",
					"            deduplicated_data = all_data.dropDuplicates([\"Elector Number\"])\n",
					"            print(f\"After simple deduplication: {deduplicated_data.count()} records\")\n",
					"    else:\n",
					"        print(\"Warning: 'Elector Number' column not found, skipping deduplication\")\n",
					"        deduplicated_data = all_data\n",
					"    \n",
					"    # Transform to target schema\n",
					"    print(\"Transforming to target schema...\")\n",
					"    \n",
					"    # First, ensure all expected columns exist\n",
					"    required_cols = [\n",
					"        \"Elector Number\", \"Elector Number Prefix\", \"Elector Markers\", \"Elector Title\",\n",
					"        \"Elector Surname\", \"Elector Forename\", \"Elector DOB\", \"Flasg/Markers\",\n",
					"        \"PostCode\", \"Address1\", \"Address2\", \"Address3\", \"Address4\", \"Address5\", \"Address6\",\n",
					"        \"LA_Code\", \"CreationDate\"\n",
					"    ]\n",
					"    \n",
					"    # Add missing columns as null\n",
					"    for col_name in required_cols:\n",
					"        if col_name not in deduplicated_data.columns:\n",
					"            deduplicated_data = deduplicated_data.withColumn(col_name, lit(None))\n",
					"    \n",
					"    # Transform to target schema\n",
					"    target_data = deduplicated_data.select(\n",
					"        # Standard fields with proper mapping\n",
					"        lit(None).cast(\"string\").alias(\"loc_code\"),\n",
					"        lit(None).cast(\"string\").alias(\"part_no\"),\n",
					"        \n",
					"        # Extract digits from LA_Code for register_lett\n",
					"        when(col(\"LA_Code\").isNotNull(), \n",
					"             regexp_replace(col(\"LA_Code\"), \"[^0-9]\", \"\").cast(\"int\"))\n",
					"        .otherwise(lit(None)).alias(\"register_lett\"),\n",
					"        \n",
					"        # Use Elector Number for poll_number\n",
					"        when(col(\"Elector Number\").isNotNull(), \n",
					"             col(\"Elector Number\").cast(\"int\"))\n",
					"        .otherwise(lit(None)).alias(\"poll_number\"),\n",
					"        \n",
					"        col(\"Elector Markers\").alias(\"new_marker\"),\n",
					"        col(\"Elector Title\").alias(\"title\"),\n",
					"        col(\"Elector Surname\").alias(\"lname\"),\n",
					"        col(\"Elector Forename\").alias(\"fname\"),\n",
					"        col(\"Elector DOB\").alias(\"dob\"),\n",
					"        when(col(\"Flasg/Markers\").isNotNull(), col(\"Flasg/Markers\"))\n",
					"        .when(col(\"flags_original\").isNotNull(), col(\"flags_original\"))\n",
					"        .when(col(\"Flag\").isNotNull(), col(\"Flag\"))\n",
					"        .when(col(\"Other Markers\").isNotNull(), col(\"Other Markers\"))\n",
					"        .when(col(\"Markers\").isNotNull(), col(\"Markers\"))\n",
					"        .when(col(\"Jury_Flag\").isNotNull(), col(\"Jury_Flag\"))\n",
					"        .otherwise(lit(None)).cast(\"string\").alias(\"flags\"),\n",
					"        col(\"Address1\").alias(\"address\"),\n",
					"        col(\"Address2\").alias(\"address2\"),\n",
					"        col(\"Address3\").alias(\"address3\"),\n",
					"        col(\"Address4\").alias(\"address4\"),\n",
					"        col(\"Address5\").alias(\"address5\"),\n",
					"        col(\"Address6\").alias(\"address6\"),\n",
					"        col(\"PostCode\").alias(\"zip\"),\n",
					"        lit(None).cast(\"timestamp\").alias(\"date_selected1\"),\n",
					"        lit(None).cast(\"timestamp\").alias(\"date_selected2\"),\n",
					"        lit(None).cast(\"timestamp\").alias(\"date_selected3\"),\n",
					"        \n",
					"        # Extract digits from LA_Code for rec_num\n",
					"        when(col(\"LA_Code\").isNotNull(), \n",
					"             regexp_replace(col(\"LA_Code\"), \"[^0-9]\", \"\").cast(\"int\"))\n",
					"        .otherwise(lit(None)).alias(\"rec_num\"),\n",
					"        \n",
					"        lit(None).cast(\"string\").alias(\"perm_disqual\"),\n",
					"        lit(None).cast(\"string\").alias(\"source_id\"),\n",
					"        \n",
					"        # Generate postcode_start from zip\n",
					"        when(col(\"PostCode\").isNotNull(),\n",
					"             split(trim(col(\"PostCode\")), \" \")[0]\n",
					"        ).otherwise(lit(None)).alias(\"postcode_start\"),\n",
					"        \n",
					"        # Hash ID for sensitive information\n",
					"        xxhash64(concat_ws(\",\", \n",
					"            coalesce(col(\"Elector Forename\"), lit(\"\")),\n",
					"            coalesce(col(\"Elector Surname\"), lit(\"\")),\n",
					"            coalesce(col(\"Elector DOB\"), lit(\"\")),\n",
					"            coalesce(col(\"Address1\"), lit(\"\"))\n",
					"        )).alias(\"hash_id\"),\n",
					"        \n",
					"        # Preserve original creation date\n",
					"        col(\"CreationDate\")\n",
					"    )\n",
					"    \n",
					"    # Show the transformed schema \n",
					"    print(\"Target schema:\")\n",
					"    target_data.printSchema()\n",
					"    \n",
					"    # Write to a single parquet file\n",
					"    print(f\"Writing {target_data.count()} records to {output_path}\")\n",
					"    target_data.coalesce(1).write.mode(\"overwrite\").parquet(output_path)\n",
					"    \n",
					"    print(\"Successfully created voters_temp parquet file\")\n",
					"    return target_data\n",
					"\n",
					"if __name__ == \"__main__\":\n",
					"    # Paths\n",
					"    storage_account = \"baubaisadfsastg\"\n",
					"    transform_folder = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/transformation\"\n",
					"    config_folder = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/config/schema\"\n",
					"    output_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_temp\"\n",
					"    \n",
					"    # Create Spark session\n",
					"    spark = create_spark_session()\n",
					"    \n",
					"    # Merge files from transformation folder using column mapping schema\n",
					"    voters_temp_df = merge_transform_to_voters_temp(spark, transform_folder, config_folder, output_path)\n",
					"    \n",
					"    # Show sample of final data\n",
					"    print(\"\\nSample of final voters_temp data:\")\n",
					"    voters_temp_df.show(5)\n",
					"    \n",
					"    print(f\"\\nProcessing completed. {voters_temp_df.count()} records written to {output_path}\")"
				],
				"execution_count": null
			}
		]
	}
}