{
	"name": "L2_Testing_Mergingfiles",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkstg",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "8e702002-c118-45ce-b899-99920fea0b55"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/74dacd4f-a248-45bb-a2f0-af700dc4cf68/resourceGroups/baubais-data-factory-rg-stg/providers/Microsoft.Synapse/workspaces/baubais-synapse-stg/bigDataPools/sparkstg",
				"name": "sparkstg",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-stg.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkstg",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession, Window\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"import json\n",
					"from datetime import datetime\n",
					"import os\n",
					"import re"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"source": [
					"def create_spark_session():\n",
					"    \"\"\"Create and return a Spark session\"\"\"\n",
					"    return SparkSession.builder \\\n",
					"        .appName(\"Electoral Data ETL\") \\\n",
					"        .getOrCreate()"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"source": [
					"def read_column_mapping(spark, config_path):\n",
					"    \"\"\"Read and parse the column mapping configuration\"\"\"\n",
					"    try:\n",
					"        # Read the JSON file\n",
					"        mapping_df = spark.read.text(config_path)\n",
					"        \n",
					"        # Combine all lines into a single string\n",
					"        json_str = mapping_df.agg(concat_ws(\"\", collect_list(\"value\"))).collect()[0][0]\n",
					"        \n",
					"        # Parse the JSON string\n",
					"        mapping_dict = json.loads(json_str)\n",
					"        return mapping_dict[\"mappings\"]\n",
					"    except Exception as e:\n",
					"        print(f\"Error reading mapping file: {str(e)}\")\n",
					"        try:\n",
					"            # Fallback to using wholeTextFiles\n",
					"            mapping_rdd = spark.sparkContext.wholeTextFiles(config_path)\n",
					"            json_str = mapping_rdd.values().first()\n",
					"            mapping_dict = json.loads(json_str)\n",
					"            return mapping_dict[\"mappings\"]\n",
					"        except Exception as e2:\n",
					"            raise Exception(f\"Failed to read mapping file. Primary error: {str(e)}, Fallback error: {str(e2)}\")\n",
					"\n",
					"def get_all_mapped_columns(column_mapping):\n",
					"    \"\"\"Get a set of all the column names in the mapping\"\"\"\n",
					"    all_cols = set()\n",
					"    \n",
					"    # Add standard column names\n",
					"    for standard_name in column_mapping.keys():\n",
					"        all_cols.add(standard_name)\n",
					"    \n",
					"    # Add all variations\n",
					"    for variations in column_mapping.values():\n",
					"        for variation in variations:\n",
					"            all_cols.add(variation)\n",
					"    \n",
					"    # Add additional required columns that might not be in the mapping\n",
					"    additional_cols = [\n",
					"        \"CreationDate\", \"LA_Code\", \"Flasg/Markers\", \"Date Of Attainment\",\n",
					"        \"Elector Number Suffix\"\n",
					"    ]\n",
					"    for col in additional_cols:\n",
					"        all_cols.add(col)\n",
					"    \n",
					"    return all_cols\n",
					"\n",
					"def merge_transform_to_voters_temp(spark, transform_folder, config_folder, output_path):\n",
					"    \"\"\"\n",
					"    Merge all files in the transformation folder into a single voters_temp parquet file\n",
					"    using the column mapping schema\n",
					"    \"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    print(f\"Merging files from {transform_folder} to single voters_temp parquet\")\n",
					"    \n",
					"    # Read the column mapping schema\n",
					"    mapping_file = f\"{config_folder}/col_mapping.json\"\n",
					"    try:\n",
					"        column_mapping = read_column_mapping(spark, mapping_file)\n",
					"        print(f\"Successfully read column mapping with {len(column_mapping)} mappings\")\n",
					"        \n",
					"        # Get all mapped columns\n",
					"        mapped_columns = get_all_mapped_columns(column_mapping)\n",
					"        print(f\"Found {len(mapped_columns)} mapped columns\")\n",
					"    except Exception as e:\n",
					"        print(f\"Error reading column mapping: {str(e)}\")\n",
					"        raise e\n",
					"    \n",
					"    # Read all parquet files from the transformation folder\n",
					"    try:\n",
					"        print(\"Listing files in transformation folder...\")\n",
					"        all_files = []\n",
					"        for item in mssparkutils.fs.ls(transform_folder):\n",
					"            if item.path.endswith(\".parquet\"):\n",
					"                all_files.append(item.path)\n",
					"                print(f\"Found parquet file: {item.path}\")\n",
					"            elif item.isDir:\n",
					"                # Check for parquet files in subdirectories\n",
					"                try:\n",
					"                    subitems = mssparkutils.fs.ls(item.path)\n",
					"                    for subitem in subitems:\n",
					"                        if subitem.path.endswith(\".parquet\"):\n",
					"                            all_files.append(subitem.path)\n",
					"                            print(f\"Found parquet file in subfolder: {subitem.path}\")\n",
					"                except Exception as sub_e:\n",
					"                    print(f\"Error listing subfolder {item.path}: {str(sub_e)}\")\n",
					"        \n",
					"        print(f\"Found {len(all_files)} parquet files to process\")\n",
					"        \n",
					"        if len(all_files) == 0:\n",
					"            raise ValueError(\"No parquet files found in the transformation folder\")\n",
					"        \n",
					"        # Read and process each file\n",
					"        dfs = []\n",
					"        for file_path in all_files:\n",
					"            try:\n",
					"                # Read the parquet file\n",
					"                df = spark.read.parquet(file_path)\n",
					"                \n",
					"                # Keep all columns - don't filter based on mapping\n",
					"                # Instead, we'll make sure to include required columns for the target schema\n",
					"                dfs.append(df)\n",
					"                \n",
					"                # Print which mapped columns are present\n",
					"                current_cols = set(df.columns)\n",
					"                mapped_present = [col for col in current_cols if col.lower() in [c.lower() for c in mapped_columns]]\n",
					"                print(f\"Read {df.count()} records from {file_path}\")\n",
					"                print(f\"File has {len(mapped_present)}/{len(current_cols)} columns that match the mapping\")\n",
					"            except Exception as e:\n",
					"                print(f\"Error processing file {file_path}: {str(e)}\")\n",
					"        \n",
					"        if not dfs:\n",
					"            raise ValueError(\"No valid data found in any parquet file\")\n",
					"        \n",
					"        # Merge all dataframes\n",
					"        try:\n",
					"            print(\"Attempting to merge dataframes with schema flexibility...\")\n",
					"            all_data = dfs[0]\n",
					"            for df in dfs[1:]:\n",
					"                try:\n",
					"                    all_data = all_data.unionByName(df, allowMissingColumns=True)\n",
					"                except Exception as union_e:\n",
					"                    print(f\"Error during union: {str(union_e)}\")\n",
					"                    print(\"Falling back to simpler approach\")\n",
					"                    \n",
					"                    # Get just the common columns\n",
					"                    common_cols = list(set(all_data.columns).intersection(set(df.columns)))\n",
					"                    if common_cols:\n",
					"                        print(f\"Using {len(common_cols)} common columns\")\n",
					"                        try:\n",
					"                            all_data = all_data.select(*common_cols).union(df.select(*common_cols))\n",
					"                        except Exception as simpler_e:\n",
					"                            print(f\"Even simpler union failed: {str(simpler_e)}\")\n",
					"                            print(\"Skipping this dataframe\")\n",
					"                    else:\n",
					"                        print(\"No common columns found, skipping this dataframe\")\n",
					"            \n",
					"            print(f\"Merged {all_data.count()} records from all files\")\n",
					"            \n",
					"        except Exception as merge_e:\n",
					"            print(f\"Error during merge: {str(merge_e)}\")\n",
					"            \n",
					"            # Last resort: try merging with the bare minimum columns\n",
					"            print(\"Trying one last approach with minimum columns...\")\n",
					"            min_cols = [\"Elector Number\", \"Elector Surname\", \"Elector Forename\", \"PostCode\"]\n",
					"            valid_dfs = []\n",
					"            \n",
					"            for df in dfs:\n",
					"                # Check if dataframe has the minimum required columns\n",
					"                missing_cols = [col for col in min_cols if col not in df.columns]\n",
					"                if not missing_cols:\n",
					"                    valid_dfs.append(df.select(*min_cols))\n",
					"                else:\n",
					"                    print(f\"Dataframe missing required columns: {missing_cols}\")\n",
					"            \n",
					"            if valid_dfs:\n",
					"                all_data = valid_dfs[0]\n",
					"                for df in valid_dfs[1:]:\n",
					"                    all_data = all_data.union(df)\n",
					"                print(f\"Created minimal merged dataset with {all_data.count()} records\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"Error processing parquet files: {str(e)}\")\n",
					"        raise e\n",
					"    \n",
					"    # Show the original schema\n",
					"    print(\"Original schema after merging:\")\n",
					"    all_data.printSchema()\n",
					"    \n",
					"    # Handle duplicates\n",
					"    print(\"Handling duplicates...\")\n",
					"    \n",
					"    # First check if CreationDate exists, if not add it with current date\n",
					"    if \"CreationDate\" not in all_data.columns:\n",
					"        print(\"Warning: 'CreationDate' column not found, adding current date\")\n",
					"        all_data = all_data.withColumn(\"CreationDate\", lit(datetime.now().strftime(\"%Y-%m-%d\")))\n",
					"    \n",
					"    # Now check for Elector Number\n",
					"    if \"Elector Number\" in all_data.columns:\n",
					"        try:\n",
					"            # Try with CreationDate\n",
					"            window_spec = Window.partitionBy(\"Elector Number\").orderBy(desc(\"CreationDate\"))\n",
					"            deduplicated_data = all_data.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                                .filter(col(\"row_num\") == 1) \\\n",
					"                                .drop(\"row_num\")\n",
					"            print(f\"After deduplication: {deduplicated_data.count()} records\")\n",
					"        except Exception as e:\n",
					"            print(f\"Error during deduplication with CreationDate: {str(e)}\")\n",
					"            print(\"Falling back to simpler deduplication\")\n",
					"            \n",
					"            # Fallback to simple deduplication without ordering\n",
					"            deduplicated_data = all_data.dropDuplicates([\"Elector Number\"])\n",
					"            print(f\"After simple deduplication: {deduplicated_data.count()} records\")\n",
					"    else:\n",
					"        print(\"Warning: 'Elector Number' column not found, skipping deduplication\")\n",
					"        deduplicated_data = all_data\n",
					"    \n",
					"    # Transform to target schema\n",
					"    print(\"Transforming to target schema...\")\n",
					"    \n",
					"    # First, ensure all expected columns exist\n",
					"    required_cols = [\n",
					"        \"Elector Number\", \"Elector Number Prefix\", \"Elector Markers\", \"Elector Title\",\n",
					"        \"Elector Surname\", \"Elector Forename\", \"Elector DOB\", \"Flasg/Markers\",\n",
					"        \"PostCode\", \"Address1\", \"Address2\", \"Address3\", \"Address4\", \"Address5\", \"Address6\",\n",
					"        \"LA_Code\", \"CreationDate\"\n",
					"    ]\n",
					"    \n",
					"    # Add missing columns as null\n",
					"    for col_name in required_cols:\n",
					"        if col_name not in deduplicated_data.columns:\n",
					"            deduplicated_data = deduplicated_data.withColumn(col_name, lit(None))\n",
					"    \n",
					"    # Transform to target schema\n",
					"    target_data = deduplicated_data.select(\n",
					"        # Standard fields with proper mapping\n",
					"        lit(None).cast(\"string\").alias(\"loc_code\"),\n",
					"        lit(None).cast(\"string\").alias(\"part_no\"),\n",
					"        \n",
					"        # Extract digits from LA_Code for register_lett\n",
					"        when(col(\"LA_Code\").isNotNull(), \n",
					"             regexp_replace(col(\"LA_Code\"), \"[^0-9]\", \"\").cast(\"int\"))\n",
					"        .otherwise(lit(None)).alias(\"register_lett\"),\n",
					"        \n",
					"        # Use Elector Number for poll_number\n",
					"        when(col(\"Elector Number\").isNotNull(), \n",
					"             col(\"Elector Number\").cast(\"int\"))\n",
					"        .otherwise(lit(None)).alias(\"poll_number\"),\n",
					"        \n",
					"        col(\"Elector Markers\").alias(\"new_marker\"),\n",
					"        col(\"Elector Title\").alias(\"title\"),\n",
					"        col(\"Elector Surname\").alias(\"lname\"),\n",
					"        col(\"Elector Forename\").alias(\"fname\"),\n",
					"        col(\"Elector DOB\").alias(\"dob\"),\n",
					"        col(\"Flasg/Markers\").alias(\"flags\"),\n",
					"        col(\"Address1\").alias(\"address\"),\n",
					"        col(\"Address2\").alias(\"address2\"),\n",
					"        col(\"Address3\").alias(\"address3\"),\n",
					"        col(\"Address4\").alias(\"address4\"),\n",
					"        col(\"Address5\").alias(\"address5\"),\n",
					"        col(\"Address6\").alias(\"address6\"),\n",
					"        col(\"PostCode\").alias(\"zip\"),\n",
					"        lit(None).cast(\"timestamp\").alias(\"date_selected1\"),\n",
					"        lit(None).cast(\"timestamp\").alias(\"date_selected2\"),\n",
					"        lit(None).cast(\"timestamp\").alias(\"date_selected3\"),\n",
					"        \n",
					"        # Extract digits from LA_Code for rec_num\n",
					"        when(col(\"LA_Code\").isNotNull(), \n",
					"             regexp_replace(col(\"LA_Code\"), \"[^0-9]\", \"\").cast(\"int\"))\n",
					"        .otherwise(lit(None)).alias(\"rec_num\"),\n",
					"        \n",
					"        lit(None).cast(\"string\").alias(\"perm_disqual\"),\n",
					"        lit(None).cast(\"string\").alias(\"source_id\"),\n",
					"        \n",
					"        # Generate postcode_start from zip\n",
					"        when(col(\"PostCode\").isNotNull(),\n",
					"             split(trim(col(\"PostCode\")), \" \")[0]\n",
					"        ).otherwise(lit(None)).alias(\"postcode_start\"),\n",
					"        \n",
					"        # Hash ID for sensitive information\n",
					"        xxhash64(concat_ws(\",\", \n",
					"            coalesce(col(\"Elector Forename\"), lit(\"\")),\n",
					"            coalesce(col(\"Elector Surname\"), lit(\"\")),\n",
					"            coalesce(col(\"Elector DOB\"), lit(\"\")),\n",
					"            coalesce(col(\"Address1\"), lit(\"\"))\n",
					"        )).alias(\"hash_id\"),\n",
					"        \n",
					"        # Preserve original creation date\n",
					"        col(\"CreationDate\")\n",
					"    )\n",
					"    \n",
					"    # Show the transformed schema \n",
					"    print(\"Target schema:\")\n",
					"    target_data.printSchema()\n",
					"    \n",
					"    # Write to a single parquet file\n",
					"    print(f\"Writing {target_data.count()} records to {output_path}\")\n",
					"    target_data.coalesce(1).write.mode(\"overwrite\").parquet(output_path)\n",
					"    \n",
					"    print(\"Successfully created voters_temp parquet file\")\n",
					"    return target_data\n",
					"\n",
					"if __name__ == \"__main__\":\n",
					"    # Paths\n",
					"    storage_account = \"baubaisadfsastg\"\n",
					"    transform_folder = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/transformation\"\n",
					"    config_folder = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/config/schema\"\n",
					"    output_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_temp\"\n",
					"    \n",
					"    # Create Spark session\n",
					"    spark = create_spark_session()\n",
					"    \n",
					"    # Merge files from transformation folder using column mapping schema\n",
					"    voters_temp_df = merge_transform_to_voters_temp(spark, transform_folder, config_folder, output_path)\n",
					"    \n",
					"    # Show sample of final data\n",
					"    print(\"\\nSample of final voters_temp data:\")\n",
					"    voters_temp_df.show(5)\n",
					"    \n",
					"    print(f\"\\nProcessing completed. {voters_temp_df.count()} records written to {output_path}\")"
				],
				"execution_count": 36
			}
		]
	}
}