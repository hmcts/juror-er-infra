{
	"name": "L2_Testing_Mergingfiles",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkstg",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "5bf9b25f-f274-4fa2-8ace-015202c82248"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/74dacd4f-a248-45bb-a2f0-af700dc4cf68/resourceGroups/baubais-data-factory-rg-stg/providers/Microsoft.Synapse/workspaces/baubais-synapse-stg/bigDataPools/sparkstg",
				"name": "sparkstg",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-stg.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkstg",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Implementation Sequence\n",
					"\n",
					"Start with Setup Functions\n",
					"\n",
					"configure_logging()\n",
					"load_config()\n",
					"create_spark_session() \n",
					"\n",
					"Column Mapping Functions\n",
					"\n",
					"normalize_column_name()\n",
					"get_standard_column_map()\n",
					"Replace or modify your existing read_column_mapping() function\n",
					"\n",
					"\n",
					"Data Processing Core Functions\n",
					"\n",
					"standardize_columns() (implement this to replace process_df_with_mapping())\n",
					"process_parquet_files()\n",
					"\n",
					"\n",
					"Data Transformation Functions\n",
					"\n",
					"merge_dataframes()\n",
					"remove_duplicates() (implement this to handle deduplication)\n",
					"transform_to_target_schema()\n",
					"\n",
					"\n",
					"Validation and Quality Control\n",
					"\n",
					"validate_output_schema()\n",
					"\n",
					"\n",
					"Main Function\n",
					"\n",
					"Rewrite your main function to use all the new components"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession, Window\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"import json\n",
					"import logging\n",
					"from datetime import datetime\n",
					"import os\n",
					"import re"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def configure_logging():\n",
					"    \"\"\"Set up proper logging with rotating file handler\"\"\"\n",
					"    log_format = '%(asctime)s [%(levelname)s] %(message)s'\n",
					"    logging.basicConfig(level=logging.INFO, format=log_format)\n",
					"    \n",
					"    # Add a handler that writes to a log file\n",
					"    file_handler = logging.FileHandler('electoral_etl.log')\n",
					"    file_handler.setFormatter(logging.Formatter(log_format))\n",
					"    \n",
					"    # Get the root logger and add the file handler\n",
					"    root_logger = logging.getLogger()\n",
					"    root_logger.addHandler(file_handler)\n",
					"    \n",
					"    return root_logger"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def load_config():\n",
					"    \"\"\"Load configuration from environment or config file\"\"\"\n",
					"    return {\n",
					"        \"storage_account\": \"baubaisadfsastg\",\n",
					"        \"input_paths\": {\n",
					"            \"transform_folder\": \"abfss://juror-etl@baubaisadfsastg.dfs.core.windows.net/transformation\",\n",
					"            \"config_folder\": \"abfss://juror-etl@baubaisadfsastg.dfs.core.windows.net/config/schema\"\n",
					"        },\n",
					"        \"output_paths\": {\n",
					"            \"voters_temp\": \"abfss://juror-etl@baubaisadfsastg.dfs.core.windows.net/voters_temp\"\n",
					"        }\n",
					"    }"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def create_spark_session():\n",
					"    \"\"\"Create and return a Spark session\"\"\"\n",
					"    return SparkSession.builder \\\n",
					"        .appName(\"Electoral Data ETL\") \\\n",
					"        .getOrCreate()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Column Mapping"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"def normalize_column_name(name):\n",
					"    \"\"\"Normalize column name for comparison (lowercase, remove spaces)\"\"\"\n",
					"    if name is None:\n",
					"        return None\n",
					"    return re.sub(r'[^a-zA-Z0-9]', '', name.lower())\n",
					"\n",
					"def get_standard_column_map(column_mapping):\n",
					"    \"\"\"Create a normalized mapping for easier column matching\"\"\"\n",
					"    normalized_map = {}\n",
					"    \n",
					"    # Map standard names\n",
					"    for standard_name, variations in column_mapping.items():\n",
					"        normalized_std = normalize_column_name(standard_name)\n",
					"        normalized_map[normalized_std] = standard_name\n",
					"        \n",
					"        # Map all variations\n",
					"        for variation in variations:\n",
					"            normalized_var = normalize_column_name(variation)\n",
					"            normalized_map[normalized_var] = standard_name\n",
					"    \n",
					"    return normalized_map"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Data Processing Core Functions"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"def normalize_column_name(name):\n",
					"    \"\"\"Normalize column name for comparison (lowercase, remove spaces)\"\"\"\n",
					"    if name is None:\n",
					"        return None\n",
					"    return re.sub(r'[^a-zA-Z0-9]', '', name.lower())\n",
					"\n",
					"def get_standard_column_map(column_mapping):\n",
					"    \"\"\"Create a normalized mapping for easier column matching\"\"\"\n",
					"    normalized_map = {}\n",
					"    \n",
					"    # Map standard names\n",
					"    for standard_name, variations in column_mapping.items():\n",
					"        normalized_std = normalize_column_name(standard_name)\n",
					"        normalized_map[normalized_std] = standard_name\n",
					"        \n",
					"        # Map all variations\n",
					"        for variation in variations:\n",
					"            normalized_var = normalize_column_name(variation)\n",
					"            normalized_map[normalized_var] = standard_name\n",
					"    \n",
					"    return normalized_map"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"def process_parquet_files(spark, folder_path, column_mapping, logger):\n",
					"    \"\"\"Process all parquet files with better progress tracking\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    # Get normalized column mapping\n",
					"    norm_mapping = get_standard_column_map(column_mapping)\n",
					"    \n",
					"    # Get all parquet files recursively\n",
					"    all_files = []\n",
					"    for item in mssparkutils.fs.ls(folder_path):\n",
					"        if item.path.endswith(\".parquet\"):\n",
					"            all_files.append(item.path)\n",
					"        elif item.isDir:\n",
					"            # Recursive search in subdirectories\n",
					"            all_files.extend([f.path for f in mssparkutils.fs.ls(item.path) \n",
					"                             if f.path.endswith(\".parquet\")])\n",
					"    \n",
					"    total_files = len(all_files)\n",
					"    logger.info(f\"Found {total_files} parquet files to process\")\n",
					"    \n",
					"    # Process files in parallel when possible\n",
					"    standardized_dfs = []\n",
					"    for i, file_path in enumerate(all_files, 1):\n",
					"        try:\n",
					"            logger.info(f\"Processing file {i}/{total_files}: {file_path}\")\n",
					"            df = spark.read.parquet(file_path)\n",
					"            \n",
					"            # Map columns according to the standardized mapping\n",
					"            mapped_df = standardize_columns(df, norm_mapping)\n",
					"            \n",
					"            if mapped_df.columns:\n",
					"                standardized_dfs.append(mapped_df)\n",
					"                logger.info(f\"Successfully mapped {len(mapped_df.columns)} columns\")\n",
					"            else:\n",
					"                logger.warning(f\"No mappable columns found in {file_path}\")\n",
					"                \n",
					"        except Exception as e:\n",
					"            logger.error(f\"Error processing {file_path}: {str(e)}\")\n",
					"    \n",
					"    return standardized_dfs"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Data Transformation Functions"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"def merge_dataframes(dfs, partition_columns=None):\n",
					"    \"\"\"Merge dataframes with optional partitioning for better performance\"\"\"\n",
					"    if not dfs:\n",
					"        raise ValueError(\"No dataframes to merge\")\n",
					"    \n",
					"    # Determine common schema\n",
					"    merged_df = dfs[0]\n",
					"    for df in dfs[1:]:\n",
					"        merged_df = merged_df.unionByName(df, allowMissingColumns=True)\n",
					"    \n",
					"    # Optimize storage if partition columns are specified\n",
					"    if partition_columns and all(col in merged_df.columns for col in partition_columns):\n",
					"        return merged_df.repartition(*partition_columns)\n",
					"    \n",
					"    # Default behavior - repartition to a reasonable number based on data size\n",
					"    count = merged_df.count()\n",
					"    num_partitions = max(1, min(200, count // 100000))  # Adjust based on your cluster\n",
					"    return merged_df.repartition(num_partitions)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"def validate_output_schema(df, required_columns):\n",
					"    \"\"\"Validate that the output dataframe meets requirements\"\"\"\n",
					"    missing_columns = [col for col in required_columns if col not in df.columns]\n",
					"    \n",
					"    if missing_columns:\n",
					"        raise ValueError(f\"Output schema missing required columns: {missing_columns}\")\n",
					"    \n",
					"    # Check for null values in key fields\n",
					"    for col in [\"poll_number\", \"lname\", \"fname\"]:\n",
					"        if col in df.columns:\n",
					"            null_count = df.filter(df[col].isNull()).count()\n",
					"            if null_count > 0:\n",
					"                logging.warning(f\"Found {null_count} records with null values in {col}\")\n",
					"    \n",
					"    # Return True if validation passes\n",
					"    return True"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## "
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Read Configuration and Mapping"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def read_column_mapping(spark, config_path):\n",
					"    \"\"\"Read and parse the column mapping configuration\"\"\"\n",
					"    try:\n",
					"        # Read the JSON file\n",
					"        mapping_df = spark.read.text(config_path)\n",
					"        \n",
					"        # Combine all lines into a single string\n",
					"        json_str = mapping_df.agg(concat_ws(\"\", collect_list(\"value\"))).collect()[0][0]\n",
					"        \n",
					"        # Parse the JSON string\n",
					"        mapping_dict = json.loads(json_str)\n",
					"        return mapping_dict[\"mappings\"]\n",
					"    except Exception as e:\n",
					"        print(f\"Error reading mapping file: {str(e)}\")\n",
					"        try:\n",
					"            # Fallback to using wholeTextFiles\n",
					"            mapping_rdd = spark.sparkContext.wholeTextFiles(config_path)\n",
					"            json_str = mapping_rdd.values().first()\n",
					"            mapping_dict = json.loads(json_str)\n",
					"            return mapping_dict[\"mappings\"]\n",
					"        except Exception as e2:\n",
					"            raise Exception(f\"Failed to read mapping file. Primary error: {str(e)}, Fallback error: {str(e2)}\")\n",
					"\n",
					"    pass"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Identify and Process New Files"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def get_new_files(spark, input_path, process_log_path):\n",
					"    \"\"\"Identify which files are new and need processing\"\"\"\n",
					"    from notebookutils import mssparkutils\n",
					"    \n",
					"    # Get all files in the input directory\n",
					"    all_files = []\n",
					"    for item in mssparkutils.fs.ls(input_path):\n",
					"        if item.path.endswith(\".parquet\"):\n",
					"            all_files.append(item.path)\n",
					"    \n",
					"    # Read process log to get previously processed files\n",
					"    if mssparkutils.fs.exists(process_log_path):\n",
					"        processed_files = spark.read.parquet(process_log_path) \\\n",
					"                              .select(\"file_path\").rdd.flatMap(lambda x: x).collect()\n",
					"    else:\n",
					"        processed_files = []\n",
					"    \n",
					"    # Identify new files\n",
					"    new_files = [f for f in all_files if f not in processed_files]\n",
					"    \n",
					"    return new_files, all_files"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Read and Standardize Files"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def process_input_files(spark, file_paths, column_mapping):\n",
					"    \"\"\"Read and standardize columns from input files\"\"\"\n",
					"    processed_dfs = []\n",
					"    \n",
					"    for file_path in file_paths:\n",
					"        try:\n",
					"            # Read the file\n",
					"            df = spark.read.parquet(file_path)\n",
					"            \n",
					"            # Standardize column names based on mapping\n",
					"            standardized_df = standardize_columns(df, column_mapping)\n",
					"            \n",
					"            # Add source metadata\n",
					"            standardized_df = standardized_df.withColumn(\"source_file\", lit(file_path))\n",
					"            standardized_df = standardized_df.withColumn(\"process_timestamp\", current_timestamp())\n",
					"            \n",
					"            processed_dfs.append(standardized_df)\n",
					"            \n",
					"        except Exception as e:\n",
					"            print(f\"Error processing {file_path}: {str(e)}\")\n",
					"    \n",
					"    return processed_dfs"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Merge Files with Standardized Schema"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def merge_dataframes(dfs):\n",
					"    \"\"\"Merge all dataframes with standardized schema\"\"\"\n",
					"    if not dfs:\n",
					"        raise ValueError(\"No dataframes to merge\")\n",
					"        \n",
					"    # Merge using unionByName to handle schema variations\n",
					"    merged_df = dfs[0]\n",
					"    for df in dfs[1:]:\n",
					"        merged_df = merged_df.unionByName(df, allowMissingColumns=True)\n",
					"        \n",
					"    return merged_df"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Deduplicate Based on Latest Data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def remove_duplicates_by_extraction_date(df):\n",
					"    \"\"\"Keep only the latest record for each Elector Number + LA_Code combination\"\"\"\n",
					"    # Use your window partitioning approach\n",
					"    window_spec = Window.partitionBy(\"Elector Number\", \"LA_Code\") \\\n",
					"                        .orderBy(desc(\"CreationDate\"))\n",
					"    \n",
					"    deduplicated = df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                     .filter(col(\"row_num\") == 1) \\\n",
					"                     .drop(\"row_num\")\n",
					"                     \n",
					"    return deduplicated"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Apply Hashing for Sensitive Data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def apply_hashing(df):\n",
					"    \"\"\"Apply hashing to sensitive fields\"\"\"\n",
					"    return df.withColumn(\n",
					"        \"hash_id\",\n",
					"        xxhash64(concat_ws(\",\", \n",
					"            coalesce(col(\"Elector Forename\"), lit(\"\")),\n",
					"            coalesce(col(\"Elector Surname\"), lit(\"\")),\n",
					"            coalesce(col(\"Elector DOB\"), lit(\"\")),\n",
					"            coalesce(col(\"Address1\"), lit(\"\"))\n",
					"        ))\n",
					"    )"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Transform to Target Schema"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def transform_to_target_schema(df, column_mapping):\n",
					"    \"\"\"Transform to the PostgreSQL target schema\"\"\"\n",
					"    # Your implementation with all the field mappings\n",
					"    # (loc_code, part_no, register_lett, poll_number, etc.)\n",
					"    pass"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Merge with Existing Data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def merge_with_existing_data(spark, new_data, output_path):\n",
					"    \"\"\"Merge new data with existing data using proper incremental logic\"\"\"\n",
					"    # Check if existing data is available\n",
					"    if mssparkutils.fs.exists(output_path):\n",
					"        existing_data = spark.read.parquet(output_path)\n",
					"        \n",
					"        # Identify primary keys (poll_number and register_lett in target schema)\n",
					"        primary_keys = [\"poll_number\", \"register_lett\"]\n",
					"        \n",
					"        # Create a view of the new data\n",
					"        new_data.createOrReplaceTempView(\"new_data\")\n",
					"        existing_data.createOrReplaceTempView(\"existing_data\")\n",
					"        \n",
					"        # Perform a merge operation to update existing records and add new ones\n",
					"        merged_data = spark.sql(\"\"\"\n",
					"            SELECT n.* \n",
					"            FROM new_data n\n",
					"            LEFT ANTI JOIN existing_data e\n",
					"            ON n.poll_number = e.poll_number AND n.register_lett = e.register_lett\n",
					"            \n",
					"            UNION ALL\n",
					"            \n",
					"            SELECT \n",
					"                CASE \n",
					"                    WHEN n.poll_number IS NOT NULL THEN n.poll_number \n",
					"                    ELSE e.poll_number \n",
					"                END as poll_number,\n",
					"                -- Include all other fields following the same pattern\n",
					"                -- ...\n",
					"                COALESCE(n.creation_date, e.creation_date) as creation_date\n",
					"            FROM existing_data e\n",
					"            LEFT JOIN new_data n\n",
					"            ON e.poll_number = n.poll_number AND e.register_lett = n.register_lett\n",
					"            WHERE n.poll_number IS NULL OR n.creation_date > e.creation_date\n",
					"        \"\"\")\n",
					"        \n",
					"        return merged_data\n",
					"    else:\n",
					"        # No existing data, just return the new data\n",
					"        return new_data"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write Output and Update Process Log"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def write_output_and_update_log(merged_data, new_file_paths, output_path, process_log_path):\n",
					"    \"\"\"Write the merged data and update the process log\"\"\"\n",
					"    # Write merged data\n",
					"    merged_data.write.mode(\"overwrite\").parquet(output_path)\n",
					"    \n",
					"    # Update process log\n",
					"    if new_file_paths:\n",
					"        new_log_entries = [(file_path, current_timestamp()) for file_path in new_file_paths]\n",
					"        new_log_df = spark.createDataFrame(new_log_entries, [\"file_path\", \"process_timestamp\"])\n",
					"        \n",
					"        if mssparkutils.fs.exists(process_log_path):\n",
					"            existing_log = spark.read.parquet(process_log_path)\n",
					"            updated_log = existing_log.union(new_log_df)\n",
					"        else:\n",
					"            updated_log = new_log_df\n",
					"            \n",
					"        updated_log.write.mode(\"overwrite\").parquet(process_log_path)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Main Pipeline Function"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def run_incremental_load():\n",
					"    \"\"\"Main function to run the incremental load process\"\"\"\n",
					"    # Initialize\n",
					"    spark = setup_spark_session()\n",
					"    config = load_config()\n",
					"    process_log_path = f\"{config['output_path']}_process_log\"\n",
					"    \n",
					"    try:\n",
					"        # 1. Get configuration and mapping\n",
					"        column_mapping = read_column_mapping(spark, f\"{config['config_path']}/col_mapping_schema.json\")\n",
					"        \n",
					"        # 2. Identify new files\n",
					"        new_file_paths, all_file_paths = get_new_files(spark, config[\"input_path\"], process_log_path)\n",
					"        \n",
					"        if not new_file_paths:\n",
					"            print(\"No new files to process\")\n",
					"            return\n",
					"            \n",
					"        print(f\"Found {len(new_file_paths)} new files to process\")\n",
					"        \n",
					"        # 3. Process new files\n",
					"        processed_dfs = process_input_files(spark, new_file_paths, column_mapping)\n",
					"        \n",
					"        if not processed_dfs:\n",
					"            print(\"No data to process after reading files\")\n",
					"            return\n",
					"            \n",
					"        # 4. Merge new data\n",
					"        merged_new_data = merge_dataframes(processed_dfs)\n",
					"        \n",
					"        # 5. Deduplicate new data\n",
					"        deduplicated_data = remove_duplicates_by_extraction_date(merged_new_data)\n",
					"        \n",
					"        # 6. Apply hashing\n",
					"        hashed_data = apply_hashing(deduplicated_data)\n",
					"        \n",
					"        # 7. Transform to target schema\n",
					"        transformed_data = transform_to_target_schema(hashed_data, column_mapping)\n",
					"        \n",
					"        # 8. Merge with existing data\n",
					"        final_data = merge_with_existing_data(spark, transformed_data, config[\"output_path\"])\n",
					"        \n",
					"        # 9. Write output and update log\n",
					"        write_output_and_update_log(\n",
					"            final_data, \n",
					"            new_file_paths, \n",
					"            config[\"output_path\"],\n",
					"            process_log_path\n",
					"        )\n",
					"        \n",
					"        print(f\"Successfully processed {len(new_file_paths)} files\")\n",
					"        print(f\"Total records in output: {final_data.count()}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"Error in incremental load process: {str(e)}\")\n",
					"        raise"
				],
				"execution_count": null
			}
		]
	}
}