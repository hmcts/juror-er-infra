{
	"name": "L2_Testing_VotersTable",
	"properties": {
		"folder": {
			"name": "Testing/L2"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkstg",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "0d041248-fd66-4911-bcc8-68974e6c81d7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/74dacd4f-a248-45bb-a2f0-af700dc4cf68/resourceGroups/baubais-data-factory-rg-stg/providers/Microsoft.Synapse/workspaces/baubais-synapse-stg/bigDataPools/sparkstg",
				"name": "sparkstg",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-stg.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkstg",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"import logging"
				],
				"execution_count": 63
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"# Read the data\n",
					"df = spark.read.parquet(\"abfss://juror-etl@baubaisadfsastg.dfs.core.windows.net/voters_postgresql\")\n",
					"\n",
					"# Coalesce to a single partition\n",
					"#df_single = df.coalesce(1)\n",
					"\n",
					"# Write as a single file to a different location\n",
					"#df_single.write.mode(\"overwrite\").parquet(\"abfss://juror-etl@baubaisadfsastg.dfs.core.windows.net/voters_temp_main\")\n",
					""
				],
				"execution_count": 64
			},
			{
				"cell_type": "code",
				"source": [
					"df.count()"
				],
				"execution_count": 65
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"display(df.limit(10))"
				],
				"execution_count": 66
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"df.limit(10).show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import col\n",
					"non_empty_zip = df.filter(col(\"poll_number\").isNull())\n",
					"\n",
					"# Print the number of rows where 'zip' is not null\n",
					"print(f\"Number of rows where 'pol' is not null: {non_empty_zip.count()}\")\n",
					"\n",
					"# Display all columns of the filtered DataFrame\n",
					"display(non_empty_zip)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"from notebookutils import mssparkutils\n",
					"path = \"abfss://juror-etl@baubaisadfsastg.dfs.core.windows.net/voters_postgresql\"\n",
					"exists = mssparkutils.fs.exists(path)\n",
					"print(f\"Path exists: {exists}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"import logging  # Ensure logging is imported\n",
					"from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"import os  # Import os for file system checks\n",
					"\n",
					"def handle_null_values(df):\n",
					"    \"\"\"\n",
					"    Handle NaN, NaT, and other invalid values in the DataFrame\n",
					"    Convert them to None (NULL in SQL) to avoid PostgreSQL errors\n",
					"    \"\"\"\n",
					"    from pyspark.sql.functions import col, when, lit\n",
					"\n",
					"    # Log the initial state\n",
					"    logging.info(f\"Starting null value handling for DataFrame with {df.count()} rows\")\n",
					"\n",
					"    # First, handle timestamp columns which might contain NaT\n",
					"    timestamp_columns = ['date_selected1', 'date_selected2', 'date_selected3']\n",
					"    for column in timestamp_columns:\n",
					"        if column in df.columns:\n",
					"            # For timestamp columns, explicitly set null values\n",
					"            df = df.withColumn(\n",
					"                column,\n",
					"                when(\n",
					"                    (col(column).isNull()) |\n",
					"                    (col(column).cast(\"string\") == \"NaT\") |\n",
					"                    (col(column).cast(\"string\") == \"nat\") |\n",
					"                    (col(column).cast(\"string\") == \"nan\") |\n",
					"                    (col(column).cast(\"string\") == \"NaN\"),\n",
					"                    lit(None)\n",
					"                ).otherwise(col(column))\n",
					"            )\n",
					"            logging.info(f\"Processed timestamp column: {column}\")\n",
					"\n",
					"    # For all other columns, handle NaN and empty strings\n",
					"    for column in df.columns:  # Use df.columns instead of undefined df_single.columns\n",
					"        if column not in timestamp_columns:\n",
					"            # Replace empty strings, \"NaN\", \"nan\", etc. with NULL\n",
					"            df = df.withColumn(\n",
					"                column,\n",
					"                when(\n",
					"                    (col(column).isNull()) |\n",
					"                    (col(column).cast(\"string\") == \"\") |\n",
					"                    (col(column).cast(\"string\") == \"nan\") |\n",
					"                    (col(column).cast(\"string\") == \"NaN\") |\n",
					"                    (col(column).cast(\"string\") == \"NULL\") |\n",
					"                    (col(column).cast(\"string\") == \"NAN\") |\n",
					"                    (col(column).cast(\"string\") == \"None\"),\n",
					"                    lit(None)\n",
					"                ).otherwise(col(column))\n",
					"            )\n",
					"\n",
					"    logging.info(f\"Completed null value handling, DataFrame now has {df.count()} rows\")\n",
					"    return df\n",
					"\n",
					"# Now to read existing data, clean it, and overwrite it:\n",
					"storage_account = \"baubaisadfsastg\"\n",
					"postgresql_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_postgresql\"\n",
					"\n",
					"# Define schema manually in case the directory is empty or schema cannot be inferred\n",
					"schema = StructType([\n",
					"    StructField(\"date_selected1\", TimestampType(), True),\n",
					"    StructField(\"date_selected2\", TimestampType(), True),\n",
					"    StructField(\"date_selected3\", TimestampType(), True),\n",
					"    StructField(\"other_column\", StringType(), True)  # Add other fields as needed\n",
					"])\n",
					"\n",
					"# Read the existing data\n",
					"try:\n",
					"    print(f\"Checking if path exists and contains files: {postgresql_path}\")\n",
					"    \n",
					"    # Check if the directory exists and contains files\n",
					"    hadoop_path = spark._jvm.org.apache.hadoop.fs.Path(postgresql_path)\n",
					"    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
					"    \n",
					"    if not fs.exists(hadoop_path):\n",
					"        raise Exception(f\"The specified path does not exist: {postgresql_path}\")\n",
					"    \n",
					"    # Check if the directory contains files\n",
					"    file_status = fs.listStatus(hadoop_path)\n",
					"    if not file_status or len(file_status) == 0:\n",
					"        raise Exception(f\"The directory exists but contains no files: {postgresql_path}\")\n",
					"\n",
					"    # Read the Parquet files\n",
					"    existing_data = spark.read.schema(schema).parquet(postgresql_path)\n",
					"    print(f\"Read {existing_data.count()} records from {postgresql_path}\")\n",
					"\n",
					"    # Apply the null value handler\n",
					"    cleaned_data = handle_null_values(existing_data)\n",
					"\n",
					"    # Overwrite the existing data\n",
					"    print(f\"Writing cleaned data back to {postgresql_path}\")\n",
					"    cleaned_data.write.mode(\"overwrite\").parquet(postgresql_path)\n",
					"\n",
					"    print(f\"Successfully overwrote data at {postgresql_path}\")\n",
					"except AnalysisException as e:\n",
					"    print(f\"Error reading data: {e}\")\n",
					"    print(\"The directory might be empty or inaccessible.\")\n",
					"except Exception as e:\n",
					"    print(f\"Error processing data: {str(e)}\")\n",
					"    import traceback\n",
					"    print(traceback.format_exc())"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import count\n",
					"\n",
					"final_count = df.groupBy('creation_date','rec_num') \\\n",
					"    .agg(count('rec_num')) \\\n",
					"    .orderBy('creation_date')\n",
					"\n",
					"display(final_count)"
				],
				"execution_count": null
			}
		]
	}
}