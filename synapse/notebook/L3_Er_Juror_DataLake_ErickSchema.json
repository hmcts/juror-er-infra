{
	"name": "L3_Er_Juror_DataLake_ErickSchema",
	"properties": {
		"folder": {
			"name": "crime"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkprod",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "18d0f8bc-f0f7-44cd-8955-aa60ba48ad85"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5ca62022-6aa2-4cee-aaa7-e7536c8d566c/resourceGroups/baubais-data-factory-rg-prod/providers/Microsoft.Synapse/workspaces/baubais-synapse-prod/bigDataPools/sparkprod",
				"name": "sparkprod",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkprod",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### **L3_Er_Juror_DataLake_ErickSchema: Voters Data Lake Export with CDC**\n",
					"\n",
					"This script implements an **ETL (Extract, Transform, Load)** pipeline for exporting and managing voter data in Azure Data Lake Storage using **Delta Lake**. It includes **Change Data Capture (CDC)** logic to merge new data with existing Delta tables, ensuring data consistency and auditability.\n",
					"\n",
					"---\n",
					"\n",
					"### **Overview**\n",
					"\n",
					"The script processes voter data from a source directory, validates it, and writes it to a Delta table in Azure Data Lake Storage. If the Delta table already exists, it merges the new data with the existing data using CDC logic. The script also tracks metadata such as creation and modification timestamps, process names, and record activity indicators.\n",
					"\n",
					"---\n",
					"\n",
					"### **Key Steps in the ETL Process**\n",
					"\n",
					"#### **1. Initialization**\n",
					"- **Logging Configuration**:\n",
					"  - Configures logging to track the ETL process and writes logs to the console.\n",
					"- **Spark Session Initialization**:\n",
					"  - Initializes a Spark session with Delta Lake support for managing Delta tables.\n",
					"\n",
					"---\n",
					"\n",
					"#### **2. Data Ingestion**\n",
					"- **Source Data Path**:\n",
					"  - Reads voter data from the specified Azure Data Lake Storage path (`source_path`).\n",
					"- **Delta Table Path**:\n",
					"  - Specifies the path where the Delta table is stored (`delta_table_path`).\n",
					"\n",
					"---\n",
					"\n",
					"#### **3. Data Validation**\n",
					"- **Primary Key Validation**:\n",
					"  - Ensures there are no duplicate or null values in the primary key column (`hash_id`):\n",
					"    ```python\n",
					"    assert source_df.select(\"hash_id\").distinct().count() == source_df.count(), \"Duplicate primary keys found in source data\"\n",
					"    assert source_df.na.drop(subset=[\"hash_id\"]).count() == source_df.count(), \"Null values found in primary keys (hash_id)\"\n",
					"    ```\n",
					"\n",
					"---\n",
					"\n",
					"#### **4. Change Data Capture (CDC) Logic**\n",
					"- **Delta Table Check**:\n",
					"  - Checks if the Delta table already exists in the specified path:\n",
					"    ```python\n",
					"    if mssparkutils.fs.exists(delta_table_path):\n",
					"    ```\n",
					"\n",
					"- **Merging New Data with Existing Data**:\n",
					"  - If the Delta table exists:\n",
					"    - Reads the existing Delta table into a DataFrame (`existing_df`).\n",
					"    - Performs a **full outer join** between the existing data (`main`) and the new data (`inc`) on the primary key (`hash_id`):\n",
					"      ```python\n",
					"      df_full_outer = existing_df.alias(\"main\").join(\n",
					"          source_df.alias(\"inc\"), \n",
					"          \"hash_id\", \n",
					"          \"outer\"\n",
					"      )\n",
					"      ```\n",
					"    - Defines **audit columns** to track metadata such as creation and modification timestamps, process names, and record activity indicators:\n",
					"      ```python\n",
					"      audit_columns = {\n",
					"          'AdtclmnFirstCreatedDatetime': when(col(\"main.hash_id\").isNull(), col(\"inc.AdtclmnFirstCreatedDatetime\"))\n",
					"                                          .otherwise(col(\"main.AdtclmnFirstCreatedDatetime\")),\n",
					"          'AdtclmnInsertedByProcessName': when(col(\"main.hash_id\").isNull(), col(\"inc.AdtclmnInsertedByProcessName\"))\n",
					"                                           .otherwise(col(\"main.AdtclmnInsertedByProcessName\")),\n",
					"          'AdtclmnModifiedDatetime': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNotNull() & \n",
					"                                           (col(\"main.HASH_ID\") != col(\"inc.HASH_ID\")), current_timestamp())\n",
					"                                      .when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), current_timestamp())\n",
					"                                      .otherwise(None),\n",
					"          'AdtclmnModifiedByProcessName': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNotNull() & \n",
					"                                               (col(\"main.HASH_ID\") != col(\"inc.HASH_ID\")), col(\"inc.AdtclmnInsertedByProcessName\"))\n",
					"                                          .when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), col(\"inc.AdtclmnInsertedByProcessName\"))\n",
					"                                          .otherwise(None),\n",
					"          'AdtclmnRecordActiveInd': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), 0)\n",
					"                                     .otherwise(1).cast(DecimalType(1, 0))\n",
					"      }\n",
					"      ```\n",
					"    - Combines the columns from the existing and new data, applying the audit logic:\n",
					"      ```python\n",
					"      combined_columns = [\n",
					"          coalesce(col(f\"inc.{c}\"), col(f\"main.{c}\")).alias(c) if c not in audit_columns \n",
					"          else audit_columns[c].alias(c) \n",
					"          for c in existing_df.columns\n",
					"      ]\n",
					"      df_full_derived = df_full_outer.select(combined_columns)\n",
					"      ```\n",
					"    - Writes the merged data back to the Delta table:\n",
					"      ```python\n",
					"      df_full_derived.write \\\n",
					"          .mode(\"overwrite\") \\\n",
					"          .option(\"overwriteSchema\", \"true\") \\\n",
					"          .format(\"delta\") \\\n",
					"          .save(delta_table_path)\n",
					"      ```\n",
					"\n",
					"- **Creating a New Delta Table**:\n",
					"  - If the Delta table does not exist:\n",
					"    - Adds audit columns to the source data:\n",
					"      ```python\n",
					"      source_df = source_df.withColumn(\"AdtclmnFirstCreatedDatetime\", current_timestamp()) \\\n",
					"                           .withColumn(\"AdtclmnInsertedByProcessName\", lit(\"InitialLoad\")) \\\n",
					"                           .withColumn(\"AdtclmnModifiedDatetime\", lit(None).cast(\"timestamp\")) \\\n",
					"                           .withColumn(\"AdtclmnModifiedByProcessName\", lit(None).cast(\"string\")) \\\n",
					"                           .withColumn(\"AdtclmnRecordActiveInd\", lit(1).cast(DecimalType(1, 0)))\n",
					"      ```\n",
					"    - Writes the source data as a new Delta table:\n",
					"      ```python\n",
					"      source_df.write \\\n",
					"          .mode(\"overwrite\") \\\n",
					"          .option(\"overwriteSchema\", \"true\") \\\n",
					"          .format(\"delta\") \\\n",
					"          .save(delta_table_path)\n",
					"      ```\n",
					"\n",
					"---\n",
					"\n",
					"#### **5. Error Handling**\n",
					"- Logs errors and stack traces if any exceptions occur during the process:\n",
					"  ```python\n",
					"  except Exception as e:\n",
					"      logger.error(f\"Error in data lake export process: {str(e)}\")\n",
					"      import traceback\n",
					"      logger.error(traceback.format_exc())\n",
					"  ```\n",
					"\n",
					"---\n",
					"\n",
					"### **Key Features**\n",
					"\n",
					"1. **Delta Lake Integration**:\n",
					"   - Uses Delta Lake for efficient data storage and management.\n",
					"   - Supports ACID transactions, schema enforcement, and time travel.\n",
					"\n",
					"2. **Change Data Capture (CDC)**:\n",
					"   - Merges new data with existing data using a full outer join.\n",
					"   - Tracks metadata such as creation and modification timestamps.\n",
					"\n",
					"3. **Primary Key Validation**:\n",
					"   - Ensures data integrity by checking for duplicate and null values in the primary key (`hash_id`).\n",
					"\n",
					"4. **Audit Columns**:\n",
					"   - Tracks metadata for each record, including:\n",
					"     - `AdtclmnFirstCreatedDatetime`\n",
					"     - `AdtclmnInsertedByProcessName`\n",
					"     - `AdtclmnModifiedDatetime`\n",
					"     - `AdtclmnModifiedByProcessName`\n",
					"     - `AdtclmnRecordActiveInd`\n",
					"\n",
					"5. **Error Handling**:\n",
					"   - Logs errors and stack traces for debugging.\n",
					"\n",
					"---\n",
					"\n",
					"### **Output**\n",
					"\n",
					"1. **Delta Table**:\n",
					"   - The final data is stored as a Delta table in Azure Data Lake Storage at the specified path (`delta_table_path`).\n",
					"\n",
					"2. **Audit Metadata**:\n",
					"   - Each record includes metadata for tracking creation, modification, and activity status.\n",
					"\n",
					"---\n",
					"\n",
					"### **Next Steps**\n",
					"\n",
					"1. **Run the Script**:\n",
					"   - Execute the `export_voters_to_datalake_with_cdc()` function to process and export the voter data.\n",
					"\n",
					"2. **Verify the Output**:\n",
					"   - Check the Delta table in Azure Data Lake Storage to ensure the data is written correctly.\n",
					"\n",
					"3. **Query the Delta Table**:\n",
					"   - Use Spark to query the Delta table:\n",
					"     ```python\n",
					"     delta_df = spark.read.format(\"delta\").load(delta_table_path)\n",
					"     delta_df.show()\n",
					"     ```\n",
					"\n",
					"4. **Optimize the Delta Table**:\n",
					"   - Periodically optimize the Delta table to improve performance:\n",
					"     ```python\n",
					"     from delta.tables import DeltaTable\n",
					"     delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
					"     delta_table.optimize().executeCompaction()\n",
					"     ```\n",
					"\n",
					"5. **Time Travel**:\n",
					"   - Use Delta Lake's time travel feature to query historical versions of the data:\n",
					"     ```python\n",
					"     spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(delta_table_path).show()\n",
					"     ```\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"from notebookutils import mssparkutils\n",
					"import logging\n",
					"from datetime import datetime\n",
					"from pyspark.sql.types import DecimalType\n",
					"from pyspark.sql.functions import col, when, coalesce, current_timestamp\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, col, desc"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"source": [
					"# Configure logging\n",
					"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"logger = logging.getLogger()\n",
					"\n",
					"# Initialize Spark session with Delta Lake support\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"Voters Data Lake Export\") \\\n",
					"    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
					"    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
					"    .getOrCreate()"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"def export_aggregated_data_for_powerbi(delta_table_path):\n",
					"    \"\"\"\n",
					"    Creates an aggregated view of voter data and exports it to Power BI blob storage.\n",
					"    \n",
					"    Args:\n",
					"        delta_table_path (str): Path to the source delta table with voter data\n",
					"        \n",
					"    Returns:\n",
					"        tuple: (success_flag, output_path)\n",
					"    \"\"\"\n",
					"    try:\n",
					"        # Define the output path for the Power BI delta table\n",
					"        storage_account = \"baubaisadfsaprod\"\n",
					"        powerbi_delta_path = f\"abfss://dl-juror-eric-power-bi@{storage_account}.dfs.core.windows.net/er_juror_report\"\n",
					"        \n",
					"        logger.info(f\"Creating aggregated view for Power BI from {delta_table_path}\")\n",
					"        \n",
					"        # Read the source delta table\n",
					"        delta_df = spark.read.format(\"delta\").load(delta_table_path)\n",
					"        \n",
					"        # Create the aggregated view as specified in the SQL query\n",
					"        from pyspark.sql.functions import count, col\n",
					"        \n",
					"        # Ensure creation_date and rec_num columns exist\n",
					"        if \"creation_date\" not in delta_df.columns or \"rec_num\" not in delta_df.columns:\n",
					"            logger.warning(\"Required columns (creation_date, rec_num) not found in source data\")\n",
					"            missing_cols = []\n",
					"            if \"creation_date\" not in delta_df.columns:\n",
					"                missing_cols.append(\"creation_date\")\n",
					"            if \"rec_num\" not in delta_df.columns:\n",
					"                missing_cols.append(\"rec_num\")\n",
					"            logger.warning(f\"Missing columns: {', '.join(missing_cols)}\")\n",
					"            \n",
					"            # Show available columns for debugging\n",
					"            logger.info(f\"Available columns: {', '.join(delta_df.columns)}\")\n",
					"            \n",
					"            # Try to use alternative columns if available\n",
					"            if \"creation_date\" not in delta_df.columns and \"CreationDate\" in delta_df.columns:\n",
					"                logger.info(\"Using 'CreationDate' as alternative to 'creation_date'\")\n",
					"                delta_df = delta_df.withColumnRenamed(\"CreationDate\", \"creation_date\")\n",
					"            \n",
					"            if \"rec_num\" not in delta_df.columns and \"LA_Code\" in delta_df.columns:\n",
					"                logger.info(\"Using 'LA_Code' as alternative to 'rec_num'\")\n",
					"                delta_df = delta_df.withColumnRenamed(\"LA_Code\", \"rec_num\")\n",
					"        \n",
					"        # Check if the needed columns are now available\n",
					"        if \"creation_date\" not in delta_df.columns or \"rec_num\" not in delta_df.columns:\n",
					"            logger.error(\"Cannot create aggregation: required columns still not available\")\n",
					"            return False, None\n",
					"        \n",
					"        # Create the aggregated dataframe\n",
					"        logger.info(\"Creating aggregation by creation_date and rec_num\")\n",
					"        aggregated_df = delta_df.groupBy(\"creation_date\", \"rec_num\") \\\n",
					"                                .agg(count(\"*\").alias(\"row_count\"))\n",
					"        \n",
					"        # Log the size of the aggregated data\n",
					"        agg_count = aggregated_df.count()\n",
					"        logger.info(f\"Created aggregated dataframe with {agg_count} rows\")\n",
					"        \n",
					"        # Extract directory from path (without using os.path)\n",
					"        powerbi_dir_path = \"/\".join(powerbi_delta_path.split(\"/\")[:-1])\n",
					"        \n",
					"        # Ensure the destination directory exists\n",
					"        if not mssparkutils.fs.exists(powerbi_dir_path):\n",
					"            logger.info(f\"Creating directory: {os.path.dirname(powerbi_delta_path)}\")\n",
					"            mssparkutils.fs.mkdirs(os.path.dirname(powerbi_delta_path))\n",
					"        \n",
					"        # Write the aggregated data to the Power BI delta location\n",
					"        logger.info(f\"Writing aggregated data to {powerbi_delta_path}\")\n",
					"        aggregated_df.write \\\n",
					"            .mode(\"overwrite\") \\\n",
					"            .format(\"delta\") \\\n",
					"            .save(powerbi_delta_path)\n",
					"        \n",
					"        logger.info(f\"Successfully wrote aggregated data to {powerbi_delta_path}\")\n",
					"        return True, powerbi_delta_path\n",
					"    \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error creating aggregated view for Power BI: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return False, None"
				],
				"execution_count": 45
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"def export_voters_to_datalake_with_cdc():\n",
					"    \"\"\"\n",
					"    Exports voters_deduplicated data to a Delta table in the data lake blob storage.\n",
					"    Implements CDC logic to merge new data with the existing Delta table.\n",
					"    Also creates an aggregated view for Power BI.\n",
					"    \"\"\"\n",
					"    try:\n",
					"        # Initialize storage paths\n",
					"        storage_account = \"baubaisadfsaprod\"\n",
					"        source_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_postgresql\"\n",
					"        delta_table_path = f\"abfss://dl-juror-eric-voters-temp@{storage_account}.dfs.core.windows.net/voters_deduplicated_delta\"\n",
					"        \n",
					"        # Read source data\n",
					"        logger.info(f\"Reading voter data from {source_path}\")\n",
					"        source_df = spark.read.parquet(source_path)\n",
					"        \n",
					"        # Get a list of numeric columns\n",
					"        numeric_cols = [field.name for field in source_df.schema.fields \n",
					"                       if str(field.dataType).startswith((\"IntegerType\", \"LongType\", \"DoubleType\", \"FloatType\", \"DecimalType\"))]\n",
					"        \n",
					"        # Clean NaN values in source data - replace with nulls\n",
					"        logger.info(\"Cleaning NaN values from source data\")\n",
					"        for col_name in numeric_cols:\n",
					"            source_df = source_df.withColumn(\n",
					"                col_name,\n",
					"                when(isnan(col(col_name)) | (col(col_name).isNull()), lit(None).cast(source_df.schema[col_name].dataType))\n",
					"                .otherwise(col(col_name))\n",
					"            )\n",
					"        \n",
					"        # IMPROVEMENT: Standardize hash_id case in source data\n",
					"        if \"hash_id\" in source_df.columns:\n",
					"            logger.info(\"Standardizing hash_id case in source data\")\n",
					"            source_df = source_df.withColumn(\"hash_id\", lower(col(\"hash_id\")))\n",
					"        \n",
					"        record_count = source_df.count()\n",
					"        logger.info(f\"Loaded {record_count} voter records\")\n",
					"        \n",
					"        # Check for duplicates and null values in primary key (hash_id)\n",
					"        logger.info(\"Validating primary key constraints on hash_id\")\n",
					"        distinct_count = source_df.select(\"hash_id\").distinct().count()\n",
					"        if distinct_count != source_df.count():\n",
					"            logger.warning(f\"Found {source_df.count() - distinct_count} duplicate hash_ids in source data\")\n",
					"            \n",
					"            # IMPROVEMENT: Enhanced duplicate logging to identify problematic records\n",
					"            dup_hash_ids = source_df.groupBy(\"hash_id\").count().filter(col(\"count\") > 1)\n",
					"            logger.warning(f\"Sample of duplicate hash_ids: {[r.hash_id for r in dup_hash_ids.limit(5).collect()]}\")\n",
					"            \n",
					"            # Remove duplicates, keeping most recent by creation_date\n",
					"            window_spec = Window.partitionBy(\"hash_id\").orderBy(desc(\"creation_date\"))\n",
					"            source_df = source_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                                .filter(col(\"row_num\") == 1) \\\n",
					"                                .drop(\"row_num\")\n",
					"            logger.info(f\"After removing duplicates: {source_df.count()} records\")\n",
					"        \n",
					"        # Check for nulls in hash_id\n",
					"        null_count = source_df.filter(col(\"hash_id\").isNull()).count()\n",
					"        if null_count > 0:\n",
					"            logger.warning(f\"Found {null_count} null hash_ids in source data - removing these records\")\n",
					"            source_df = source_df.filter(col(\"hash_id\").isNotNull())\n",
					"            logger.info(f\"After removing null hash_ids: {source_df.count()} records\")\n",
					"        \n",
					"        # Check if Delta table exists\n",
					"        if mssparkutils.fs.exists(delta_table_path):\n",
					"            logger.info(f\"Delta table exists at {delta_table_path}. Merging new data.\")\n",
					"            \n",
					"            # Read existing Delta table\n",
					"            existing_df = spark.read.format(\"delta\").load(delta_table_path)\n",
					"            \n",
					"            # IMPROVEMENT: Standardize hash_id case in existing data\n",
					"            if \"hash_id\" in existing_df.columns:\n",
					"                logger.info(\"Standardizing hash_id case in existing data\")\n",
					"                existing_df = existing_df.withColumn(\"hash_id\", lower(col(\"hash_id\")))\n",
					"            \n",
					"            # Clean NaN values in existing data\n",
					"            logger.info(\"Cleaning NaN values from existing Delta table\")\n",
					"            existing_numeric_cols = [field.name for field in existing_df.schema.fields \n",
					"                                   if str(field.dataType).startswith((\"IntegerType\", \"LongType\", \"DoubleType\", \"FloatType\", \"DecimalType\"))]\n",
					"            \n",
					"            for col_name in existing_numeric_cols:\n",
					"                existing_df = existing_df.withColumn(\n",
					"                    col_name,\n",
					"                    when(isnan(col(col_name)) | (col(col_name).isNull()), lit(None).cast(existing_df.schema[col_name].dataType))\n",
					"                    .otherwise(col(col_name))\n",
					"                )\n",
					"            \n",
					"            # IMPROVEMENT: Check for duplicates in existing data\n",
					"            existing_distinct = existing_df.select(\"hash_id\").distinct().count()\n",
					"            if existing_distinct < existing_df.count():\n",
					"                logger.warning(f\"Found {existing_df.count() - existing_distinct} duplicate hash_ids in existing data\")\n",
					"                \n",
					"                # Deduplicate existing data before merge if duplicates found\n",
					"                window_spec = Window.partitionBy(\"hash_id\").orderBy(\n",
					"                    desc(\"AdtclmnModifiedDatetime\"), \n",
					"                    desc(\"AdtclmnFirstCreatedDatetime\")\n",
					"                )\n",
					"                existing_df = existing_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                                    .filter(col(\"row_num\") == 1) \\\n",
					"                                    .drop(\"row_num\")\n",
					"                logger.info(f\"Deduplicated existing data to {existing_df.count()} records\")\n",
					"            \n",
					"            # Add audit columns to source data before the join if they don't exist\n",
					"            logger.info(\"Adding audit columns to source data if needed\")\n",
					"            for audit_col in [\"AdtclmnFirstCreatedDatetime\", \"AdtclmnInsertedByProcessName\", \n",
					"                             \"AdtclmnModifiedDatetime\", \"AdtclmnModifiedByProcessName\", \n",
					"                             \"AdtclmnRecordActiveInd\"]:\n",
					"                if audit_col not in source_df.columns:\n",
					"                    if audit_col == \"AdtclmnFirstCreatedDatetime\":\n",
					"                        source_df = source_df.withColumn(audit_col, current_timestamp())\n",
					"                    elif audit_col == \"AdtclmnInsertedByProcessName\":\n",
					"                        source_df = source_df.withColumn(audit_col, lit(\"CDC_Update\"))\n",
					"                    elif audit_col in [\"AdtclmnModifiedDatetime\", \"AdtclmnModifiedByProcessName\"]:\n",
					"                        source_df = source_df.withColumn(audit_col, lit(None).cast(\"timestamp\" if \"Datetime\" in audit_col else \"string\"))\n",
					"                    elif audit_col == \"AdtclmnRecordActiveInd\":\n",
					"                        source_df = source_df.withColumn(audit_col, lit(1).cast(DecimalType(1, 0)))\n",
					"            \n",
					"            # IMPROVEMENT: Validate schema compatibility between existing and source data\n",
					"            logger.info(\"Validating schema compatibility\")\n",
					"            existing_columns = set(existing_df.columns)\n",
					"            source_columns = set(source_df.columns)\n",
					"            \n",
					"            # Find common columns that will be used in the merge\n",
					"            common_columns = existing_columns.intersection(source_columns)\n",
					"            logger.info(f\"Found {len(common_columns)} common columns between existing and source data\")\n",
					"            \n",
					"            # Find columns that exist in one dataset but not the other\n",
					"            existing_only = existing_columns - source_columns\n",
					"            source_only = source_columns - existing_columns\n",
					"            \n",
					"            if existing_only:\n",
					"                logger.warning(f\"Columns in existing data but not in source data: {', '.join(existing_only)}\")\n",
					"                \n",
					"            if source_only:\n",
					"                logger.warning(f\"Columns in source data but not in existing data: {', '.join(source_only)}\")\n",
					"            \n",
					"            # Log details before join\n",
					"            logger.info(f\"Existing data record count: {existing_df.count()}\")\n",
					"            logger.info(f\"Source data record count: {source_df.count()}\")\n",
					"            \n",
					"            # IMPROVEMENT: Use case-insensitive join for hash_id\n",
					"            logger.info(\"Performing full outer join for CDC with case-insensitive matching\")\n",
					"            df_full_outer = existing_df.alias(\"main\").join(\n",
					"                source_df.alias(\"inc\"), \n",
					"                lower(col(\"main.hash_id\")) == lower(col(\"inc.hash_id\")),  # Case-insensitive join\n",
					"                \"outer\"\n",
					"            )\n",
					"            \n",
					"            # Log details after join\n",
					"            logger.info(f\"After full outer join: {df_full_outer.count()} records\")\n",
					"            \n",
					"            # Define audit columns with correct CDC logic - fixing the case sensitivity issue\n",
					"            audit_columns = {\n",
					"                # For new records (only in incoming data), use the new timestamp\n",
					"                # For existing records, preserve the original creation timestamp\n",
					"                'AdtclmnFirstCreatedDatetime': when(col(\"main.hash_id\").isNull(), col(\"inc.AdtclmnFirstCreatedDatetime\"))\n",
					"                                                .otherwise(col(\"main.AdtclmnFirstCreatedDatetime\")),\n",
					"                \n",
					"                # Preserve the original process name that created the record\n",
					"                'AdtclmnInsertedByProcessName': when(col(\"main.hash_id\").isNull(), col(\"inc.AdtclmnInsertedByProcessName\"))\n",
					"                                                 .otherwise(col(\"main.AdtclmnInsertedByProcessName\")),\n",
					"                \n",
					"                # Update modification timestamp for updates and deactivations\n",
					"                # Fixed: Using hash_id instead of HASH_ID\n",
					"                'AdtclmnModifiedDatetime': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNotNull(), current_timestamp())\n",
					"                                            .when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), current_timestamp())\n",
					"                                            .otherwise(col(\"main.AdtclmnModifiedDatetime\")),\n",
					"                \n",
					"                # Update modification process name similarly\n",
					"                'AdtclmnModifiedByProcessName': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNotNull(), lit(\"CDC_Update\"))\n",
					"                                                .when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), lit(\"CDC_Deactivate\"))\n",
					"                                                .otherwise(col(\"main.AdtclmnModifiedByProcessName\")),\n",
					"                \n",
					"                # Mark records as inactive if they exist only in main (they've been removed in source)\n",
					"                'AdtclmnRecordActiveInd': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), lit(0).cast(DecimalType(1, 0)))\n",
					"                                         .otherwise(lit(1).cast(DecimalType(1, 0)))\n",
					"            }\n",
					"            \n",
					"            # Get all columns from existing dataframe\n",
					"            all_columns = existing_df.columns\n",
					"            \n",
					"            # Combine columns with improved null/NaN handling\n",
					"            logger.info(\"Building merged dataframe with improved NaN handling\")\n",
					"            combined_columns = []\n",
					"            for c in all_columns:\n",
					"                if c in audit_columns:\n",
					"                    combined_columns.append(audit_columns[c].alias(c))\n",
					"                else:\n",
					"                    # Check column data type\n",
					"                    col_type = next((f.dataType for f in existing_df.schema.fields if f.name == c), StringType())\n",
					"                    \n",
					"                    # Handle numeric columns specially to avoid NaN\n",
					"                    if str(col_type).startswith((\"IntegerType\", \"LongType\", \"DoubleType\", \"FloatType\", \"DecimalType\")):\n",
					"                        combined_columns.append(\n",
					"                            when(col(f\"inc.{c}\").isNotNull() & ~isnan(col(f\"inc.{c}\")), col(f\"inc.{c}\"))\n",
					"                            .when(col(f\"main.{c}\").isNotNull() & ~isnan(col(f\"main.{c}\")), col(f\"main.{c}\"))\n",
					"                            .otherwise(lit(None).cast(col_type))\n",
					"                            .alias(c)\n",
					"                        )\n",
					"                    else:\n",
					"                        # For non-numeric columns, use regular coalesce with null awareness\n",
					"                        combined_columns.append(\n",
					"                            coalesce(col(f\"inc.{c}\"), col(f\"main.{c}\")).alias(c)\n",
					"                        )\n",
					"            \n",
					"            # Create the final DataFrame\n",
					"            logger.info(\"Creating final merged dataframe\")\n",
					"            df_full_derived = df_full_outer.select(combined_columns)\n",
					"            \n",
					"            # Final NaN cleanup on the merged data\n",
					"            logger.info(\"Performing final NaN cleanup\")\n",
					"            numeric_cols_final = [field.name for field in df_full_derived.schema.fields \n",
					"                               if str(field.dataType).startswith((\"IntegerType\", \"LongType\", \"DoubleType\", \"FloatType\", \"DecimalType\"))]\n",
					"            \n",
					"            for col_name in numeric_cols_final:\n",
					"                df_full_derived = df_full_derived.withColumn(\n",
					"                    col_name,\n",
					"                    when(isnan(col(col_name)), lit(None).cast(df_full_derived.schema[col_name].dataType))\n",
					"                    .otherwise(col(col_name))\n",
					"                )\n",
					"            \n",
					"            # IMPROVEMENT: Add explicit deduplication step after merging\n",
					"            logger.info(\"Checking for duplicates in final merged dataframe\")\n",
					"            before_dedup_count = df_full_derived.count()\n",
					"            distinct_hash_count = df_full_derived.select(\"hash_id\").distinct().count()\n",
					"            \n",
					"            if before_dedup_count > distinct_hash_count:\n",
					"                logger.warning(f\"Found {before_dedup_count - distinct_hash_count} duplicate hash_ids in merged result - applying deduplication\")\n",
					"                \n",
					"                # Create a window spec to keep the most recently modified record for each hash_id\n",
					"                window_spec = Window.partitionBy(\"hash_id\").orderBy(\n",
					"                    # First priority: active records\n",
					"                    desc(\"AdtclmnRecordActiveInd\"),\n",
					"                    # Second priority: last modified date\n",
					"                    desc(\"AdtclmnModifiedDatetime\"),\n",
					"                    # Third priority: creation date\n",
					"                    desc(\"AdtclmnFirstCreatedDatetime\")\n",
					"                )\n",
					"                \n",
					"                # Apply deduplication - keep only the first record for each hash_id\n",
					"                df_full_derived = df_full_derived.withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
					"                                           .filter(col(\"row_number\") == 1) \\\n",
					"                                           .drop(\"row_number\")\n",
					"                \n",
					"                logger.info(f\"After deduplication: {df_full_derived.count()} records \" +\n",
					"                            f\"(removed {before_dedup_count - df_full_derived.count()} duplicates)\")\n",
					"            else:\n",
					"                logger.info(\"No duplicates found in final merged dataframe\")\n",
					"            \n",
					"            # Log record count\n",
					"            logger.info(f\"Final merged dataframe has {df_full_derived.count()} records\")\n",
					"            \n",
					"            # Check for any remaining NaN values\n",
					"            for col_name in numeric_cols_final:\n",
					"                nan_count = df_full_derived.filter(isnan(col(col_name))).count()\n",
					"                if nan_count > 0:\n",
					"                    logger.warning(f\"Column {col_name} still has {nan_count} NaN values after cleanup\")\n",
					"            \n",
					"            # IMPROVEMENT: Final validation before writing\n",
					"            final_df_count = df_full_derived.count()\n",
					"            final_distinct_count = df_full_derived.select(\"hash_id\").distinct().count()\n",
					"            \n",
					"            if final_df_count > final_distinct_count:\n",
					"                logger.error(f\"CRITICAL: Still found {final_df_count - final_distinct_count} duplicates \" +\n",
					"                            f\"after deduplication. Forcing final deduplication.\")\n",
					"                \n",
					"                # Force final deduplication to ensure data integrity\n",
					"                df_full_derived = df_full_derived.dropDuplicates([\"hash_id\"])\n",
					"                logger.info(f\"After forced deduplication: {df_full_derived.count()} records\")\n",
					"            \n",
					"            # Write the merged data back to the Delta table\n",
					"            logger.info(f\"Writing merged data to Delta table at {delta_table_path}\")\n",
					"            df_full_derived.write \\\n",
					"                .mode(\"overwrite\") \\\n",
					"                .option(\"overwriteSchema\", \"true\") \\\n",
					"                .format(\"delta\") \\\n",
					"                .save(delta_table_path)\n",
					"            \n",
					"            logger.info(f\"Successfully merged and wrote data to Delta table at {delta_table_path}\")\n",
					"        \n",
					"        else:\n",
					"            logger.info(f\"Delta table does not exist. Creating a new Delta table at {delta_table_path}.\")\n",
					"            \n",
					"            # Add audit columns to source data\n",
					"            source_df = source_df.withColumn(\"AdtclmnFirstCreatedDatetime\", current_timestamp()) \\\n",
					"                               .withColumn(\"AdtclmnInsertedByProcessName\", lit(\"InitialLoad\")) \\\n",
					"                               .withColumn(\"AdtclmnModifiedDatetime\", lit(None).cast(\"timestamp\")) \\\n",
					"                               .withColumn(\"AdtclmnModifiedByProcessName\", lit(None).cast(\"string\")) \\\n",
					"                               .withColumn(\"AdtclmnRecordActiveInd\", lit(1).cast(DecimalType(1, 0)))\n",
					"            \n",
					"            # Final NaN cleanup before writing\n",
					"            logger.info(\"Performing final NaN cleanup before initial write\")\n",
					"            for col_name in numeric_cols:\n",
					"                source_df = source_df.withColumn(\n",
					"                    col_name,\n",
					"                    when(isnan(col(col_name)), lit(None).cast(source_df.schema[col_name].dataType))\n",
					"                    .otherwise(col(col_name))\n",
					"                )\n",
					"    \n",
					"            # IMPROVEMENT: Final check for duplicates before initial write\n",
					"            initial_count = source_df.count()\n",
					"            initial_distinct = source_df.select(\"hash_id\").distinct().count()\n",
					"            \n",
					"            if initial_count > initial_distinct:\n",
					"                logger.warning(f\"Found {initial_count - initial_distinct} duplicates before initial write - deduplicating\")\n",
					"                \n",
					"                # Deduplicate by keeping the most recent record\n",
					"                window_spec = Window.partitionBy(\"hash_id\").orderBy(desc(\"creation_date\"))\n",
					"                source_df = source_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                                     .filter(col(\"row_num\") == 1) \\\n",
					"                                     .drop(\"row_num\")\n",
					"                logger.info(f\"After deduplication: {source_df.count()} records\")\n",
					"            \n",
					"            # Write the source data as a new Delta table\n",
					"            logger.info(f\"Writing {source_df.count()} records to new Delta table\")\n",
					"            source_df.write \\\n",
					"                .mode(\"overwrite\") \\\n",
					"                .option(\"overwriteSchema\", \"true\") \\\n",
					"                .format(\"delta\") \\\n",
					"                .save(delta_table_path)\n",
					"            \n",
					"            logger.info(f\"Successfully created new Delta table at {delta_table_path}\")\n",
					"        \n",
					"        # NEW CODE: Create aggregated view for Power BI\n",
					"        logger.info(\"Creating aggregated view for Power BI\")\n",
					"        agg_success, agg_path = export_aggregated_data_for_powerbi(delta_table_path)\n",
					"        \n",
					"        if agg_success:\n",
					"            logger.info(f\"Successfully created aggregated view at {agg_path}\")\n",
					"        else:\n",
					"            logger.warning(\"Failed to create aggregated view for Power BI\")\n",
					"        \n",
					"        # Return success with both paths\n",
					"        return True, {\n",
					"            \"delta_table_path\": delta_table_path,\n",
					"            \"aggregated_path\": agg_path if agg_success else None\n",
					"        }\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error in data lake export process: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return False, None"
				],
				"execution_count": 46
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"# Update the main execution code to handle the new return format and provide more detailed reporting\n",
					"if __name__ == \"__main__\":\n",
					"    try:\n",
					"        logger.info(\"Starting voter data lake export process with CDC\")\n",
					"        print(\"=\" * 80)\n",
					"        print(\"VOTER DATA LAKE EXPORT WITH CDC\")\n",
					"        print(\"=\" * 80)\n",
					"        \n",
					"        # Execute the CDC function\n",
					"        success, paths = export_voters_to_datalake_with_cdc()\n",
					"        \n",
					"        if success:\n",
					"            print(\"\\n✅ Data lake export with CDC completed successfully\")\n",
					"            \n",
					"            # Report on the Delta table\n",
					"            delta_path = paths['delta_table_path']\n",
					"            print(f\"\\nDelta files created/merged at: {delta_path}\")\n",
					"            \n",
					"            # Try to load and count the Delta table for verification\n",
					"            try:\n",
					"                delta_df = spark.read.format(\"delta\").load(delta_path)\n",
					"                record_count = delta_df.count()\n",
					"                distinct_count = delta_df.select(\"hash_id\").distinct().count()\n",
					"                \n",
					"                print(f\"Total records in Delta table: {record_count}\")\n",
					"                print(f\"Unique hash_ids in Delta table: {distinct_count}\")\n",
					"                \n",
					"                if record_count > distinct_count:\n",
					"                    print(f\"⚠️ WARNING: Found {record_count - distinct_count} duplicate records in final Delta table!\")\n",
					"                else:\n",
					"                    print(\"✅ Verification successful: No duplicates found in final Delta table\")\n",
					"                \n",
					"                # Sample data\n",
					"                print(\"\\nSample records from Delta table:\")\n",
					"                delta_df.show(5, truncate=False)\n",
					"                \n",
					"            except Exception as e:\n",
					"                print(f\"Error verifying Delta table: {str(e)}\")\n",
					"            \n",
					"            # Report on Power BI aggregation\n",
					"            if paths.get('aggregated_path'):\n",
					"                agg_path = paths['aggregated_path']\n",
					"                print(f\"\\nAggregated Power BI data created at: {agg_path}\")\n",
					"                \n",
					"                # Try to load and count the aggregated data\n",
					"                try:\n",
					"                    agg_df = spark.read.format(\"delta\").load(agg_path)\n",
					"                    agg_count = agg_df.count()\n",
					"                    print(f\"Aggregation contains {agg_count} records with counts by creation_date and rec_num\")\n",
					"                    \n",
					"                    # Sample data\n",
					"                    print(\"\\nSample records from aggregated data:\")\n",
					"                    agg_df.show(5, truncate=False)\n",
					"                    \n",
					"                except Exception as e:\n",
					"                    print(f\"Error verifying aggregated data: {str(e)}\")\n",
					"            else:\n",
					"                print(\"\\n⚠️ Warning: Aggregated view for Power BI was not created\")\n",
					"            \n",
					"            print(\"\\n✨ Use these paths for your downstream processes once permissions are granted\")\n",
					"            \n",
					"        else:\n",
					"            print(\"\\n❌ Data lake export with CDC failed\")\n",
					"            print(\"Check the logs for more details on the error\")\n",
					"        \n",
					"        print(\"\\n\" + \"=\" * 80)\n",
					"        print(\"PROCESS COMPLETED\")\n",
					"        print(\"=\" * 80)\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Unhandled error in main execution: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        print(f\"\\n❌ CRITICAL ERROR: {str(e)}\")\n",
					"        print(\"Check the logs for detailed stack trace\")"
				],
				"execution_count": 47
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"delta_df = spark.read.format(\"delta\").load( f\"abfss://dl-juror-eric-voters-temp@baubaisadfsaprod.dfs.core.windows.net/voters_deduplicated_delta\")"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import col\n",
					"\n",
					"# Filter rows WHERE hash_id contains \"_\"\n",
					"df_with_underscore = delta_df.filter(col(\"hash_id\").contains(\"_\"))\n",
					"\n",
					"count_with_underscore = df_with_underscore.count()\n",
					"\n",
					"\n",
					"# Filter rows WHERE hash_id does NOT contain \"_\"\n",
					"#df_without_underscore = df.filter(~col(\"hash_id\").contains(\"_\"))"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"print(f\"Records WITH underscore: {count_with_underscore}\")"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import to_date, col, lit\n",
					"\n",
					"# Define your date range\n",
					"start_date = \"2025-01-08\"\n",
					"end_date = \"2025-01-09\"\n",
					"\n",
					"# Load Delta table\n",
					"\n",
					"\n",
					"# Filter using cast to string and parse as date\n",
					"filtered_df = delta_df.filter(\n",
					"    (col(\"creation_date\") == lit(\"20250212\")) |   # string comparison\n",
					"    (col(\"creation_date\") == lit(\"20250213\")) |\n",
					"    (col(\"creation_date\") == lit(\"20250218\"))\n",
					"    \n",
					")\n",
					"\n",
					"# Get all column names\n",
					"all_columns = filtered_df.columns\n",
					"\n",
					"# Keep all except the last 6\n",
					"columns_to_keep = all_columns[:-6]\n",
					"\n",
					"\n",
					"# Select only the columns you want\n",
					"filtered_trimmed_df = filtered_df.select(*columns_to_keep)\n",
					"\n",
					"# Show result\n",
					"display(filtered_trimmed_df)\n",
					""
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"import datetime\n",
					"# Configuration\n",
					"storage_account = \"baubaisadfsaprod\"\n",
					"input_df = filtered_trimmed_df\n",
					"output_dir = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/exports\"\n",
					"\n",
					"# Ensure output directory exists\n",
					"if not mssparkutils.fs.exists(output_dir):\n",
					"    mssparkutils.fs.mkdirs(output_dir)\n",
					"\n",
					"try:\n",
					"    print(f\"🔄 Processing filtered DataFrame data\")\n",
					"    \n",
					"    # Use your existing DataFrame directly (no need to read from parquet)\n",
					"    df = input_df\n",
					"    record_count = df.count()\n",
					"    print(f\"✅ Processing {record_count} records from filtered DataFrame\")\n",
					"    \n",
					"    # Generate a single CSV export\n",
					"    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
					"    temp_output_dir = f\"{output_dir}/temp_{timestamp}\"\n",
					"    \n",
					"    # Write to temporary CSV folder\n",
					"    # You can remove coalesce(1) if you don't need a single file, or keep it if you do\n",
					"    df.coalesce(1).write \\\n",
					"        .option(\"header\", \"true\") \\\n",
					"        .option(\"quoteAll\", \"true\") \\\n",
					"        .mode(\"overwrite\") \\\n",
					"        .csv(temp_output_dir)\n",
					"    \n",
					"    # Locate and rename the .csv file\n",
					"    files_written = mssparkutils.fs.ls(temp_output_dir)\n",
					"    for f in files_written:\n",
					"        if f.path.endswith(\".csv\") and \"part-\" in f.path:\n",
					"            final_csv_path = f\"{output_dir}/voters_export_{timestamp}.csv\"\n",
					"            mssparkutils.fs.cp(f.path, final_csv_path)\n",
					"            print(f\"📄 Exported to: {final_csv_path}\")\n",
					"            break\n",
					"    \n",
					"    # Clean up temp folder\n",
					"    mssparkutils.fs.rm(temp_output_dir, recurse=True)\n",
					"    \n",
					"except Exception as e:\n",
					"    print(f\"❌ Failed to process: {str(e)}\")\n",
					"    import traceback\n",
					"    print(traceback.format_exc())"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"source": [
					"display(filtered_trimmed_df.count())"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					},
					"collapsed": false
				},
				"source": [
					"# Check L2 Delta table\n",
					"l2_df = spark.read.format(\"delta\").load(\"abfss://juror-etl@baubaisadfsaprod.dfs.core.windows.net/voters_transformed\")\n",
					"\n",
					"l2_check = l2_df.filter(\n",
					"    (col(\"rec_num\") == lit(\"110\"))  \n",
					"   # (col(\"creation_date\") == lit(\"20241216\"))\n",
					")\n",
					"\n",
					"print(f\"L2 records for target dates: {l2_check.count()}\")\n",
					"display(l2_check)"
				],
				"execution_count": 58
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import count\n",
					"\n",
					"final_count = delta_df.groupBy('creation_date','rec_num') \\\n",
					"    .agg(count('rec_num')) \\\n",
					"    .orderBy('rec_num')\n",
					"\n",
					"display(final_count)"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					"display(delta_df.count())"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"display(filtered_trimmed_df)"
				],
				"execution_count": 51
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"import datetime\n",
					"import time\n",
					"import os\n",
					"import re\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col, when, lit, regexp_replace, trim, upper, substring, to_date, date_format, expr\n",
					"from pyspark.sql.types import StringType, IntegerType, TimestampType, LongType, DateType\n",
					"import pyspark.sql.functions as F\n",
					"\n",
					"storage_account = \"baubaisadfsaprod\"\n",
					"output_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/exports\"\n",
					"\n",
					"# Create timestamp for unique filename\n",
					"timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
					"\n",
					"# Write to temporary CSV folder\n",
					"temp_output_dir = f\"{output_path}/temp_{timestamp}\"\n",
					"filtered_trimmed_df.coalesce(1).write \\\n",
					"    .option(\"header\", \"true\") \\\n",
					"    .option(\"quoteAll\", \"true\") \\\n",
					"    .option(\"escape\", \"\\\\\") \\\n",
					"    .mode(\"overwrite\") \\\n",
					"    .csv(temp_output_dir)\n",
					"\n",
					"# Locate and rename the .csv file\n",
					"files_written = mssparkutils.fs.ls(temp_output_dir)\n",
					"csv_found = False\n",
					"\n",
					"for f in files_written:\n",
					"    if f.path.endswith(\".csv\") and \"part-\" in f.path:\n",
					"        final_csv_path = f\"{output_path}/voters_export_{timestamp}.csv\"\n",
					"        mssparkutils.fs.cp(f.path, final_csv_path)\n",
					"        print(f\"📄 Exported to: {final_csv_path}\")\n",
					"        csv_found = True\n",
					"        break\n",
					"\n",
					"# Check if a CSV file was found and copied\n",
					"if not csv_found:\n",
					"    print(\"⚠️ No CSV file was found in the output directory\")\n",
					"    \n",
					"# Clean up temp directory\n",
					"mssparkutils.fs.rm(temp_output_dir, recurse=True)"
				],
				"execution_count": 64
			}
		]
	}
}