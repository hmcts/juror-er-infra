{
	"name": "L3_Er_Juror_DataLake_ErickSchema",
	"properties": {
		"folder": {
			"name": "crime"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkprod",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "eec2746b-d37a-4c12-ae6a-091d2d8c6c22"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5ca62022-6aa2-4cee-aaa7-e7536c8d566c/resourceGroups/baubais-data-factory-rg-prod/providers/Microsoft.Synapse/workspaces/baubais-synapse-prod/bigDataPools/sparkprod",
				"name": "sparkprod",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkprod",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# **L3_Er_Juror_DataLake_ErickSchema: Voters Data Lake Export with CDC**\n",
					"\n",
					"This notebook implements an **ETL (Extract, Transform, Load)** pipeline for exporting and managing voter data in Azure Data Lake Storage using **Delta Lake**. It includes **Change Data Capture (CDC)** logic to merge new data with existing Delta tables, ensuring data consistency and auditability.\n",
					"\n",
					"---\n",
					"\n",
					"### **Workflow Overview**\n",
					"\n",
					"1. **Initialization**\n",
					"    - Configure logging.\n",
					"    - Initialize Spark session with Delta Lake support.\n",
					"\n",
					"2. **Data Export with CDC**\n",
					"    - **Function:** `export_voters_to_datalake_with_cdc()`\n",
					"        - Reads source data from Azure Data Lake.\n",
					"        - Cleans and deduplicates data.\n",
					"        - Validates primary key (`hash_id`) uniqueness and non-nullness.\n",
					"        - If Delta table exists:\n",
					"            - Reads existing Delta table.\n",
					"            - Cleans and deduplicates existing data.\n",
					"            - Merges new and existing data using a full outer join on `hash_id`.\n",
					"            - Applies CDC logic and audit columns.\n",
					"            - Deduplicates merged data.\n",
					"            - Writes merged data back to Delta table.\n",
					"        - If Delta table does not exist:\n",
					"            - Adds audit columns to source data.\n",
					"            - Deduplicates and writes as a new Delta table.\n",
					"        - Calls `export_aggregated_data_for_powerbi()` to create an aggregated view for Power BI.\n",
					"\n",
					"3. **Aggregated Data Export**\n",
					"    - **Function:** `export_aggregated_data_for_powerbi(delta_table_path)`\n",
					"        - Reads the Delta table.\n",
					"        - Aggregates data by `creation_date` and `rec_num`.\n",
					"        - Writes the aggregated data to a Power BI Delta location.\n",
					"\n",
					"4. **Main Execution**\n",
					"    - Runs the CDC export and reports results.\n",
					"    - Loads and verifies the Delta table.\n",
					"    - Loads and verifies the Power BI aggregation.\n",
					"    - Prints sample records and summary statistics.\n",
					"\n",
					"5. **Post-Export Analysis**\n",
					"    - Loads the Delta table and PostgreSQL export.\n",
					"    - Compares hash_ids to find:\n",
					"        - Records in Delta but missing from PostgreSQL.\n",
					"        - Records in PostgreSQL but not in Delta.\n",
					"    - Exports missing and extra records for further investigation.\n",
					"\n",
					"---\n",
					"\n",
					"### **Key Functions**\n",
					"\n",
					"- **`export_voters_to_datalake_with_cdc()`**\n",
					"    - Main ETL and CDC logic for exporting and merging voter data.\n",
					"\n",
					"- **`export_aggregated_data_for_powerbi(delta_table_path)`**\n",
					"    - Aggregates and exports data for Power BI reporting.\n",
					"\n",
					"---\n",
					"\n",
					"### **Validation & Reporting**\n",
					"\n",
					"- Ensures data integrity by checking for duplicates and nulls in `hash_id`.\n",
					"- Logs all major steps and warnings.\n",
					"- Provides sample data and summary statistics after each major operation.\n",
					"- Exports discrepancies for further analysis.\n",
					"\n",
					"---\n",
					"\n",
					"### **Usage**\n",
					"\n",
					"- Run the notebook cells in order to execute the ETL pipeline.\n",
					"- Review logs and output paths for verification and downstream processing.\n",
					"- Use exported CSVs for investigating data mismatches between Delta and PostgreSQL.\n",
					"\n",
					"---\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"from notebookutils import mssparkutils\n",
					"import logging\n",
					"from datetime import datetime\n",
					"from pyspark.sql.types import DecimalType\n",
					"from pyspark.sql.functions import col, when, coalesce, current_timestamp\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, col, desc"
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"source": [
					"# Configure logging\n",
					"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"logger = logging.getLogger()\n",
					"\n",
					"# Initialize Spark session with Delta Lake support\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"Voters Data Lake Export\") \\\n",
					"    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
					"    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
					"    .getOrCreate()"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"def export_aggregated_data_for_powerbi(delta_table_path):\n",
					"    \"\"\"\n",
					"    Creates an aggregated view of voter data and exports it to Power BI blob storage.\n",
					"    \n",
					"    Args:\n",
					"        delta_table_path (str): Path to the source delta table with voter data\n",
					"        \n",
					"    Returns:\n",
					"        tuple: (success_flag, output_path)\n",
					"    \"\"\"\n",
					"    try:\n",
					"        # Define the output path for the Power BI delta table\n",
					"        storage_account = \"baubaisadfsaprod\"\n",
					"        powerbi_delta_path = f\"abfss://dl-juror-eric-power-bi@{storage_account}.dfs.core.windows.net/er_juror_report\"\n",
					"        \n",
					"        logger.info(f\"Creating aggregated view for Power BI from {delta_table_path}\")\n",
					"        \n",
					"        # Read the source delta table\n",
					"        delta_df = spark.read.format(\"delta\").load(delta_table_path)\n",
					"        \n",
					"        # Create the aggregated view as specified in the SQL query\n",
					"        from pyspark.sql.functions import count, col\n",
					"        \n",
					"        # Ensure creation_date and rec_num columns exist\n",
					"        if \"creation_date\" not in delta_df.columns or \"rec_num\" not in delta_df.columns:\n",
					"            logger.warning(\"Required columns (creation_date, rec_num) not found in source data\")\n",
					"            missing_cols = []\n",
					"            if \"creation_date\" not in delta_df.columns:\n",
					"                missing_cols.append(\"creation_date\")\n",
					"            if \"rec_num\" not in delta_df.columns:\n",
					"                missing_cols.append(\"rec_num\")\n",
					"            logger.warning(f\"Missing columns: {', '.join(missing_cols)}\")\n",
					"            \n",
					"            # Show available columns for debugging\n",
					"            logger.info(f\"Available columns: {', '.join(delta_df.columns)}\")\n",
					"            \n",
					"            # Try to use alternative columns if available\n",
					"            if \"creation_date\" not in delta_df.columns and \"CreationDate\" in delta_df.columns:\n",
					"                logger.info(\"Using 'CreationDate' as alternative to 'creation_date'\")\n",
					"                delta_df = delta_df.withColumnRenamed(\"CreationDate\", \"creation_date\")\n",
					"            \n",
					"            if \"rec_num\" not in delta_df.columns and \"LA_Code\" in delta_df.columns:\n",
					"                logger.info(\"Using 'LA_Code' as alternative to 'rec_num'\")\n",
					"                delta_df = delta_df.withColumnRenamed(\"LA_Code\", \"rec_num\")\n",
					"        \n",
					"        # Check if the needed columns are now available\n",
					"        if \"creation_date\" not in delta_df.columns or \"rec_num\" not in delta_df.columns:\n",
					"            logger.error(\"Cannot create aggregation: required columns still not available\")\n",
					"            return False, None\n",
					"        \n",
					"        # Create the aggregated dataframe\n",
					"        logger.info(\"Creating aggregation by creation_date and rec_num\")\n",
					"        aggregated_df = delta_df.groupBy(\"creation_date\", \"rec_num\") \\\n",
					"                                .agg(count(\"*\").alias(\"row_count\"))\n",
					"        \n",
					"        # Log the size of the aggregated data\n",
					"        agg_count = aggregated_df.count()\n",
					"        logger.info(f\"Created aggregated dataframe with {agg_count} rows\")\n",
					"        \n",
					"        # Extract directory from path (without using os.path)\n",
					"        powerbi_dir_path = \"/\".join(powerbi_delta_path.split(\"/\")[:-1])\n",
					"        \n",
					"        # Ensure the destination directory exists\n",
					"        if not mssparkutils.fs.exists(powerbi_dir_path):\n",
					"            logger.info(f\"Creating directory: {os.path.dirname(powerbi_delta_path)}\")\n",
					"            mssparkutils.fs.mkdirs(os.path.dirname(powerbi_delta_path))\n",
					"        \n",
					"        # Write the aggregated data to the Power BI delta location\n",
					"        logger.info(f\"Writing aggregated data to {powerbi_delta_path}\")\n",
					"        aggregated_df.write \\\n",
					"            .mode(\"overwrite\") \\\n",
					"            .format(\"delta\") \\\n",
					"            .save(powerbi_delta_path)\n",
					"        \n",
					"        logger.info(f\"Successfully wrote aggregated data to {powerbi_delta_path}\")\n",
					"        return True, powerbi_delta_path\n",
					"    \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error creating aggregated view for Power BI: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return False, None"
				],
				"execution_count": 45
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"def export_voters_to_datalake_with_cdc():\n",
					"    \"\"\"\n",
					"    Exports voters_deduplicated data to a Delta table in the data lake blob storage.\n",
					"    Implements CDC logic to merge new data with the existing Delta table.\n",
					"    Also creates an aggregated view for Power BI.\n",
					"    \"\"\"\n",
					"    try:\n",
					"        # Initialize storage paths\n",
					"        storage_account = \"baubaisadfsaprod\"\n",
					"        source_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_postgresql\"\n",
					"        delta_table_path = f\"abfss://dl-juror-eric-voters-temp@{storage_account}.dfs.core.windows.net/voters_deduplicated_delta\"\n",
					"        \n",
					"        # Read source data\n",
					"        logger.info(f\"Reading voter data from {source_path}\")\n",
					"        source_df = spark.read.parquet(source_path)\n",
					"        \n",
					"        # Get a list of numeric columns\n",
					"        numeric_cols = [field.name for field in source_df.schema.fields \n",
					"                       if str(field.dataType).startswith((\"IntegerType\", \"LongType\", \"DoubleType\", \"FloatType\", \"DecimalType\"))]\n",
					"        \n",
					"        # Clean NaN values in source data - replace with nulls\n",
					"        logger.info(\"Cleaning NaN values from source data\")\n",
					"        for col_name in numeric_cols:\n",
					"            source_df = source_df.withColumn(\n",
					"                col_name,\n",
					"                when(isnan(col(col_name)) | (col(col_name).isNull()), lit(None).cast(source_df.schema[col_name].dataType))\n",
					"                .otherwise(col(col_name))\n",
					"            )\n",
					"        \n",
					"        # IMPROVEMENT: Standardize hash_id case in source data\n",
					"        if \"hash_id\" in source_df.columns:\n",
					"            logger.info(\"Standardizing hash_id case in source data\")\n",
					"            source_df = source_df.withColumn(\"hash_id\", lower(col(\"hash_id\")))\n",
					"        \n",
					"        record_count = source_df.count()\n",
					"        logger.info(f\"Loaded {record_count} voter records\")\n",
					"        \n",
					"        # Check for duplicates and null values in primary key (hash_id)\n",
					"        logger.info(\"Validating primary key constraints on hash_id\")\n",
					"        distinct_count = source_df.select(\"hash_id\").distinct().count()\n",
					"        if distinct_count != source_df.count():\n",
					"            logger.warning(f\"Found {source_df.count() - distinct_count} duplicate hash_ids in source data\")\n",
					"            \n",
					"            # IMPROVEMENT: Enhanced duplicate logging to identify problematic records\n",
					"            dup_hash_ids = source_df.groupBy(\"hash_id\").count().filter(col(\"count\") > 1)\n",
					"            logger.warning(f\"Sample of duplicate hash_ids: {[r.hash_id for r in dup_hash_ids.limit(5).collect()]}\")\n",
					"            \n",
					"            # Remove duplicates, keeping most recent by creation_date\n",
					"            window_spec = Window.partitionBy(\"hash_id\").orderBy(desc(\"creation_date\"))\n",
					"            source_df = source_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                                .filter(col(\"row_num\") == 1) \\\n",
					"                                .drop(\"row_num\")\n",
					"            logger.info(f\"After removing duplicates: {source_df.count()} records\")\n",
					"        \n",
					"        # Check for nulls in hash_id\n",
					"        null_count = source_df.filter(col(\"hash_id\").isNull()).count()\n",
					"        if null_count > 0:\n",
					"            logger.warning(f\"Found {null_count} null hash_ids in source data - removing these records\")\n",
					"            source_df = source_df.filter(col(\"hash_id\").isNotNull())\n",
					"            logger.info(f\"After removing null hash_ids: {source_df.count()} records\")\n",
					"        \n",
					"        # Check if Delta table exists\n",
					"        if mssparkutils.fs.exists(delta_table_path):\n",
					"            logger.info(f\"Delta table exists at {delta_table_path}. Merging new data.\")\n",
					"            \n",
					"            # Read existing Delta table\n",
					"            existing_df = spark.read.format(\"delta\").load(delta_table_path)\n",
					"            \n",
					"            # IMPROVEMENT: Standardize hash_id case in existing data\n",
					"            if \"hash_id\" in existing_df.columns:\n",
					"                logger.info(\"Standardizing hash_id case in existing data\")\n",
					"                existing_df = existing_df.withColumn(\"hash_id\", lower(col(\"hash_id\")))\n",
					"            \n",
					"            # Clean NaN values in existing data\n",
					"            logger.info(\"Cleaning NaN values from existing Delta table\")\n",
					"            existing_numeric_cols = [field.name for field in existing_df.schema.fields \n",
					"                                   if str(field.dataType).startswith((\"IntegerType\", \"LongType\", \"DoubleType\", \"FloatType\", \"DecimalType\"))]\n",
					"            \n",
					"            for col_name in existing_numeric_cols:\n",
					"                existing_df = existing_df.withColumn(\n",
					"                    col_name,\n",
					"                    when(isnan(col(col_name)) | (col(col_name).isNull()), lit(None).cast(existing_df.schema[col_name].dataType))\n",
					"                    .otherwise(col(col_name))\n",
					"                )\n",
					"            \n",
					"            # IMPROVEMENT: Check for duplicates in existing data\n",
					"            existing_distinct = existing_df.select(\"hash_id\").distinct().count()\n",
					"            if existing_distinct < existing_df.count():\n",
					"                logger.warning(f\"Found {existing_df.count() - existing_distinct} duplicate hash_ids in existing data\")\n",
					"                \n",
					"                # Deduplicate existing data before merge if duplicates found\n",
					"                window_spec = Window.partitionBy(\"hash_id\").orderBy(\n",
					"                    desc(\"AdtclmnModifiedDatetime\"), \n",
					"                    desc(\"AdtclmnFirstCreatedDatetime\")\n",
					"                )\n",
					"                existing_df = existing_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                                    .filter(col(\"row_num\") == 1) \\\n",
					"                                    .drop(\"row_num\")\n",
					"                logger.info(f\"Deduplicated existing data to {existing_df.count()} records\")\n",
					"            \n",
					"            # Add audit columns to source data before the join if they don't exist\n",
					"            logger.info(\"Adding audit columns to source data if needed\")\n",
					"            for audit_col in [\"AdtclmnFirstCreatedDatetime\", \"AdtclmnInsertedByProcessName\", \n",
					"                             \"AdtclmnModifiedDatetime\", \"AdtclmnModifiedByProcessName\", \n",
					"                             \"AdtclmnRecordActiveInd\"]:\n",
					"                if audit_col not in source_df.columns:\n",
					"                    if audit_col == \"AdtclmnFirstCreatedDatetime\":\n",
					"                        source_df = source_df.withColumn(audit_col, current_timestamp())\n",
					"                    elif audit_col == \"AdtclmnInsertedByProcessName\":\n",
					"                        source_df = source_df.withColumn(audit_col, lit(\"CDC_Update\"))\n",
					"                    elif audit_col in [\"AdtclmnModifiedDatetime\", \"AdtclmnModifiedByProcessName\"]:\n",
					"                        source_df = source_df.withColumn(audit_col, lit(None).cast(\"timestamp\" if \"Datetime\" in audit_col else \"string\"))\n",
					"                    elif audit_col == \"AdtclmnRecordActiveInd\":\n",
					"                        source_df = source_df.withColumn(audit_col, lit(1).cast(DecimalType(1, 0)))\n",
					"            \n",
					"            # IMPROVEMENT: Validate schema compatibility between existing and source data\n",
					"            logger.info(\"Validating schema compatibility\")\n",
					"            existing_columns = set(existing_df.columns)\n",
					"            source_columns = set(source_df.columns)\n",
					"            \n",
					"            # Find common columns that will be used in the merge\n",
					"            common_columns = existing_columns.intersection(source_columns)\n",
					"            logger.info(f\"Found {len(common_columns)} common columns between existing and source data\")\n",
					"            \n",
					"            # Find columns that exist in one dataset but not the other\n",
					"            existing_only = existing_columns - source_columns\n",
					"            source_only = source_columns - existing_columns\n",
					"            \n",
					"            if existing_only:\n",
					"                logger.warning(f\"Columns in existing data but not in source data: {', '.join(existing_only)}\")\n",
					"                \n",
					"            if source_only:\n",
					"                logger.warning(f\"Columns in source data but not in existing data: {', '.join(source_only)}\")\n",
					"            \n",
					"            # Log details before join\n",
					"            logger.info(f\"Existing data record count: {existing_df.count()}\")\n",
					"            logger.info(f\"Source data record count: {source_df.count()}\")\n",
					"            \n",
					"            # IMPROVEMENT: Use case-insensitive join for hash_id\n",
					"            logger.info(\"Performing full outer join for CDC with case-insensitive matching\")\n",
					"            df_full_outer = existing_df.alias(\"main\").join(\n",
					"                source_df.alias(\"inc\"), \n",
					"                lower(col(\"main.hash_id\")) == lower(col(\"inc.hash_id\")),  # Case-insensitive join\n",
					"                \"outer\"\n",
					"            )\n",
					"            \n",
					"            # Log details after join\n",
					"            logger.info(f\"After full outer join: {df_full_outer.count()} records\")\n",
					"            \n",
					"            # Define audit columns with correct CDC logic - fixing the case sensitivity issue\n",
					"            audit_columns = {\n",
					"                # For new records (only in incoming data), use the new timestamp\n",
					"                # For existing records, preserve the original creation timestamp\n",
					"                'AdtclmnFirstCreatedDatetime': when(col(\"main.hash_id\").isNull(), col(\"inc.AdtclmnFirstCreatedDatetime\"))\n",
					"                                                .otherwise(col(\"main.AdtclmnFirstCreatedDatetime\")),\n",
					"                \n",
					"                # Preserve the original process name that created the record\n",
					"                'AdtclmnInsertedByProcessName': when(col(\"main.hash_id\").isNull(), col(\"inc.AdtclmnInsertedByProcessName\"))\n",
					"                                                 .otherwise(col(\"main.AdtclmnInsertedByProcessName\")),\n",
					"                \n",
					"                # Update modification timestamp for updates and deactivations\n",
					"                # Fixed: Using hash_id instead of HASH_ID\n",
					"                'AdtclmnModifiedDatetime': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNotNull(), current_timestamp())\n",
					"                                            .when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), current_timestamp())\n",
					"                                            .otherwise(col(\"main.AdtclmnModifiedDatetime\")),\n",
					"                \n",
					"                # Update modification process name similarly\n",
					"                'AdtclmnModifiedByProcessName': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNotNull(), lit(\"CDC_Update\"))\n",
					"                                                .when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), lit(\"CDC_Deactivate\"))\n",
					"                                                .otherwise(col(\"main.AdtclmnModifiedByProcessName\")),\n",
					"                \n",
					"                # Mark records as inactive if they exist only in main (they've been removed in source)\n",
					"                'AdtclmnRecordActiveInd': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), lit(0).cast(DecimalType(1, 0)))\n",
					"                                         .otherwise(lit(1).cast(DecimalType(1, 0)))\n",
					"            }\n",
					"            \n",
					"            # Get all columns from existing dataframe\n",
					"            all_columns = existing_df.columns\n",
					"            \n",
					"            # Combine columns with improved null/NaN handling\n",
					"            logger.info(\"Building merged dataframe with improved NaN handling\")\n",
					"            combined_columns = []\n",
					"            for c in all_columns:\n",
					"                if c in audit_columns:\n",
					"                    combined_columns.append(audit_columns[c].alias(c))\n",
					"                else:\n",
					"                    # Check column data type\n",
					"                    col_type = next((f.dataType for f in existing_df.schema.fields if f.name == c), StringType())\n",
					"                    \n",
					"                    # Handle numeric columns specially to avoid NaN\n",
					"                    if str(col_type).startswith((\"IntegerType\", \"LongType\", \"DoubleType\", \"FloatType\", \"DecimalType\")):\n",
					"                        combined_columns.append(\n",
					"                            when(col(f\"inc.{c}\").isNotNull() & ~isnan(col(f\"inc.{c}\")), col(f\"inc.{c}\"))\n",
					"                            .when(col(f\"main.{c}\").isNotNull() & ~isnan(col(f\"main.{c}\")), col(f\"main.{c}\"))\n",
					"                            .otherwise(lit(None).cast(col_type))\n",
					"                            .alias(c)\n",
					"                        )\n",
					"                    else:\n",
					"                        # For non-numeric columns, use regular coalesce with null awareness\n",
					"                        combined_columns.append(\n",
					"                            coalesce(col(f\"inc.{c}\"), col(f\"main.{c}\")).alias(c)\n",
					"                        )\n",
					"            \n",
					"            # Create the final DataFrame\n",
					"            logger.info(\"Creating final merged dataframe\")\n",
					"            df_full_derived = df_full_outer.select(combined_columns)\n",
					"            \n",
					"            # Final NaN cleanup on the merged data\n",
					"            logger.info(\"Performing final NaN cleanup\")\n",
					"            numeric_cols_final = [field.name for field in df_full_derived.schema.fields \n",
					"                               if str(field.dataType).startswith((\"IntegerType\", \"LongType\", \"DoubleType\", \"FloatType\", \"DecimalType\"))]\n",
					"            \n",
					"            for col_name in numeric_cols_final:\n",
					"                df_full_derived = df_full_derived.withColumn(\n",
					"                    col_name,\n",
					"                    when(isnan(col(col_name)), lit(None).cast(df_full_derived.schema[col_name].dataType))\n",
					"                    .otherwise(col(col_name))\n",
					"                )\n",
					"            \n",
					"            # IMPROVEMENT: Add explicit deduplication step after merging\n",
					"            logger.info(\"Checking for duplicates in final merged dataframe\")\n",
					"            before_dedup_count = df_full_derived.count()\n",
					"            distinct_hash_count = df_full_derived.select(\"hash_id\").distinct().count()\n",
					"            \n",
					"            if before_dedup_count > distinct_hash_count:\n",
					"                logger.warning(f\"Found {before_dedup_count - distinct_hash_count} duplicate hash_ids in merged result - applying deduplication\")\n",
					"                \n",
					"                # Create a window spec to keep the most recently modified record for each hash_id\n",
					"                window_spec = Window.partitionBy(\"hash_id\").orderBy(\n",
					"                    # First priority: active records\n",
					"                    desc(\"AdtclmnRecordActiveInd\"),\n",
					"                    # Second priority: last modified date\n",
					"                    desc(\"AdtclmnModifiedDatetime\"),\n",
					"                    # Third priority: creation date\n",
					"                    desc(\"AdtclmnFirstCreatedDatetime\")\n",
					"                )\n",
					"                \n",
					"                # Apply deduplication - keep only the first record for each hash_id\n",
					"                df_full_derived = df_full_derived.withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
					"                                           .filter(col(\"row_number\") == 1) \\\n",
					"                                           .drop(\"row_number\")\n",
					"                \n",
					"                logger.info(f\"After deduplication: {df_full_derived.count()} records \" +\n",
					"                            f\"(removed {before_dedup_count - df_full_derived.count()} duplicates)\")\n",
					"            else:\n",
					"                logger.info(\"No duplicates found in final merged dataframe\")\n",
					"            \n",
					"            # Log record count\n",
					"            logger.info(f\"Final merged dataframe has {df_full_derived.count()} records\")\n",
					"            \n",
					"            # Check for any remaining NaN values\n",
					"            for col_name in numeric_cols_final:\n",
					"                nan_count = df_full_derived.filter(isnan(col(col_name))).count()\n",
					"                if nan_count > 0:\n",
					"                    logger.warning(f\"Column {col_name} still has {nan_count} NaN values after cleanup\")\n",
					"            \n",
					"            # IMPROVEMENT: Final validation before writing\n",
					"            final_df_count = df_full_derived.count()\n",
					"            final_distinct_count = df_full_derived.select(\"hash_id\").distinct().count()\n",
					"            \n",
					"            if final_df_count > final_distinct_count:\n",
					"                logger.error(f\"CRITICAL: Still found {final_df_count - final_distinct_count} duplicates \" +\n",
					"                            f\"after deduplication. Forcing final deduplication.\")\n",
					"                \n",
					"                # Force final deduplication to ensure data integrity\n",
					"                df_full_derived = df_full_derived.dropDuplicates([\"hash_id\"])\n",
					"                logger.info(f\"After forced deduplication: {df_full_derived.count()} records\")\n",
					"            \n",
					"            # Write the merged data back to the Delta table\n",
					"            logger.info(f\"Writing merged data to Delta table at {delta_table_path}\")\n",
					"            df_full_derived.write \\\n",
					"                .mode(\"overwrite\") \\\n",
					"                .option(\"overwriteSchema\", \"true\") \\\n",
					"                .format(\"delta\") \\\n",
					"                .save(delta_table_path)\n",
					"            \n",
					"            logger.info(f\"Successfully merged and wrote data to Delta table at {delta_table_path}\")\n",
					"        \n",
					"        else:\n",
					"            logger.info(f\"Delta table does not exist. Creating a new Delta table at {delta_table_path}.\")\n",
					"            \n",
					"            # Add audit columns to source data\n",
					"            source_df = source_df.withColumn(\"AdtclmnFirstCreatedDatetime\", current_timestamp()) \\\n",
					"                               .withColumn(\"AdtclmnInsertedByProcessName\", lit(\"InitialLoad\")) \\\n",
					"                               .withColumn(\"AdtclmnModifiedDatetime\", lit(None).cast(\"timestamp\")) \\\n",
					"                               .withColumn(\"AdtclmnModifiedByProcessName\", lit(None).cast(\"string\")) \\\n",
					"                               .withColumn(\"AdtclmnRecordActiveInd\", lit(1).cast(DecimalType(1, 0)))\n",
					"            \n",
					"            # Final NaN cleanup before writing\n",
					"            logger.info(\"Performing final NaN cleanup before initial write\")\n",
					"            for col_name in numeric_cols:\n",
					"                source_df = source_df.withColumn(\n",
					"                    col_name,\n",
					"                    when(isnan(col(col_name)), lit(None).cast(source_df.schema[col_name].dataType))\n",
					"                    .otherwise(col(col_name))\n",
					"                )\n",
					"    \n",
					"            # IMPROVEMENT: Final check for duplicates before initial write\n",
					"            initial_count = source_df.count()\n",
					"            initial_distinct = source_df.select(\"hash_id\").distinct().count()\n",
					"            \n",
					"            if initial_count > initial_distinct:\n",
					"                logger.warning(f\"Found {initial_count - initial_distinct} duplicates before initial write - deduplicating\")\n",
					"                \n",
					"                # Deduplicate by keeping the most recent record\n",
					"                window_spec = Window.partitionBy(\"hash_id\").orderBy(desc(\"creation_date\"))\n",
					"                source_df = source_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                                     .filter(col(\"row_num\") == 1) \\\n",
					"                                     .drop(\"row_num\")\n",
					"                logger.info(f\"After deduplication: {source_df.count()} records\")\n",
					"            \n",
					"            # Write the source data as a new Delta table\n",
					"            logger.info(f\"Writing {source_df.count()} records to new Delta table\")\n",
					"            source_df.write \\\n",
					"                .mode(\"overwrite\") \\\n",
					"                .option(\"overwriteSchema\", \"true\") \\\n",
					"                .format(\"delta\") \\\n",
					"                .save(delta_table_path)\n",
					"            \n",
					"            logger.info(f\"Successfully created new Delta table at {delta_table_path}\")\n",
					"        \n",
					"        # NEW CODE: Create aggregated view for Power BI\n",
					"        logger.info(\"Creating aggregated view for Power BI\")\n",
					"        agg_success, agg_path = export_aggregated_data_for_powerbi(delta_table_path)\n",
					"        \n",
					"        if agg_success:\n",
					"            logger.info(f\"Successfully created aggregated view at {agg_path}\")\n",
					"        else:\n",
					"            logger.warning(\"Failed to create aggregated view for Power BI\")\n",
					"        \n",
					"        # Return success with both paths\n",
					"        return True, {\n",
					"            \"delta_table_path\": delta_table_path,\n",
					"            \"aggregated_path\": agg_path if agg_success else None\n",
					"        }\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error in data lake export process: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return False, None"
				],
				"execution_count": 46
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"# Update the main execution code to handle the new return format and provide more detailed reporting\n",
					"if __name__ == \"__main__\":\n",
					"    try:\n",
					"        logger.info(\"Starting voter data lake export process with CDC\")\n",
					"        print(\"=\" * 80)\n",
					"        print(\"VOTER DATA LAKE EXPORT WITH CDC\")\n",
					"        print(\"=\" * 80)\n",
					"        \n",
					"        # Execute the CDC function\n",
					"        success, paths = export_voters_to_datalake_with_cdc()\n",
					"        \n",
					"        if success:\n",
					"            print(\"\\n‚úÖ Data lake export with CDC completed successfully\")\n",
					"            \n",
					"            # Report on the Delta table\n",
					"            delta_path = paths['delta_table_path']\n",
					"            print(f\"\\nDelta files created/merged at: {delta_path}\")\n",
					"            \n",
					"            # Try to load and count the Delta table for verification\n",
					"            try:\n",
					"                delta_df = spark.read.format(\"delta\").load(delta_path)\n",
					"                record_count = delta_df.count()\n",
					"                distinct_count = delta_df.select(\"hash_id\").distinct().count()\n",
					"                \n",
					"                print(f\"Total records in Delta table: {record_count}\")\n",
					"                print(f\"Unique hash_ids in Delta table: {distinct_count}\")\n",
					"                \n",
					"                if record_count > distinct_count:\n",
					"                    print(f\"‚ö†Ô∏è WARNING: Found {record_count - distinct_count} duplicate records in final Delta table!\")\n",
					"                else:\n",
					"                    print(\"‚úÖ Verification successful: No duplicates found in final Delta table\")\n",
					"                \n",
					"                # Sample data\n",
					"                print(\"\\nSample records from Delta table:\")\n",
					"                delta_df.show(5, truncate=False)\n",
					"                \n",
					"            except Exception as e:\n",
					"                print(f\"Error verifying Delta table: {str(e)}\")\n",
					"            \n",
					"            # Report on Power BI aggregation\n",
					"            if paths.get('aggregated_path'):\n",
					"                agg_path = paths['aggregated_path']\n",
					"                print(f\"\\nAggregated Power BI data created at: {agg_path}\")\n",
					"                \n",
					"                # Try to load and count the aggregated data\n",
					"                try:\n",
					"                    agg_df = spark.read.format(\"delta\").load(agg_path)\n",
					"                    agg_count = agg_df.count()\n",
					"                    print(f\"Aggregation contains {agg_count} records with counts by creation_date and rec_num\")\n",
					"                    \n",
					"                    # Sample data\n",
					"                    print(\"\\nSample records from aggregated data:\")\n",
					"                    agg_df.show(5, truncate=False)\n",
					"                    \n",
					"                except Exception as e:\n",
					"                    print(f\"Error verifying aggregated data: {str(e)}\")\n",
					"            else:\n",
					"                print(\"\\n‚ö†Ô∏è Warning: Aggregated view for Power BI was not created\")\n",
					"            \n",
					"            print(\"\\n‚ú® Use these paths for your downstream processes once permissions are granted\")\n",
					"            \n",
					"        else:\n",
					"            print(\"\\n‚ùå Data lake export with CDC failed\")\n",
					"            print(\"Check the logs for more details on the error\")\n",
					"        \n",
					"        print(\"\\n\" + \"=\" * 80)\n",
					"        print(\"PROCESS COMPLETED\")\n",
					"        print(\"=\" * 80)\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Unhandled error in main execution: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        print(f\"\\n‚ùå CRITICAL ERROR: {str(e)}\")\n",
					"        print(\"Check the logs for detailed stack trace\")"
				],
				"execution_count": 47
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"delta_df = spark.read.format(\"delta\").load( f\"abfss://dl-juror-eric-voters-temp@baubaisadfsaprod.dfs.core.windows.net/voters_deduplicated_delta\")"
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"print(\"Loading PostgreSQL CSV export...\")\n",
					"postgres_csv_df = spark.read.option(\"header\", \"true\").csv(\n",
					"    \"abfss://juror-etl@baubaisadfsaprod.dfs.core.windows.net/exports/staging_voters.csv\"\n",
					")"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"# Since CSV only has hash_id column, just select it\n",
					"postgres_hash_ids = postgres_csv_df.select('hash_id')\n",
					"postgres_count = postgres_hash_ids.count()\n",
					"print(f\"PostgreSQL export records: {postgres_count:,}\")\n",
					"\n",
					"# === COMPARISON ANALYSIS ===\n",
					"print(\"\\n=== COMPARISON ANALYSIS ===\")\n",
					"\n",
					"# Partition both datasets for efficient operations\n",
					"delta_hash_ids = delta_df.select('hash_id')\n",
					"delta_hash_ids = delta_hash_ids.repartition(200, 'hash_id')\n",
					"postgres_hash_ids = postgres_hash_ids.repartition(200, 'hash_id')\n",
					"# 1. Find records in Delta but NOT in PostgreSQL (missing from PostgreSQL)\n",
					"missing_from_postgres = delta_hash_ids.subtract(postgres_hash_ids)\n",
					"missing_from_postgres_count = missing_from_postgres.count()\n",
					"print(f\"Records in Delta but MISSING from PostgreSQL: {missing_from_postgres_count:,}\")\n",
					""
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"# === VALIDATION ===\n",
					"\n",
					"# === EXPORT MISSING RECORDS ===\n",
					"if missing_from_postgres_count > 0:\n",
					"    # Get full records from Delta table for missing hash_ids\n",
					"    recovery_data = delta_df.join(missing_from_postgres, on='hash_id', how='inner')\n",
					"    \n",
					"    # Export all columns to CSV\n",
					"    recovery_data.coalesce(1).write.mode('overwrite') \\\n",
					"        .option('header', 'true') \\\n",
					"        .csv(\"abfss://juror-etl@baubaisadfsaprod.dfs.core.windows.net/exports/missing_records.csv\")\n",
					"    \n",
					"    print(f\"‚úÖ Exported {missing_from_postgres_count:,} records with all columns\")\n",
					"\n",
					"# === ANALYZE EXTRA RECORDS IN POSTGRESQL ===\n",
					"\n",
					"    print(f\"\\n=== EXTRA RECORDS IN POSTGRESQL ===\")\n",
					"    print(\"These records exist in PostgreSQL but not in your Delta table\")\n",
					"    print(\"This might indicate:\")\n",
					"    print(\"- Records added directly to PostgreSQL\")\n",
					"    print(\"- Different data processing versions\")\n",
					"    print(\"- Data that should be in Delta but isn't\")\n",
					"    \n",
					"    \n",
					"    # Export extra records for investigation\n",
					"    extra_output_path = \"abfss://juror-etl@baubaisadfsaprod.dfs.core.windows.net/exports/extra_in_postgres.csv\"\n",
					"    extra_in_postgres.coalesce(1).write.mode('overwrite') \\\n",
					"        .option('header', 'true') \\\n",
					"        .csv(extra_output_path)\n",
					"    \n",
					"    print(f\"Extra PostgreSQL hash_ids exported to: {extra_output_path}\")\n",
					""
				],
				"execution_count": 43
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import to_date, col, lit\n",
					"\n",
					"# Define your date range\n",
					"start_date = \"2025-01-08\"\n",
					"end_date = \"2025-01-09\"\n",
					"\n",
					"# Load Delta table\n",
					"\n",
					"\n",
					"# Filter using cast to string and parse as date\n",
					"filtered_df = delta_df.filter(\n",
					"    (col(\"creation_date\") == lit(\"20250212\")) |   # string comparison\n",
					"    (col(\"creation_date\") == lit(\"20250213\")) |\n",
					"    (col(\"creation_date\") == lit(\"20250218\"))\n",
					"    \n",
					")\n",
					"\n",
					"# Get all column names\n",
					"all_columns = filtered_df.columns\n",
					"\n",
					"# Keep all except the last 6\n",
					"columns_to_keep = all_columns[:-6]\n",
					"\n",
					"\n",
					"# Select only the columns you want\n",
					"filtered_trimmed_df = filtered_df.select(*columns_to_keep)\n",
					"\n",
					"# Show result\n",
					"display(filtered_trimmed_df)\n",
					""
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"import datetime\n",
					"# Configuration\n",
					"storage_account = \"baubaisadfsaprod\"\n",
					"input_df = filtered_trimmed_df\n",
					"output_dir = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/exports\"\n",
					"\n",
					"# Ensure output directory exists\n",
					"if not mssparkutils.fs.exists(output_dir):\n",
					"    mssparkutils.fs.mkdirs(output_dir)\n",
					"\n",
					"try:\n",
					"    print(f\"üîÑ Processing filtered DataFrame data\")\n",
					"    \n",
					"    # Use your existing DataFrame directly (no need to read from parquet)\n",
					"    df = input_df\n",
					"    record_count = df.count()\n",
					"    print(f\"‚úÖ Processing {record_count} records from filtered DataFrame\")\n",
					"    \n",
					"    # Generate a single CSV export\n",
					"    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
					"    temp_output_dir = f\"{output_dir}/temp_{timestamp}\"\n",
					"    \n",
					"    # Write to temporary CSV folder\n",
					"    # You can remove coalesce(1) if you don't need a single file, or keep it if you do\n",
					"    df.coalesce(1).write \\\n",
					"        .option(\"header\", \"true\") \\\n",
					"        .option(\"quoteAll\", \"true\") \\\n",
					"        .mode(\"overwrite\") \\\n",
					"        .csv(temp_output_dir)\n",
					"    \n",
					"    # Locate and rename the .csv file\n",
					"    files_written = mssparkutils.fs.ls(temp_output_dir)\n",
					"    for f in files_written:\n",
					"        if f.path.endswith(\".csv\") and \"part-\" in f.path:\n",
					"            final_csv_path = f\"{output_dir}/voters_export_{timestamp}.csv\"\n",
					"            mssparkutils.fs.cp(f.path, final_csv_path)\n",
					"            print(f\"üìÑ Exported to: {final_csv_path}\")\n",
					"            break\n",
					"    \n",
					"    # Clean up temp folder\n",
					"    mssparkutils.fs.rm(temp_output_dir, recurse=True)\n",
					"    \n",
					"except Exception as e:\n",
					"    print(f\"‚ùå Failed to process: {str(e)}\")\n",
					"    import traceback\n",
					"    print(traceback.format_exc())"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"source": [
					"display(filtered_trimmed_df.count())"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					},
					"collapsed": false
				},
				"source": [
					"# Check L2 Delta table\n",
					"l2_df = spark.read.format(\"delta\").load(\"abfss://juror-etl@baubaisadfsaprod.dfs.core.windows.net/voters_transformed\")\n",
					"\n",
					"l2_check = l2_df.filter(\n",
					"    (col(\"rec_num\") == lit(\"110\"))  \n",
					"   # (col(\"creation_date\") == lit(\"20241216\"))\n",
					")\n",
					"\n",
					"print(f\"L2 records for target dates: {l2_check.count()}\")\n",
					"display(l2_check)"
				],
				"execution_count": 58
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import count\n",
					"\n",
					"final_count = delta_df.groupBy('creation_date','rec_num') \\\n",
					"    .agg(count('rec_num')) \\\n",
					"    .orderBy('rec_num')\n",
					"\n",
					"display(final_count)"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					"display(delta_df.count())"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"display(filtered_trimmed_df)"
				],
				"execution_count": 51
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"import datetime\n",
					"import time\n",
					"import os\n",
					"import re\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col, when, lit, regexp_replace, trim, upper, substring, to_date, date_format, expr\n",
					"from pyspark.sql.types import StringType, IntegerType, TimestampType, LongType, DateType\n",
					"import pyspark.sql.functions as F\n",
					"\n",
					"storage_account = \"baubaisadfsaprod\"\n",
					"output_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/exports\"\n",
					"\n",
					"# Create timestamp for unique filename\n",
					"timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
					"\n",
					"# Write to temporary CSV folder\n",
					"temp_output_dir = f\"{output_path}/temp_{timestamp}\"\n",
					"filtered_trimmed_df.coalesce(1).write \\\n",
					"    .option(\"header\", \"true\") \\\n",
					"    .option(\"quoteAll\", \"true\") \\\n",
					"    .option(\"escape\", \"\\\\\") \\\n",
					"    .mode(\"overwrite\") \\\n",
					"    .csv(temp_output_dir)\n",
					"\n",
					"# Locate and rename the .csv file\n",
					"files_written = mssparkutils.fs.ls(temp_output_dir)\n",
					"csv_found = False\n",
					"\n",
					"for f in files_written:\n",
					"    if f.path.endswith(\".csv\") and \"part-\" in f.path:\n",
					"        final_csv_path = f\"{output_path}/voters_export_{timestamp}.csv\"\n",
					"        mssparkutils.fs.cp(f.path, final_csv_path)\n",
					"        print(f\"üìÑ Exported to: {final_csv_path}\")\n",
					"        csv_found = True\n",
					"        break\n",
					"\n",
					"# Check if a CSV file was found and copied\n",
					"if not csv_found:\n",
					"    print(\"‚ö†Ô∏è No CSV file was found in the output directory\")\n",
					"    \n",
					"# Clean up temp directory\n",
					"mssparkutils.fs.rm(temp_output_dir, recurse=True)"
				],
				"execution_count": 64
			}
		]
	}
}