{
	"name": "L3_Er_Juror_DataLake_ErickSchema",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkstg",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "05ce3d7e-fa49-490e-8817-6eccf98e899f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/74dacd4f-a248-45bb-a2f0-af700dc4cf68/resourceGroups/baubais-data-factory-rg-stg/providers/Microsoft.Synapse/workspaces/baubais-synapse-stg/bigDataPools/sparkstg",
				"name": "sparkstg",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-stg.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkstg",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### **L3_Er_Juror_DataLake_ErickSchema: Voters Data Lake Export with CDC**\n",
					"\n",
					"This script implements an **ETL (Extract, Transform, Load)** pipeline for exporting and managing voter data in Azure Data Lake Storage using **Delta Lake**. It includes **Change Data Capture (CDC)** logic to merge new data with existing Delta tables, ensuring data consistency and auditability.\n",
					"\n",
					"---\n",
					"\n",
					"### **Overview**\n",
					"\n",
					"The script processes voter data from a source directory, validates it, and writes it to a Delta table in Azure Data Lake Storage. If the Delta table already exists, it merges the new data with the existing data using CDC logic. The script also tracks metadata such as creation and modification timestamps, process names, and record activity indicators.\n",
					"\n",
					"---\n",
					"\n",
					"### **Key Steps in the ETL Process**\n",
					"\n",
					"#### **1. Initialization**\n",
					"- **Logging Configuration**:\n",
					"  - Configures logging to track the ETL process and writes logs to the console.\n",
					"- **Spark Session Initialization**:\n",
					"  - Initializes a Spark session with Delta Lake support for managing Delta tables.\n",
					"\n",
					"---\n",
					"\n",
					"#### **2. Data Ingestion**\n",
					"- **Source Data Path**:\n",
					"  - Reads voter data from the specified Azure Data Lake Storage path (`source_path`).\n",
					"- **Delta Table Path**:\n",
					"  - Specifies the path where the Delta table is stored (`delta_table_path`).\n",
					"\n",
					"---\n",
					"\n",
					"#### **3. Data Validation**\n",
					"- **Primary Key Validation**:\n",
					"  - Ensures there are no duplicate or null values in the primary key column (`hash_id`):\n",
					"    ```python\n",
					"    assert source_df.select(\"hash_id\").distinct().count() == source_df.count(), \"Duplicate primary keys found in source data\"\n",
					"    assert source_df.na.drop(subset=[\"hash_id\"]).count() == source_df.count(), \"Null values found in primary keys (hash_id)\"\n",
					"    ```\n",
					"\n",
					"---\n",
					"\n",
					"#### **4. Change Data Capture (CDC) Logic**\n",
					"- **Delta Table Check**:\n",
					"  - Checks if the Delta table already exists in the specified path:\n",
					"    ```python\n",
					"    if mssparkutils.fs.exists(delta_table_path):\n",
					"    ```\n",
					"\n",
					"- **Merging New Data with Existing Data**:\n",
					"  - If the Delta table exists:\n",
					"    - Reads the existing Delta table into a DataFrame (`existing_df`).\n",
					"    - Performs a **full outer join** between the existing data (`main`) and the new data (`inc`) on the primary key (`hash_id`):\n",
					"      ```python\n",
					"      df_full_outer = existing_df.alias(\"main\").join(\n",
					"          source_df.alias(\"inc\"), \n",
					"          \"hash_id\", \n",
					"          \"outer\"\n",
					"      )\n",
					"      ```\n",
					"    - Defines **audit columns** to track metadata such as creation and modification timestamps, process names, and record activity indicators:\n",
					"      ```python\n",
					"      audit_columns = {\n",
					"          'AdtclmnFirstCreatedDatetime': when(col(\"main.hash_id\").isNull(), col(\"inc.AdtclmnFirstCreatedDatetime\"))\n",
					"                                          .otherwise(col(\"main.AdtclmnFirstCreatedDatetime\")),\n",
					"          'AdtclmnInsertedByProcessName': when(col(\"main.hash_id\").isNull(), col(\"inc.AdtclmnInsertedByProcessName\"))\n",
					"                                           .otherwise(col(\"main.AdtclmnInsertedByProcessName\")),\n",
					"          'AdtclmnModifiedDatetime': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNotNull() & \n",
					"                                           (col(\"main.HASH_ID\") != col(\"inc.HASH_ID\")), current_timestamp())\n",
					"                                      .when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), current_timestamp())\n",
					"                                      .otherwise(None),\n",
					"          'AdtclmnModifiedByProcessName': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNotNull() & \n",
					"                                               (col(\"main.HASH_ID\") != col(\"inc.HASH_ID\")), col(\"inc.AdtclmnInsertedByProcessName\"))\n",
					"                                          .when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), col(\"inc.AdtclmnInsertedByProcessName\"))\n",
					"                                          .otherwise(None),\n",
					"          'AdtclmnRecordActiveInd': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), 0)\n",
					"                                     .otherwise(1).cast(DecimalType(1, 0))\n",
					"      }\n",
					"      ```\n",
					"    - Combines the columns from the existing and new data, applying the audit logic:\n",
					"      ```python\n",
					"      combined_columns = [\n",
					"          coalesce(col(f\"inc.{c}\"), col(f\"main.{c}\")).alias(c) if c not in audit_columns \n",
					"          else audit_columns[c].alias(c) \n",
					"          for c in existing_df.columns\n",
					"      ]\n",
					"      df_full_derived = df_full_outer.select(combined_columns)\n",
					"      ```\n",
					"    - Writes the merged data back to the Delta table:\n",
					"      ```python\n",
					"      df_full_derived.write \\\n",
					"          .mode(\"overwrite\") \\\n",
					"          .option(\"overwriteSchema\", \"true\") \\\n",
					"          .format(\"delta\") \\\n",
					"          .save(delta_table_path)\n",
					"      ```\n",
					"\n",
					"- **Creating a New Delta Table**:\n",
					"  - If the Delta table does not exist:\n",
					"    - Adds audit columns to the source data:\n",
					"      ```python\n",
					"      source_df = source_df.withColumn(\"AdtclmnFirstCreatedDatetime\", current_timestamp()) \\\n",
					"                           .withColumn(\"AdtclmnInsertedByProcessName\", lit(\"InitialLoad\")) \\\n",
					"                           .withColumn(\"AdtclmnModifiedDatetime\", lit(None).cast(\"timestamp\")) \\\n",
					"                           .withColumn(\"AdtclmnModifiedByProcessName\", lit(None).cast(\"string\")) \\\n",
					"                           .withColumn(\"AdtclmnRecordActiveInd\", lit(1).cast(DecimalType(1, 0)))\n",
					"      ```\n",
					"    - Writes the source data as a new Delta table:\n",
					"      ```python\n",
					"      source_df.write \\\n",
					"          .mode(\"overwrite\") \\\n",
					"          .option(\"overwriteSchema\", \"true\") \\\n",
					"          .format(\"delta\") \\\n",
					"          .save(delta_table_path)\n",
					"      ```\n",
					"\n",
					"---\n",
					"\n",
					"#### **5. Error Handling**\n",
					"- Logs errors and stack traces if any exceptions occur during the process:\n",
					"  ```python\n",
					"  except Exception as e:\n",
					"      logger.error(f\"Error in data lake export process: {str(e)}\")\n",
					"      import traceback\n",
					"      logger.error(traceback.format_exc())\n",
					"  ```\n",
					"\n",
					"---\n",
					"\n",
					"### **Key Features**\n",
					"\n",
					"1. **Delta Lake Integration**:\n",
					"   - Uses Delta Lake for efficient data storage and management.\n",
					"   - Supports ACID transactions, schema enforcement, and time travel.\n",
					"\n",
					"2. **Change Data Capture (CDC)**:\n",
					"   - Merges new data with existing data using a full outer join.\n",
					"   - Tracks metadata such as creation and modification timestamps.\n",
					"\n",
					"3. **Primary Key Validation**:\n",
					"   - Ensures data integrity by checking for duplicate and null values in the primary key (`hash_id`).\n",
					"\n",
					"4. **Audit Columns**:\n",
					"   - Tracks metadata for each record, including:\n",
					"     - `AdtclmnFirstCreatedDatetime`\n",
					"     - `AdtclmnInsertedByProcessName`\n",
					"     - `AdtclmnModifiedDatetime`\n",
					"     - `AdtclmnModifiedByProcessName`\n",
					"     - `AdtclmnRecordActiveInd`\n",
					"\n",
					"5. **Error Handling**:\n",
					"   - Logs errors and stack traces for debugging.\n",
					"\n",
					"---\n",
					"\n",
					"### **Output**\n",
					"\n",
					"1. **Delta Table**:\n",
					"   - The final data is stored as a Delta table in Azure Data Lake Storage at the specified path (`delta_table_path`).\n",
					"\n",
					"2. **Audit Metadata**:\n",
					"   - Each record includes metadata for tracking creation, modification, and activity status.\n",
					"\n",
					"---\n",
					"\n",
					"### **Next Steps**\n",
					"\n",
					"1. **Run the Script**:\n",
					"   - Execute the `export_voters_to_datalake_with_cdc()` function to process and export the voter data.\n",
					"\n",
					"2. **Verify the Output**:\n",
					"   - Check the Delta table in Azure Data Lake Storage to ensure the data is written correctly.\n",
					"\n",
					"3. **Query the Delta Table**:\n",
					"   - Use Spark to query the Delta table:\n",
					"     ```python\n",
					"     delta_df = spark.read.format(\"delta\").load(delta_table_path)\n",
					"     delta_df.show()\n",
					"     ```\n",
					"\n",
					"4. **Optimize the Delta Table**:\n",
					"   - Periodically optimize the Delta table to improve performance:\n",
					"     ```python\n",
					"     from delta.tables import DeltaTable\n",
					"     delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
					"     delta_table.optimize().executeCompaction()\n",
					"     ```\n",
					"\n",
					"5. **Time Travel**:\n",
					"   - Use Delta Lake's time travel feature to query historical versions of the data:\n",
					"     ```python\n",
					"     spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(delta_table_path).show()\n",
					"     ```\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"from notebookutils import mssparkutils\n",
					"import logging\n",
					"from datetime import datetime\n",
					"from pyspark.sql.types import DecimalType\n",
					"from pyspark.sql.functions import col, when, coalesce, current_timestamp"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"source": [
					"# Configure logging\n",
					"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"logger = logging.getLogger()\n",
					"\n",
					"# Initialize Spark session with Delta Lake support\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"Voters Data Lake Export\") \\\n",
					"    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
					"    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
					"    .getOrCreate()"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"source": [
					"def export_voters_to_datalake_with_cdc():\n",
					"    \"\"\"\n",
					"    Exports voters_deduplicated data to a Delta table in the data lake blob storage.\n",
					"    Implements CDC logic to merge new data with the existing Delta table.\n",
					"    \"\"\"\n",
					"    try:\n",
					"        # Initialize storage paths\n",
					"        storage_account = \"baubaisadfsastg\"\n",
					"        source_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_deduplicated\"\n",
					"        delta_table_path = f\"abfss://dl-juror-eric-voters-temp@{storage_account}.dfs.core.windows.net/voters_deduplicated_delta\"\n",
					"        \n",
					"        # Read source data\n",
					"        logger.info(f\"Reading voter data from {source_path}\")\n",
					"        source_df = spark.read.parquet(source_path)\n",
					"        record_count = source_df.count()\n",
					"        logger.info(f\"Loaded {record_count} voter records\")\n",
					"        \n",
					"        # Check for duplicates and null values in primary key (hash_id)\n",
					"        logger.info(\"Validating primary key constraints on hash_id\")\n",
					"        assert source_df.select(\"hash_id\").distinct().count() == source_df.count(), \"Duplicate primary keys found in source data\"\n",
					"        assert source_df.na.drop(subset=[\"hash_id\"]).count() == source_df.count(), \"Null values found in primary keys (hash_id)\"\n",
					"        \n",
					"        # Check if Delta table exists\n",
					"        if mssparkutils.fs.exists(delta_table_path):\n",
					"            logger.info(f\"Delta table exists at {delta_table_path}. Merging new data.\")\n",
					"            \n",
					"            # Read existing Delta table\n",
					"            existing_df = spark.read.format(\"delta\").load(delta_table_path)\n",
					"            \n",
					"            # Perform full outer join for CDC\n",
					"            df_full_outer = existing_df.alias(\"main\").join(\n",
					"                source_df.alias(\"inc\"), \n",
					"                \"hash_id\",  # Join on primary key\n",
					"                \"outer\"\n",
					"            )\n",
					"            \n",
					"            # Define audit columns\n",
					"            audit_columns = {\n",
					"                'AdtclmnFirstCreatedDatetime': when(col(\"main.hash_id\").isNull(), col(\"inc.AdtclmnFirstCreatedDatetime\"))\n",
					"                                                .otherwise(col(\"main.AdtclmnFirstCreatedDatetime\")),\n",
					"                'AdtclmnInsertedByProcessName': when(col(\"main.hash_id\").isNull(), col(\"inc.AdtclmnInsertedByProcessName\"))\n",
					"                                                 .otherwise(col(\"main.AdtclmnInsertedByProcessName\")),\n",
					"                'AdtclmnModifiedDatetime': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNotNull() & \n",
					"                                                 (col(\"main.HASH_ID\") != col(\"inc.HASH_ID\")), current_timestamp())\n",
					"                                            .when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), current_timestamp())\n",
					"                                            .otherwise(None),\n",
					"                'AdtclmnModifiedByProcessName': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNotNull() & \n",
					"                                                     (col(\"main.HASH_ID\") != col(\"inc.HASH_ID\")), col(\"inc.AdtclmnInsertedByProcessName\"))\n",
					"                                                .when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), col(\"inc.AdtclmnInsertedByProcessName\"))\n",
					"                                                .otherwise(None)\n",
					"            }\n",
					"            \n",
					"            # Combine columns with audit logic\n",
					"            combined_columns = [\n",
					"                coalesce(col(f\"inc.{c}\"), col(f\"main.{c}\")).alias(c) if c not in audit_columns \n",
					"                else audit_columns[c].alias(c) \n",
					"                for c in existing_df.columns\n",
					"            ]\n",
					"            \n",
					"            # Create the final DataFrame\n",
					"            df_full_derived = df_full_outer.select(combined_columns)\n",
					"            \n",
					"            # Write the merged data back to the Delta table\n",
					"            logger.info(f\"Writing merged data to Delta table at {delta_table_path}\")\n",
					"            df_full_derived.write \\\n",
					"                .mode(\"overwrite\") \\\n",
					"                .option(\"overwriteSchema\", \"true\") \\\n",
					"                .format(\"delta\") \\\n",
					"                .save(delta_table_path)\n",
					"            \n",
					"            logger.info(f\"Successfully merged and wrote data to Delta table at {delta_table_path}\")\n",
					"        else:\n",
					"            logger.info(f\"Delta table does not exist. Creating a new Delta table at {delta_table_path}.\")\n",
					"            \n",
					"            # Add audit columns to source data\n",
					"            source_df = source_df.withColumn(\"AdtclmnFirstCreatedDatetime\", current_timestamp()) \\\n",
					"                                 .withColumn(\"AdtclmnInsertedByProcessName\", lit(\"InitialLoad\")) \\\n",
					"                                 .withColumn(\"AdtclmnModifiedDatetime\", lit(None).cast(\"timestamp\")) \\\n",
					"                                 .withColumn(\"AdtclmnModifiedByProcessName\", lit(None).cast(\"string\"))\n",
					"    \n",
					"            # Write the source data as a new Delta table\n",
					"            source_df.write \\\n",
					"                .mode(\"overwrite\") \\\n",
					"                .option(\"overwriteSchema\", \"true\") \\\n",
					"                .format(\"delta\") \\\n",
					"                .save(delta_table_path)\n",
					"            \n",
					"            logger.info(f\"Successfully created new Delta table at {delta_table_path}\")\n",
					"        \n",
					"        # Return success\n",
					"        return True, delta_table_path\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error in data lake export process: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return False, None\n",
					"\n",
					"# Execute the export with CDC logic\n",
					"if __name__ == \"__main__\":\n",
					"    success, table_path = export_voters_to_datalake_with_cdc()\n",
					"    if success:\n",
					"        print(f\"Data lake export with CDC completed successfully\")\n",
					"        print(f\"Delta files created/merged at: {table_path}\")\n",
					"        print(f\"Use this path for your downstream processes once permissions are granted\")\n",
					"    else:\n",
					"        print(\"Data lake export with CDC failed\")\n",
					""
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"source": [
					"delta_df = spark.read.format(\"delta\").load( f\"abfss://dl-juror-eric-voters-temp@baubaisadfsastg.dfs.core.windows.net/voters_deduplicated_delta\")"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"display(delta_df)"
				],
				"execution_count": 26
			}
		]
	}
}