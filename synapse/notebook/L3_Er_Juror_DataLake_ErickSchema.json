{
	"name": "L3_Er_Juror_DataLake_ErickSchema",
	"properties": {
		"folder": {
			"name": "crime"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "21afe92d-68e8-4098-8216-2b4c609a92de"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### **L3_Er_Juror_DataLake_ErickSchema: Voters Data Lake Export with CDC**\n",
					"\n",
					"This script implements an **ETL (Extract, Transform, Load)** pipeline for exporting and managing voter data in Azure Data Lake Storage using **Delta Lake**. It includes **Change Data Capture (CDC)** logic to merge new data with existing Delta tables, ensuring data consistency and auditability.\n",
					"\n",
					"---\n",
					"\n",
					"### **Overview**\n",
					"\n",
					"The script processes voter data from a source directory, validates it, and writes it to a Delta table in Azure Data Lake Storage. If the Delta table already exists, it merges the new data with the existing data using CDC logic. The script also tracks metadata such as creation and modification timestamps, process names, and record activity indicators.\n",
					"\n",
					"---\n",
					"\n",
					"### **Key Steps in the ETL Process**\n",
					"\n",
					"#### **1. Initialization**\n",
					"- **Logging Configuration**:\n",
					"  - Configures logging to track the ETL process and writes logs to the console.\n",
					"- **Spark Session Initialization**:\n",
					"  - Initializes a Spark session with Delta Lake support for managing Delta tables.\n",
					"\n",
					"---\n",
					"\n",
					"#### **2. Data Ingestion**\n",
					"- **Source Data Path**:\n",
					"  - Reads voter data from the specified Azure Data Lake Storage path (`source_path`).\n",
					"- **Delta Table Path**:\n",
					"  - Specifies the path where the Delta table is stored (`delta_table_path`).\n",
					"\n",
					"---\n",
					"\n",
					"#### **3. Data Validation**\n",
					"- **Primary Key Validation**:\n",
					"  - Ensures there are no duplicate or null values in the primary key column (`hash_id`):\n",
					"    ```python\n",
					"    assert source_df.select(\"hash_id\").distinct().count() == source_df.count(), \"Duplicate primary keys found in source data\"\n",
					"    assert source_df.na.drop(subset=[\"hash_id\"]).count() == source_df.count(), \"Null values found in primary keys (hash_id)\"\n",
					"    ```\n",
					"\n",
					"---\n",
					"\n",
					"#### **4. Change Data Capture (CDC) Logic**\n",
					"- **Delta Table Check**:\n",
					"  - Checks if the Delta table already exists in the specified path:\n",
					"    ```python\n",
					"    if mssparkutils.fs.exists(delta_table_path):\n",
					"    ```\n",
					"\n",
					"- **Merging New Data with Existing Data**:\n",
					"  - If the Delta table exists:\n",
					"    - Reads the existing Delta table into a DataFrame (`existing_df`).\n",
					"    - Performs a **full outer join** between the existing data (`main`) and the new data (`inc`) on the primary key (`hash_id`):\n",
					"      ```python\n",
					"      df_full_outer = existing_df.alias(\"main\").join(\n",
					"          source_df.alias(\"inc\"), \n",
					"          \"hash_id\", \n",
					"          \"outer\"\n",
					"      )\n",
					"      ```\n",
					"    - Defines **audit columns** to track metadata such as creation and modification timestamps, process names, and record activity indicators:\n",
					"      ```python\n",
					"      audit_columns = {\n",
					"          'AdtclmnFirstCreatedDatetime': when(col(\"main.hash_id\").isNull(), col(\"inc.AdtclmnFirstCreatedDatetime\"))\n",
					"                                          .otherwise(col(\"main.AdtclmnFirstCreatedDatetime\")),\n",
					"          'AdtclmnInsertedByProcessName': when(col(\"main.hash_id\").isNull(), col(\"inc.AdtclmnInsertedByProcessName\"))\n",
					"                                           .otherwise(col(\"main.AdtclmnInsertedByProcessName\")),\n",
					"          'AdtclmnModifiedDatetime': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNotNull() & \n",
					"                                           (col(\"main.HASH_ID\") != col(\"inc.HASH_ID\")), current_timestamp())\n",
					"                                      .when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), current_timestamp())\n",
					"                                      .otherwise(None),\n",
					"          'AdtclmnModifiedByProcessName': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNotNull() & \n",
					"                                               (col(\"main.HASH_ID\") != col(\"inc.HASH_ID\")), col(\"inc.AdtclmnInsertedByProcessName\"))\n",
					"                                          .when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), col(\"inc.AdtclmnInsertedByProcessName\"))\n",
					"                                          .otherwise(None),\n",
					"          'AdtclmnRecordActiveInd': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), 0)\n",
					"                                     .otherwise(1).cast(DecimalType(1, 0))\n",
					"      }\n",
					"      ```\n",
					"    - Combines the columns from the existing and new data, applying the audit logic:\n",
					"      ```python\n",
					"      combined_columns = [\n",
					"          coalesce(col(f\"inc.{c}\"), col(f\"main.{c}\")).alias(c) if c not in audit_columns \n",
					"          else audit_columns[c].alias(c) \n",
					"          for c in existing_df.columns\n",
					"      ]\n",
					"      df_full_derived = df_full_outer.select(combined_columns)\n",
					"      ```\n",
					"    - Writes the merged data back to the Delta table:\n",
					"      ```python\n",
					"      df_full_derived.write \\\n",
					"          .mode(\"overwrite\") \\\n",
					"          .option(\"overwriteSchema\", \"true\") \\\n",
					"          .format(\"delta\") \\\n",
					"          .save(delta_table_path)\n",
					"      ```\n",
					"\n",
					"- **Creating a New Delta Table**:\n",
					"  - If the Delta table does not exist:\n",
					"    - Adds audit columns to the source data:\n",
					"      ```python\n",
					"      source_df = source_df.withColumn(\"AdtclmnFirstCreatedDatetime\", current_timestamp()) \\\n",
					"                           .withColumn(\"AdtclmnInsertedByProcessName\", lit(\"InitialLoad\")) \\\n",
					"                           .withColumn(\"AdtclmnModifiedDatetime\", lit(None).cast(\"timestamp\")) \\\n",
					"                           .withColumn(\"AdtclmnModifiedByProcessName\", lit(None).cast(\"string\")) \\\n",
					"                           .withColumn(\"AdtclmnRecordActiveInd\", lit(1).cast(DecimalType(1, 0)))\n",
					"      ```\n",
					"    - Writes the source data as a new Delta table:\n",
					"      ```python\n",
					"      source_df.write \\\n",
					"          .mode(\"overwrite\") \\\n",
					"          .option(\"overwriteSchema\", \"true\") \\\n",
					"          .format(\"delta\") \\\n",
					"          .save(delta_table_path)\n",
					"      ```\n",
					"\n",
					"---\n",
					"\n",
					"#### **5. Error Handling**\n",
					"- Logs errors and stack traces if any exceptions occur during the process:\n",
					"  ```python\n",
					"  except Exception as e:\n",
					"      logger.error(f\"Error in data lake export process: {str(e)}\")\n",
					"      import traceback\n",
					"      logger.error(traceback.format_exc())\n",
					"  ```\n",
					"\n",
					"---\n",
					"\n",
					"### **Key Features**\n",
					"\n",
					"1. **Delta Lake Integration**:\n",
					"   - Uses Delta Lake for efficient data storage and management.\n",
					"   - Supports ACID transactions, schema enforcement, and time travel.\n",
					"\n",
					"2. **Change Data Capture (CDC)**:\n",
					"   - Merges new data with existing data using a full outer join.\n",
					"   - Tracks metadata such as creation and modification timestamps.\n",
					"\n",
					"3. **Primary Key Validation**:\n",
					"   - Ensures data integrity by checking for duplicate and null values in the primary key (`hash_id`).\n",
					"\n",
					"4. **Audit Columns**:\n",
					"   - Tracks metadata for each record, including:\n",
					"     - `AdtclmnFirstCreatedDatetime`\n",
					"     - `AdtclmnInsertedByProcessName`\n",
					"     - `AdtclmnModifiedDatetime`\n",
					"     - `AdtclmnModifiedByProcessName`\n",
					"     - `AdtclmnRecordActiveInd`\n",
					"\n",
					"5. **Error Handling**:\n",
					"   - Logs errors and stack traces for debugging.\n",
					"\n",
					"---\n",
					"\n",
					"### **Output**\n",
					"\n",
					"1. **Delta Table**:\n",
					"   - The final data is stored as a Delta table in Azure Data Lake Storage at the specified path (`delta_table_path`).\n",
					"\n",
					"2. **Audit Metadata**:\n",
					"   - Each record includes metadata for tracking creation, modification, and activity status.\n",
					"\n",
					"---\n",
					"\n",
					"### **Next Steps**\n",
					"\n",
					"1. **Run the Script**:\n",
					"   - Execute the `export_voters_to_datalake_with_cdc()` function to process and export the voter data.\n",
					"\n",
					"2. **Verify the Output**:\n",
					"   - Check the Delta table in Azure Data Lake Storage to ensure the data is written correctly.\n",
					"\n",
					"3. **Query the Delta Table**:\n",
					"   - Use Spark to query the Delta table:\n",
					"     ```python\n",
					"     delta_df = spark.read.format(\"delta\").load(delta_table_path)\n",
					"     delta_df.show()\n",
					"     ```\n",
					"\n",
					"4. **Optimize the Delta Table**:\n",
					"   - Periodically optimize the Delta table to improve performance:\n",
					"     ```python\n",
					"     from delta.tables import DeltaTable\n",
					"     delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
					"     delta_table.optimize().executeCompaction()\n",
					"     ```\n",
					"\n",
					"5. **Time Travel**:\n",
					"   - Use Delta Lake's time travel feature to query historical versions of the data:\n",
					"     ```python\n",
					"     spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(delta_table_path).show()\n",
					"     ```\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"from notebookutils import mssparkutils\n",
					"import logging\n",
					"from datetime import datetime\n",
					"from pyspark.sql.types import DecimalType\n",
					"from pyspark.sql.functions import col, when, coalesce, current_timestamp\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, col, desc"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Configure logging\n",
					"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"logger = logging.getLogger()\n",
					"\n",
					"# Initialize Spark session with Delta Lake support\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"Voters Data Lake Export\") \\\n",
					"    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
					"    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
					"    .getOrCreate()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def export_aggregated_data_for_powerbi(delta_table_path):\n",
					"    \"\"\"\n",
					"    Creates an aggregated view of voter data and exports it to Power BI blob storage.\n",
					"    \n",
					"    Args:\n",
					"        delta_table_path (str): Path to the source delta table with voter data\n",
					"        \n",
					"    Returns:\n",
					"        tuple: (success_flag, output_path)\n",
					"    \"\"\"\n",
					"    try:\n",
					"        # Define the output path for the Power BI delta table\n",
					"        storage_account = \"baubaisadfsastg\"\n",
					"        powerbi_delta_path = f\"abfss://dl-juror-eric-power-bi@{storage_account}.dfs.core.windows.net/er_juror_report\"\n",
					"        \n",
					"        logger.info(f\"Creating aggregated view for Power BI from {delta_table_path}\")\n",
					"        \n",
					"        # Read the source delta table\n",
					"        delta_df = spark.read.format(\"delta\").load(delta_table_path)\n",
					"        \n",
					"        # Create the aggregated view as specified in the SQL query\n",
					"        from pyspark.sql.functions import count, col\n",
					"        \n",
					"        # Ensure creation_date and rec_num columns exist\n",
					"        if \"creation_date\" not in delta_df.columns or \"rec_num\" not in delta_df.columns:\n",
					"            logger.warning(\"Required columns (creation_date, rec_num) not found in source data\")\n",
					"            missing_cols = []\n",
					"            if \"creation_date\" not in delta_df.columns:\n",
					"                missing_cols.append(\"creation_date\")\n",
					"            if \"rec_num\" not in delta_df.columns:\n",
					"                missing_cols.append(\"rec_num\")\n",
					"            logger.warning(f\"Missing columns: {', '.join(missing_cols)}\")\n",
					"            \n",
					"            # Show available columns for debugging\n",
					"            logger.info(f\"Available columns: {', '.join(delta_df.columns)}\")\n",
					"            \n",
					"            # Try to use alternative columns if available\n",
					"            if \"creation_date\" not in delta_df.columns and \"CreationDate\" in delta_df.columns:\n",
					"                logger.info(\"Using 'CreationDate' as alternative to 'creation_date'\")\n",
					"                delta_df = delta_df.withColumnRenamed(\"CreationDate\", \"creation_date\")\n",
					"            \n",
					"            if \"rec_num\" not in delta_df.columns and \"LA_Code\" in delta_df.columns:\n",
					"                logger.info(\"Using 'LA_Code' as alternative to 'rec_num'\")\n",
					"                delta_df = delta_df.withColumnRenamed(\"LA_Code\", \"rec_num\")\n",
					"        \n",
					"        # Check if the needed columns are now available\n",
					"        if \"creation_date\" not in delta_df.columns or \"rec_num\" not in delta_df.columns:\n",
					"            logger.error(\"Cannot create aggregation: required columns still not available\")\n",
					"            return False, None\n",
					"        \n",
					"        # Create the aggregated dataframe\n",
					"        logger.info(\"Creating aggregation by creation_date and rec_num\")\n",
					"        aggregated_df = delta_df.groupBy(\"creation_date\", \"rec_num\") \\\n",
					"                                .agg(count(\"*\").alias(\"row_count\"))\n",
					"        \n",
					"        # Log the size of the aggregated data\n",
					"        agg_count = aggregated_df.count()\n",
					"        logger.info(f\"Created aggregated dataframe with {agg_count} rows\")\n",
					"        \n",
					"        # Extract directory from path (without using os.path)\n",
					"        powerbi_dir_path = \"/\".join(powerbi_delta_path.split(\"/\")[:-1])\n",
					"        \n",
					"        # Ensure the destination directory exists\n",
					"        if not mssparkutils.fs.exists(powerbi_dir_path):\n",
					"            logger.info(f\"Creating directory: {os.path.dirname(powerbi_delta_path)}\")\n",
					"            mssparkutils.fs.mkdirs(os.path.dirname(powerbi_delta_path))\n",
					"        \n",
					"        # Write the aggregated data to the Power BI delta location\n",
					"        logger.info(f\"Writing aggregated data to {powerbi_delta_path}\")\n",
					"        aggregated_df.write \\\n",
					"            .mode(\"overwrite\") \\\n",
					"            .format(\"delta\") \\\n",
					"            .save(powerbi_delta_path)\n",
					"        \n",
					"        logger.info(f\"Successfully wrote aggregated data to {powerbi_delta_path}\")\n",
					"        return True, powerbi_delta_path\n",
					"    \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error creating aggregated view for Power BI: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return False, None\n",
					"\n",
					"# Modify the main function to include the new aggregation step\n",
					"def export_voters_to_datalake_with_cdc():\n",
					"    \"\"\"\n",
					"    Exports voters_deduplicated data to a Delta table in the data lake blob storage.\n",
					"    Implements CDC logic to merge new data with the existing Delta table.\n",
					"    Also creates an aggregated view for Power BI.\n",
					"    \"\"\"\n",
					"    try:\n",
					"        # Original code remains the same...\n",
					"        storage_account = \"baubaisadfsastg\"\n",
					"        source_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_transformed\"\n",
					"        delta_table_path = f\"abfss://dl-juror-eric-voters-temp@{storage_account}.dfs.core.windows.net/voters_deduplicated_delta\"\n",
					"        \n",
					"        # Step 1: Read source data\n",
					"        logger.info(f\"Reading voter data from {source_path}\")\n",
					"        source_df = spark.read.format(\"delta\").load(source_path)\n",
					"        \n",
					"        # Get a list of numeric columns\n",
					"        numeric_cols = [field.name for field in source_df.schema.fields \n",
					"                       if str(field.dataType).startswith((\"IntegerType\", \"LongType\", \"DoubleType\", \"FloatType\", \"DecimalType\"))]\n",
					"        \n",
					"        # Clean NaN values in source data - replace with nulls\n",
					"        logger.info(\"Cleaning NaN values from source data\")\n",
					"        for col_name in numeric_cols:\n",
					"            source_df= source_df.withColumn(\n",
					"                col_name,\n",
					"                when(isnan(col(col_name)) | (col(col_name).isNull()), lit(None).cast(source_df.schema[col_name].dataType))\n",
					"                .otherwise(col(col_name))\n",
					"            )\n",
					"        \n",
					"        record_count = source_df.count()\n",
					"        logger.info(f\"Loaded {record_count} voter records\")\n",
					"        \n",
					"        # Check for duplicates and null values in primary key (hash_id)\n",
					"        logger.info(\"Validating primary key constraints on hash_id\")\n",
					"        distinct_count = source_df.select(\"hash_id\").distinct().count()\n",
					"        if distinct_count != source_df.count():\n",
					"            logger.warning(f\"Found {source_df.count() - distinct_count} duplicate hash_ids in source data\")\n",
					"            # Remove duplicates, keeping first occurrence\n",
					"            window_spec = Window.partitionBy(\"hash_id\").orderBy(desc(\"creation_date\"))\n",
					"            source_df = source_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                                .filter(col(\"row_num\") == 1) \\\n",
					"                                .drop(\"row_num\")\n",
					"            logger.info(f\"After removing duplicates: {source_df.count()} records\")\n",
					"        \n",
					"        # Check for nulls in hash_id\n",
					"        null_count = source_df.filter(col(\"hash_id\").isNull()).count()\n",
					"        if null_count > 0:\n",
					"            logger.warning(f\"Found {null_count} null hash_ids in source data - removing these records\")\n",
					"            source_df = source_df.filter(col(\"hash_id\").isNotNull())\n",
					"            logger.info(f\"After removing null hash_ids: {source_df.count()} records\")\n",
					"        \n",
					"        # Check if Delta table exists\n",
					"        if mssparkutils.fs.exists(delta_table_path):\n",
					"            logger.info(f\"Delta table exists at {delta_table_path}. Merging new data.\")\n",
					"            \n",
					"            # Read existing Delta table\n",
					"            existing_df = spark.read.format(\"delta\").load(delta_table_path)\n",
					"            \n",
					"            # Clean NaN values in existing data\n",
					"            logger.info(\"Cleaning NaN values from existing Delta table\")\n",
					"            existing_numeric_cols = [field.name for field in existing_df.schema.fields \n",
					"                                   if str(field.dataType).startswith((\"IntegerType\", \"LongType\", \"DoubleType\", \"FloatType\", \"DecimalType\"))]\n",
					"            \n",
					"            for col_name in existing_numeric_cols:\n",
					"                existing_df = existing_df.withColumn(\n",
					"                    col_name,\n",
					"                    when(isnan(col(col_name)) | (col(col_name).isNull()), lit(None).cast(existing_df.schema[col_name].dataType))\n",
					"                    .otherwise(col(col_name))\n",
					"                )\n",
					"            \n",
					"            # Add audit columns to source data before the join if they don't exist\n",
					"            logger.info(\"Adding audit columns to source data if needed\")\n",
					"            for audit_col in [\"AdtclmnFirstCreatedDatetime\", \"AdtclmnInsertedByProcessName\", \n",
					"                             \"AdtclmnModifiedDatetime\", \"AdtclmnModifiedByProcessName\", \n",
					"                             \"AdtclmnRecordActiveInd\"]:\n",
					"                if audit_col not in source_df.columns:\n",
					"                    if audit_col == \"AdtclmnFirstCreatedDatetime\":\n",
					"                        source_df = source_df.withColumn(audit_col, current_timestamp())\n",
					"                    elif audit_col == \"AdtclmnInsertedByProcessName\":\n",
					"                        source_df = source_df.withColumn(audit_col, lit(\"CDC_Update\"))\n",
					"                    elif audit_col in [\"AdtclmnModifiedDatetime\", \"AdtclmnModifiedByProcessName\"]:\n",
					"                        source_df = source_df.withColumn(audit_col, lit(None).cast(\"timestamp\" if \"Datetime\" in audit_col else \"string\"))\n",
					"                    elif audit_col == \"AdtclmnRecordActiveInd\":\n",
					"                        source_df = source_df.withColumn(audit_col, lit(1).cast(DecimalType(1, 0)))\n",
					"            \n",
					"            # Log details before join\n",
					"            logger.info(f\"Existing data record count: {existing_df.count()}\")\n",
					"            logger.info(f\"Source data record count: {source_df.count()}\")\n",
					"            \n",
					"            # Perform full outer join for CDC\n",
					"            logger.info(\"Performing full outer join for CDC\")\n",
					"            df_full_outer = existing_df.alias(\"main\").join(\n",
					"                source_df.alias(\"inc\"), \n",
					"                \"hash_id\",  # Join on primary key\n",
					"                \"outer\"\n",
					"            )\n",
					"            \n",
					"            # Log details after join\n",
					"            logger.info(f\"After full outer join: {df_full_outer.count()} records\")\n",
					"            \n",
					"            # Define audit columns with correct CDC logic\n",
					"            audit_columns = {\n",
					"                # For new records (only in incoming data), use the new timestamp\n",
					"                # For existing records, preserve the original creation timestamp\n",
					"                'AdtclmnFirstCreatedDatetime': when(col(\"main.hash_id\").isNull(), col(\"inc.AdtclmnFirstCreatedDatetime\"))\n",
					"                                                .otherwise(col(\"main.AdtclmnFirstCreatedDatetime\")),\n",
					"                \n",
					"                # Preserve the original process name that created the record\n",
					"                'AdtclmnInsertedByProcessName': when(col(\"main.hash_id\").isNull(), col(\"inc.AdtclmnInsertedByProcessName\"))\n",
					"                                                 .otherwise(col(\"main.AdtclmnInsertedByProcessName\")),\n",
					"                \n",
					"                # Update modification timestamp for updates and deactivations\n",
					"                'AdtclmnModifiedDatetime': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNotNull(), current_timestamp())\n",
					"                                            .when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), current_timestamp())\n",
					"                                            .otherwise(col(\"main.AdtclmnModifiedDatetime\")),\n",
					"                \n",
					"                # Update modification process name similarly\n",
					"                'AdtclmnModifiedByProcessName': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNotNull(), lit(\"CDC_Update\"))\n",
					"                                                .when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), lit(\"CDC_Deactivate\"))\n",
					"                                                .otherwise(col(\"main.AdtclmnModifiedByProcessName\")),\n",
					"                \n",
					"                # Mark records as inactive if they exist only in main (they've been removed in source)\n",
					"                'AdtclmnRecordActiveInd': when(col(\"main.hash_id\").isNotNull() & col(\"inc.hash_id\").isNull(), lit(0).cast(DecimalType(1, 0)))\n",
					"                                         .otherwise(lit(1).cast(DecimalType(1, 0)))\n",
					"            }\n",
					"            \n",
					"            # Get all columns from existing dataframe\n",
					"            all_columns = existing_df.columns\n",
					"            \n",
					"            # Combine columns with improved null/NaN handling\n",
					"            logger.info(\"Building merged dataframe with improved NaN handling\")\n",
					"            combined_columns = []\n",
					"            for c in all_columns:\n",
					"                if c in audit_columns:\n",
					"                    combined_columns.append(audit_columns[c].alias(c))\n",
					"                else:\n",
					"                    # Check column data type\n",
					"                    col_type = next((f.dataType for f in existing_df.schema.fields if f.name == c), StringType())\n",
					"                    \n",
					"                    # Handle numeric columns specially to avoid NaN\n",
					"                    if str(col_type).startswith((\"IntegerType\", \"LongType\", \"DoubleType\", \"FloatType\", \"DecimalType\")):\n",
					"                        combined_columns.append(\n",
					"                            when(col(f\"inc.{c}\").isNotNull() & ~isnan(col(f\"inc.{c}\")), col(f\"inc.{c}\"))\n",
					"                            .when(col(f\"main.{c}\").isNotNull() & ~isnan(col(f\"main.{c}\")), col(f\"main.{c}\"))\n",
					"                            .otherwise(lit(None).cast(col_type))\n",
					"                            .alias(c)\n",
					"                        )\n",
					"                    else:\n",
					"                        # For non-numeric columns, use regular coalesce with null awareness\n",
					"                        combined_columns.append(\n",
					"                            coalesce(col(f\"inc.{c}\"), col(f\"main.{c}\")).alias(c)\n",
					"                        )\n",
					"            \n",
					"            # Create the final DataFrame\n",
					"            logger.info(\"Creating final merged dataframe\")\n",
					"            df_full_derived = df_full_outer.select(combined_columns)\n",
					"            \n",
					"            # Final NaN cleanup on the merged data\n",
					"            logger.info(\"Performing final NaN cleanup\")\n",
					"            numeric_cols_final = [field.name for field in df_full_derived.schema.fields \n",
					"                               if str(field.dataType).startswith((\"IntegerType\", \"LongType\", \"DoubleType\", \"FloatType\", \"DecimalType\"))]\n",
					"            \n",
					"            for col_name in numeric_cols_final:\n",
					"                df_full_derived = df_full_derived.withColumn(\n",
					"                    col_name,\n",
					"                    when(isnan(col(col_name)), lit(None).cast(df_full_derived.schema[col_name].dataType))\n",
					"                    .otherwise(col(col_name))\n",
					"                )\n",
					"            \n",
					"            # Log record count\n",
					"            logger.info(f\"Final merged dataframe has {df_full_derived.count()} records\")\n",
					"            \n",
					"            # Check for any remaining NaN values\n",
					"            for col_name in numeric_cols_final:\n",
					"                nan_count = df_full_derived.filter(isnan(col(col_name))).count()\n",
					"                if nan_count > 0:\n",
					"                    logger.warning(f\"Column {col_name} still has {nan_count} NaN values after cleanup\")\n",
					"            \n",
					"            # Write the merged data back to the Delta table\n",
					"            logger.info(f\"Writing merged data to Delta table at {delta_table_path}\")\n",
					"            df_full_derived.write \\\n",
					"                .mode(\"overwrite\") \\\n",
					"                .option(\"overwriteSchema\", \"true\") \\\n",
					"                .format(\"delta\") \\\n",
					"                .save(delta_table_path)\n",
					"            \n",
					"            logger.info(f\"Successfully merged and wrote data to Delta table at {delta_table_path}\")\n",
					"        \n",
					"        else:\n",
					"            logger.info(f\"Delta table does not exist. Creating a new Delta table at {delta_table_path}.\")\n",
					"            \n",
					"            # Add audit columns to source data\n",
					"            source_df = source_df.withColumn(\"AdtclmnFirstCreatedDatetime\", current_timestamp()) \\\n",
					"                               .withColumn(\"AdtclmnInsertedByProcessName\", lit(\"InitialLoad\")) \\\n",
					"                               .withColumn(\"AdtclmnModifiedDatetime\", lit(None).cast(\"timestamp\")) \\\n",
					"                               .withColumn(\"AdtclmnModifiedByProcessName\", lit(None).cast(\"string\")) \\\n",
					"                               .withColumn(\"AdtclmnRecordActiveInd\", lit(1).cast(DecimalType(1, 0)))\n",
					"            \n",
					"            # Final NaN cleanup before writing\n",
					"            logger.info(\"Performing final NaN cleanup before initial write\")\n",
					"            for col_name in numeric_cols:\n",
					"                source_df = source_df.withColumn(\n",
					"                    col_name,\n",
					"                    when(isnan(col(col_name)), lit(None).cast(source_df.schema[col_name].dataType))\n",
					"                    .otherwise(col(col_name))\n",
					"                )\n",
					"    \n",
					"            # Write the source data as a new Delta table\n",
					"            logger.info(f\"Writing {source_df.count()} records to new Delta table\")\n",
					"            source_df.write \\\n",
					"                .mode(\"overwrite\") \\\n",
					"                .option(\"overwriteSchema\", \"true\") \\\n",
					"                .format(\"delta\") \\\n",
					"                .save(delta_table_path)\n",
					"            \n",
					"            logger.info(f\"Successfully created new Delta table at {delta_table_path}\")\n",
					"        \n",
					"        # NEW CODE: Create aggregated view for Power BI\n",
					"        logger.info(\"Creating aggregated view for Power BI\")\n",
					"        agg_success, agg_path = export_aggregated_data_for_powerbi(delta_table_path)\n",
					"        \n",
					"        if agg_success:\n",
					"            logger.info(f\"Successfully created aggregated view at {agg_path}\")\n",
					"        else:\n",
					"            logger.warning(\"Failed to create aggregated view for Power BI\")\n",
					"        \n",
					"        # Return success with both paths\n",
					"        return True, {\n",
					"            \"delta_table_path\": delta_table_path,\n",
					"            \"aggregated_path\": agg_path if agg_success else None\n",
					"        }\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error in data lake export process: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return False, None\n",
					"\n",
					"# Update the main execution code to handle the new return format\n",
					"if __name__ == \"__main__\":\n",
					"    success, paths = export_voters_to_datalake_with_cdc()\n",
					"    if success:\n",
					"        print(f\"Data lake export with CDC completed successfully\")\n",
					"        print(f\"Delta files created/merged at: {paths['delta_table_path']}\")\n",
					"        \n",
					"        if paths.get('aggregated_path'):\n",
					"            print(f\"Aggregated Power BI data created at: {paths['aggregated_path']}\")\n",
					"            print(f\"This aggregated data contains counts by creation_date and rec_num\")\n",
					"        else:\n",
					"            print(\"Warning: Aggregated view for Power BI was not created\")\n",
					"            \n",
					"        print(f\"Use these paths for your downstream processes once permissions are granted\")\n",
					"    else:\n",
					"        print(\"Data lake export with CDC failed\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"delta_df = spark.read.format(\"delta\").load( f\"abfss://dl-juror-eric-voters-temp@baubaisadfsastg.dfs.core.windows.net/voters_deduplicated_delta\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"delta_df.count()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					},
					"collapsed": false
				},
				"source": [
					"display(delta_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import count\n",
					"\n",
					"final_count = delta_df.groupBy('address6','rec_num') \\\n",
					"    .agg(count('address6')) \\\n",
					"    .orderBy('rec_num')\n",
					"\n",
					"display(final_count)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					},
					"collapsed": false
				},
				"source": [
					"foo_df = delta_df.filter(delta_df.rec_num==249)\n",
					"foo_df.count()"
				],
				"execution_count": null
			}
		]
	}
}