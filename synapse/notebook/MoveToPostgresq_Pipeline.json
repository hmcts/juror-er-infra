{
	"name": "MoveToPostgresq_Pipeline",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkprod",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "fc395b24-a8bc-4553-9640-169e562207fe"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5ca62022-6aa2-4cee-aaa7-e7536c8d566c/resourceGroups/baubais-data-factory-rg-prod/providers/Microsoft.Synapse/workspaces/baubais-synapse-prod/bigDataPools/sparkprod",
				"name": "sparkprod",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkprod",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"from notebookutils import mssparkutils\n",
					"import logging\n",
					"from datetime import datetime\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Configure logging\n",
					"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"logger = logging.getLogger()\n",
					"\n",
					"# Initialize Spark session with Delta Lake support\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"Voters Data Lake Export - PostgreSQL Schema\") \\\n",
					"    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
					"    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
					"    .getOrCreate()\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def clean_data_before_transformation(df):\n",
					"    \"\"\"\n",
					"    Comprehensive data cleaning before PostgreSQL transformation\n",
					"    \"\"\"\n",
					"    logger.info(\"Starting comprehensive data cleaning\")\n",
					"    \n",
					"    initial_count = df.count()\n",
					"    logger.info(f\"Initial record count: {initial_count}\")\n",
					"    \n",
					"    # Step 1: Remove records with NULL/empty lname or fname\n",
					"    logger.info(\"Step 1: Removing records with NULL/empty lname or fname\")\n",
					"    df = df.filter(\n",
					"        (col(\"lname\").isNotNull()) & \n",
					"        (trim(col(\"lname\")) != \"\") & \n",
					"        (upper(trim(col(\"lname\"))) != \"NULL\") &\n",
					"        (upper(trim(col(\"lname\"))) != \"NA\") &\n",
					"        (upper(trim(col(\"lname\"))) != \"NAN\") &\n",
					"        (col(\"fname\").isNotNull()) & \n",
					"        (trim(col(\"fname\")) != \"\") & \n",
					"        (upper(trim(col(\"fname\"))) != \"NULL\") &\n",
					"        (upper(trim(col(\"fname\"))) != \"NA\") &\n",
					"        (upper(trim(col(\"fname\"))) != \"NAN\")\n",
					"    )\n",
					"    \n",
					"    after_name_filter = df.count()\n",
					"    logger.info(f\"Records after name filtering: {after_name_filter} (removed: {initial_count - after_name_filter})\")\n",
					"    \n",
					"    # Step 2: Ensure address is not null and not empty\n",
					"    logger.info(\"Step 2: Ensuring address is not null/empty\")\n",
					"    df = df.filter(\n",
					"        (col(\"address\").isNotNull()) & \n",
					"        (trim(col(\"address\")) != \"\") &\n",
					"        (upper(trim(col(\"address\"))) != \"NULL\") &\n",
					"        (upper(trim(col(\"address\"))) != \"NA\") &\n",
					"        (upper(trim(col(\"address\"))) != \"NAN\")\n",
					"    )\n",
					"    \n",
					"    after_address_filter = df.count()\n",
					"    logger.info(f\"Records after address filtering: {after_address_filter} (removed: {after_name_filter - after_address_filter})\")\n",
					"    \n",
					"    # Step 3: Clean address4 - remove if it matches zip without spaces\n",
					"    logger.info(\"Step 3: Cleaning address4 - removing zip code duplicates\")\n",
					"    df = df.withColumn(\"address4\", \n",
					"        when(col(\"address4\") == regexp_replace(col(\"zip\"), \" \", \"\"), lit(None))\n",
					"        .otherwise(col(\"address4\"))\n",
					"    )\n",
					"    \n",
					"    # Step 4: Remove \"NAN\" values from address4\n",
					"    logger.info(\"Step 4: Removing 'NAN' values from address4\")\n",
					"    df = df.withColumn(\"address4\", \n",
					"        when(upper(trim(col(\"address4\"))) == \"NAN\", lit(None))\n",
					"        .otherwise(col(\"address4\"))\n",
					"    )\n",
					"    \n",
					"    # Step 5: Address field reorganization - shift addresses that start with numbers\n",
					"    logger.info(\"Step 5: Reorganizing address fields\")\n",
					"    \n",
					"    # If address2 or address3 starts with a number and address is not a number, shift to address\n",
					"    df = df.withColumn(\"temp_address_needs_shift\",\n",
					"        when(\n",
					"            (col(\"address2\").isNotNull()) & \n",
					"            (regexp_extract(trim(col(\"address2\")), \"^([0-9])\", 1) != \"\") &\n",
					"            (regexp_extract(trim(col(\"address\")), \"^([0-9])\", 1) == \"\"),\n",
					"            lit(\"address2\")\n",
					"        ).when(\n",
					"            (col(\"address3\").isNotNull()) & \n",
					"            (regexp_extract(trim(col(\"address3\")), \"^([0-9])\", 1) != \"\") &\n",
					"            (regexp_extract(trim(col(\"address\")), \"^([0-9])\", 1) == \"\"),\n",
					"            lit(\"address3\")\n",
					"        ).otherwise(lit(None))\n",
					"    )\n",
					"    \n",
					"    # Perform the address shifting\n",
					"    df = df.withColumn(\"address\",\n",
					"        when(col(\"temp_address_needs_shift\") == \"address2\", col(\"address2\"))\n",
					"        .when(col(\"temp_address_needs_shift\") == \"address3\", col(\"address3\"))\n",
					"        .otherwise(col(\"address\"))\n",
					"    ).withColumn(\"address2\",\n",
					"        when(col(\"temp_address_needs_shift\") == \"address2\", col(\"address\"))\n",
					"        .otherwise(col(\"address2\"))\n",
					"    ).withColumn(\"address3\",\n",
					"        when(col(\"temp_address_needs_shift\") == \"address3\", col(\"address\"))\n",
					"        .otherwise(col(\"address3\"))\n",
					"    ).drop(\"temp_address_needs_shift\")\n",
					"    \n",
					"    # Step 6: Address4 cannot be empty - comprehensive address shifting\n",
					"    logger.info(\"Step 6: Ensuring address4 is not empty through comprehensive shifting\")\n",
					"    \n",
					"    # Shift address3 to address4 if address4 is null and address3 is not null\n",
					"    df = df.withColumn(\"address4\",\n",
					"        when(\n",
					"            (col(\"address4\").isNull()) & \n",
					"            (col(\"address3\").isNotNull()) &\n",
					"            (trim(col(\"address3\")) != \"\"),\n",
					"            col(\"address3\")\n",
					"        ).otherwise(col(\"address4\"))\n",
					"    ).withColumn(\"address3\",\n",
					"        when(\n",
					"            (col(\"address4\") == col(\"address3\")) & \n",
					"            (col(\"address4\").isNotNull()),\n",
					"            lit(None)\n",
					"        ).otherwise(col(\"address3\"))\n",
					"    )\n",
					"    \n",
					"    # Shift address2 to address4 if address4 is still null and address2 doesn't start with number\n",
					"    df = df.withColumn(\"address4\",\n",
					"        when(\n",
					"            (col(\"address4\").isNull()) & \n",
					"            (col(\"address2\").isNotNull()) &\n",
					"            (trim(col(\"address2\")) != \"\") &\n",
					"            (regexp_extract(trim(col(\"address2\")), \"^([0-9])\", 1) == \"\"),  # doesn't start with number\n",
					"            col(\"address2\")\n",
					"        ).otherwise(col(\"address4\"))\n",
					"    ).withColumn(\"address2\",\n",
					"        when(\n",
					"            (col(\"address4\") == col(\"address2\")) & \n",
					"            (col(\"address4\").isNotNull()),\n",
					"            lit(None)\n",
					"        ).otherwise(col(\"address2\"))\n",
					"    )\n",
					"    \n",
					"    # If address4 is still null, try address5 or address6\n",
					"    df = df.withColumn(\"address4\",\n",
					"        when(\n",
					"            (col(\"address4\").isNull()) & \n",
					"            (col(\"address5\").isNotNull()) &\n",
					"            (trim(col(\"address5\")) != \"\"),\n",
					"            col(\"address5\")\n",
					"        ).when(\n",
					"            (col(\"address4\").isNull()) & \n",
					"            (col(\"address6\").isNotNull()) &\n",
					"            (trim(col(\"address6\")) != \"\"),\n",
					"            col(\"address6\")\n",
					"        ).otherwise(col(\"address4\"))\n",
					"    )\n",
					"    \n",
					"    # Step 7: Final cleanup - remove records that still have null address4\n",
					"    logger.info(\"Step 7: Final cleanup - removing records with null address4\")\n",
					"    before_final_filter = df.count()\n",
					"    df = df.filter(col(\"address4\").isNotNull() & (trim(col(\"address4\")) != \"\"))\n",
					"    after_final_filter = df.count()\n",
					"    logger.info(f\"Records after final address4 filtering: {after_final_filter} (removed: {before_final_filter - after_final_filter})\")\n",
					"    \n",
					"    # Step 8: Final cleanup - remove other NAN/NULL values\n",
					"    logger.info(\"Step 8: Cleaning other NAN/NULL values\")\n",
					"    \n",
					"    # Clean address fields\n",
					"    for addr_col in [\"address\", \"address2\", \"address3\", \"address4\", \"address5\", \"address6\"]:\n",
					"        df = df.withColumn(addr_col,\n",
					"            when(upper(trim(col(addr_col))).isin([\"NAN\", \"NULL\", \"NA\", \"\"]), lit(None))\n",
					"            .otherwise(col(addr_col))\n",
					"        )\n",
					"    \n",
					"    # Clean other string fields\n",
					"    for str_col in [\"title\", \"new_marker\", \"flags\", \"perm_disqual\", \"source_id\"]:\n",
					"        if str_col in df.columns:\n",
					"            df = df.withColumn(str_col,\n",
					"                when(upper(trim(col(str_col))).isin([\"NAN\", \"NULL\", \"NA\", \"\"]), lit(None))\n",
					"                .otherwise(col(str_col))\n",
					"            )\n",
					"    \n",
					"    final_count = df.count()\n",
					"    logger.info(f\"Final record count after comprehensive cleaning: {final_count}\")\n",
					"    logger.info(f\"Total records removed: {initial_count - final_count}\")\n",
					"    \n",
					"    return df\n",
					"\n",
					"def transform_to_postgresql_schema(df):\n",
					"    \"\"\"\n",
					"    Transform DataFrame to match exact PostgreSQL juror_mod.voters schema\n",
					"    Note: part_no will be NULL here as it's generated by PostgreSQL sequence\n",
					"    \"\"\"\n",
					"    logger.info(\"Starting transformation to PostgreSQL schema\")\n",
					"    \n",
					"    # Apply comprehensive data cleaning first\n",
					"    df = clean_data_before_transformation(df)\n",
					"    \n",
					"    # Define the PostgreSQL staging schema structure (part_no will be NULL)\n",
					"    postgresql_schema = StructType([\n",
					"        StructField(\"part_no\", StringType(), True),            # NULL - generated in PostgreSQL\n",
					"        StructField(\"register_lett\", StringType(), True),      # VARCHAR(5)\n",
					"        StructField(\"poll_number\", StringType(), True),        # VARCHAR(5)\n",
					"        StructField(\"new_marker\", StringType(), True),         # VARCHAR(1)\n",
					"        StructField(\"title\", StringType(), True),              # VARCHAR(10)\n",
					"        StructField(\"lname\", StringType(), False),             # NOT NULL, VARCHAR(20)\n",
					"        StructField(\"fname\", StringType(), False),             # NOT NULL, VARCHAR(20)\n",
					"        StructField(\"dob\", DateType(), True),                  # DATE\n",
					"        StructField(\"flags\", StringType(), True),              # VARCHAR(2)\n",
					"        StructField(\"address\", StringType(), False),           # NOT NULL, VARCHAR(35)\n",
					"        StructField(\"address2\", StringType(), True),           # VARCHAR(35)\n",
					"        StructField(\"address3\", StringType(), True),           # VARCHAR(35)\n",
					"        StructField(\"address4\", StringType(), True),           # VARCHAR(35)\n",
					"        StructField(\"address5\", StringType(), True),           # VARCHAR(35)\n",
					"        StructField(\"address6\", StringType(), True),           # VARCHAR(35)\n",
					"        StructField(\"zip\", StringType(), True),                # VARCHAR(10)\n",
					"        StructField(\"date_selected1\", DateType(), True),       # DATE\n",
					"        StructField(\"date_selected2\", DateType(), True),       # DATE\n",
					"        StructField(\"date_selected3\", DateType(), True),       # DATE\n",
					"        StructField(\"rec_num\", IntegerType(), True),           # INTEGER\n",
					"        StructField(\"perm_disqual\", StringType(), True),       # VARCHAR(1)\n",
					"        StructField(\"source_id\", StringType(), True),          # VARCHAR(1)\n",
					"        StructField(\"postcode_start\", StringType(), True),     # VARCHAR(10) - generated in PostgreSQL\n",
					"        StructField(\"hash_id\", StringType(), True)             # Keep hash_id for deduplication\n",
					"    ])\n",
					"    \n",
					"    # Apply field length constraints and transformations\n",
					"    logger.info(\"Applying field length constraints\")\n",
					"    \n",
					"    df_constrained = df \\\n",
					"        .withColumn(\"part_no\", lit(None).cast(StringType())) \\\n",
					"        .withColumn(\"register_lett\", \n",
					"                   when(col(\"register_lett\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"register_lett\")), 1, 5))) \\\n",
					"        .withColumn(\"poll_number\", \n",
					"                   when(col(\"poll_number\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"poll_number\")), 1, 5))) \\\n",
					"        .withColumn(\"new_marker\", \n",
					"                   when(col(\"new_marker\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"new_marker\")), 1, 1))) \\\n",
					"        .withColumn(\"title\", \n",
					"                   when(col(\"title\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"title\")), 1, 10))) \\\n",
					"        .withColumn(\"lname\", \n",
					"                   substring(trim(upper(col(\"lname\"))), 1, 20)) \\\n",
					"        .withColumn(\"fname\", \n",
					"                   substring(trim(upper(col(\"fname\"))), 1, 20)) \\\n",
					"        .withColumn(\"flags\", \n",
					"                   when(col(\"flags\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"flags\")), 1, 2))) \\\n",
					"        .withColumn(\"address\", \n",
					"                   substring(trim(upper(col(\"address\"))), 1, 35)) \\\n",
					"        .withColumn(\"address2\", \n",
					"                   when(col(\"address2\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(upper(col(\"address2\"))), 1, 35))) \\\n",
					"        .withColumn(\"address3\", \n",
					"                   when(col(\"address3\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(upper(col(\"address3\"))), 1, 35))) \\\n",
					"        .withColumn(\"address4\", \n",
					"                   when(col(\"address4\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(upper(col(\"address4\"))), 1, 35))) \\\n",
					"        .withColumn(\"address5\", \n",
					"                   when(col(\"address5\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(upper(col(\"address5\"))), 1, 35))) \\\n",
					"        .withColumn(\"address6\", \n",
					"                   when(col(\"address6\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(upper(col(\"address6\"))), 1, 35))) \\\n",
					"        .withColumn(\"zip\", \n",
					"                   when(col(\"zip\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(upper(col(\"zip\"))), 1, 10))) \\\n",
					"        .withColumn(\"perm_disqual\", \n",
					"                   when(col(\"perm_disqual\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"perm_disqual\")), 1, 1))) \\\n",
					"        .withColumn(\"source_id\", \n",
					"                   when(col(\"source_id\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"source_id\")), 1, 1)))\n",
					"    \n",
					"    # Handle date fields - convert string dates to proper date type\n",
					"    logger.info(\"Converting date fields\")\n",
					"    \n",
					"    df_with_dates = df_constrained \\\n",
					"        .withColumn(\"dob\", \n",
					"                   when(col(\"dob\").isNull(), lit(None).cast(DateType()))\n",
					"                   .when(col(\"dob\") == \"\", lit(None).cast(DateType()))\n",
					"                   .otherwise(to_date(col(\"dob\"), \"yyyy-MM-dd\"))) \\\n",
					"        .withColumn(\"date_selected1\", \n",
					"                   when(col(\"date_selected1\").isNull(), lit(None).cast(DateType()))\n",
					"                   .when(col(\"date_selected1\") == \"\", lit(None).cast(DateType()))\n",
					"                   .otherwise(to_date(col(\"date_selected1\"), \"yyyy-MM-dd\"))) \\\n",
					"        .withColumn(\"date_selected2\", \n",
					"                   when(col(\"date_selected2\").isNull(), lit(None).cast(DateType()))\n",
					"                   .when(col(\"date_selected2\") == \"\", lit(None).cast(DateType()))\n",
					"                   .otherwise(to_date(col(\"date_selected2\"), \"yyyy-MM-dd\"))) \\\n",
					"        .withColumn(\"date_selected3\", \n",
					"                   when(col(\"date_selected3\").isNull(), lit(None).cast(DateType()))\n",
					"                   .when(col(\"date_selected3\") == \"\", lit(None).cast(DateType()))\n",
					"                   .otherwise(to_date(col(\"date_selected3\"), \"yyyy-MM-dd\")))\n",
					"    \n",
					"    # Set postcode_start to NULL (generated by PostgreSQL)\n",
					"    logger.info(\"Setting postcode_start to NULL - will be generated by PostgreSQL\")\n",
					"    \n",
					"    df_with_postcode = df_with_dates \\\n",
					"        .withColumn(\"postcode_start\", lit(None).cast(StringType()))\n",
					"    \n",
					"    # Ensure rec_num is integer type\n",
					"    df_with_rec_num = df_with_postcode \\\n",
					"        .withColumn(\"rec_num\", \n",
					"                   when(col(\"rec_num\").isNull(), lit(None).cast(IntegerType()))\n",
					"                   .otherwise(col(\"rec_num\").cast(IntegerType())))\n",
					"    \n",
					"    # Select only the required columns in the correct order\n",
					"    final_columns = [\n",
					"        \"part_no\", \"register_lett\", \"poll_number\", \"new_marker\", \"title\",\n",
					"        \"lname\", \"fname\", \"dob\", \"flags\", \"address\", \"address2\", \"address3\",\n",
					"        \"address4\", \"address5\", \"address6\", \"zip\", \"date_selected1\",\n",
					"        \"date_selected2\", \"date_selected3\", \"rec_num\", \"perm_disqual\",\n",
					"        \"source_id\", \"postcode_start\", \"hash_id\"\n",
					"    ]\n",
					"    \n",
					"    df_final = df_with_rec_num.select(*final_columns)\n",
					"    \n",
					"    # Final validation - check for any remaining NULL violations\n",
					"    logger.info(\"Performing final validation\")\n",
					"    \n",
					"    null_lname = df_final.filter(col(\"lname\").isNull()).count()\n",
					"    null_fname = df_final.filter(col(\"fname\").isNull()).count()\n",
					"    null_address = df_final.filter(col(\"address\").isNull()).count()\n",
					"    null_address4 = df_final.filter(col(\"address4\").isNull()).count()\n",
					"    \n",
					"    if null_lname > 0 or null_fname > 0 or null_address > 0 or null_address4 > 0:\n",
					"        logger.warning(f\"NULL constraint violations found: lname={null_lname}, fname={null_fname}, address={null_address}, address4={null_address4}\")\n",
					"        \n",
					"        # Remove records with NULL violations\n",
					"        df_final = df_final.filter(\n",
					"            col(\"lname\").isNotNull() &\n",
					"            col(\"fname\").isNotNull() &\n",
					"            col(\"address\").isNotNull() &\n",
					"            col(\"address4\").isNotNull()\n",
					"        )\n",
					"    \n",
					"    # Check hash_id uniqueness for deduplication verification\n",
					"    unique_hash_ids = df_final.select(\"hash_id\").distinct().count()\n",
					"    total_records = df_final.count()\n",
					"    \n",
					"    if unique_hash_ids != total_records:\n",
					"        logger.warning(f\"Duplicate hash_ids found: {total_records - unique_hash_ids} duplicates\")\n",
					"        # Remove duplicates keeping the first occurrence\n",
					"        from pyspark.sql.window import Window\n",
					"        window_spec = Window.partitionBy(\"hash_id\").orderBy(col(\"hash_id\"))\n",
					"        df_final = df_final.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                          .filter(col(\"row_num\") == 1) \\\n",
					"                          .drop(\"row_num\")\n",
					"    \n",
					"    logger.info(f\"Final record count after PostgreSQL schema transformation: {df_final.count()}\")\n",
					"    \n",
					"    return df_final\n",
					"\n",
					"def export_voters_to_postgresql_schema():\n",
					"    \"\"\"\n",
					"    Main function to export voters data with PostgreSQL schema compliance\n",
					"    \"\"\"\n",
					"    try:\n",
					"        # Initialize storage paths\n",
					"        storage_account = \"baubaisadfsaprod\"\n",
					"        source_path = f\"abfss://dl-juror-eric-voters-temp@{storage_account}.dfs.core.windows.net/voters_deduplicated_delta\"\n",
					"        output_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_postgresql_schema\"\n",
					"        \n",
					"        # Read the existing Delta table\n",
					"        logger.info(f\"Reading voter data from {source_path}\")\n",
					"        source_df = spark.read.format(\"delta\").load(source_path)\n",
					"        \n",
					"        initial_count = source_df"
				],
				"execution_count": null
			}
		]
	}
}