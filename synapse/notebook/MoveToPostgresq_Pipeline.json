{
	"name": "MoveToPostgresq_Pipeline",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkprod",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "36c4c8ea-08eb-4961-a51d-b9bb26ccb5eb"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5ca62022-6aa2-4cee-aaa7-e7536c8d566c/resourceGroups/baubais-data-factory-rg-prod/providers/Microsoft.Synapse/workspaces/baubais-synapse-prod/bigDataPools/sparkprod",
				"name": "sparkprod",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkprod",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"from notebookutils import mssparkutils\n",
					"import logging\n",
					"from datetime import datetime\n",
					"from pyspark.sql.types import *  # This should include LongType\n",
					"from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType, LongType"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"# Configure logging\n",
					"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"logger = logging.getLogger()\n",
					"\n",
					"# Initialize Spark session with Delta Lake support\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"Voters Data Lake Export - PostgreSQL Schema\") \\\n",
					"    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
					"    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
					"    .getOrCreate()"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"source": [
					"def clean_data_before_transformation(df):\n",
					"    \"\"\"\n",
					"    Comprehensive data cleaning before PostgreSQL transformation\n",
					"    \"\"\"\n",
					"    logger.info(\"Starting comprehensive data cleaning\")\n",
					"    \n",
					"    initial_count = df.count()\n",
					"    logger.info(f\"Initial record count: {initial_count}\")\n",
					"    \n",
					"    # Step 1: Remove records with NULL/empty lname or fname\n",
					"    logger.info(\"Step 1: Filtering records with NULL/empty lname or fname\")\n",
					"    df = df.filter(\n",
					"        (col(\"lname\").isNotNull()) & \n",
					"        (trim(col(\"lname\")) != \"\") & \n",
					"        (upper(trim(col(\"lname\"))) != \"NULL\") &\n",
					"        (upper(trim(col(\"lname\"))) != \"NA\") &\n",
					"        (upper(trim(col(\"lname\"))) != \"NAN\") &\n",
					"        (col(\"fname\").isNotNull()) & \n",
					"        (trim(col(\"fname\")) != \"\") & \n",
					"        (upper(trim(col(\"fname\"))) != \"NULL\") &\n",
					"        (upper(trim(col(\"fname\"))) != \"NA\") &\n",
					"        (upper(trim(col(\"fname\"))) != \"NAN\")\n",
					"    )\n",
					"    \n",
					"    after_name_filter = df.count()\n",
					"    logger.info(f\"Records after name filtering: {after_name_filter} (removed: {initial_count - after_name_filter})\")\n",
					"    \n",
					"    # Step 2: Log address issues but don't filter\n",
					"    logger.info(\"Step 2: Checking address quality (no filtering)\")\n",
					"    address_issues = df.filter(\n",
					"        (col(\"address\").isNull()) | \n",
					"        (trim(col(\"address\")) == \"\") |\n",
					"        (upper(trim(col(\"address\"))) == \"NULL\") |\n",
					"        (upper(trim(col(\"address\"))) == \"NA\") |\n",
					"        (upper(trim(col(\"address\"))) == \"NAN\")\n",
					"    ).count()\n",
					"    \n",
					"    logger.info(f\"Found {address_issues} records with address issues (keeping all records)\")\n",
					"    \n",
					"    # Step 3: Clean zip code duplicates from address fields\n",
					"    logger.info(\"Step 3: Cleaning address2-5 - removing zip code duplicates\")\n",
					"    \n",
					"    zip_duplicates_count = df.filter(\n",
					"        (col(\"address2\") == regexp_replace(col(\"zip\"), \" \", \"\")) |\n",
					"        (col(\"address3\") == regexp_replace(col(\"zip\"), \" \", \"\")) |\n",
					"        (col(\"address4\") == regexp_replace(col(\"zip\"), \" \", \"\")) |\n",
					"        (col(\"address5\") == regexp_replace(col(\"zip\"), \" \", \"\"))\n",
					"    ).count()\n",
					"    \n",
					"    logger.info(f\"Found {zip_duplicates_count} records with zip code duplicates in address fields\")\n",
					"    \n",
					"    # Clean all address fields that match zip without spaces\n",
					"    df = df.withColumn(\"address2\",\n",
					"        when(col(\"address2\") == regexp_replace(col(\"zip\"), \" \", \"\"), lit(None))\n",
					"        .otherwise(col(\"address2\"))\n",
					"    ).withColumn(\"address3\",\n",
					"        when(col(\"address3\") == regexp_replace(col(\"zip\"), \" \", \"\"), lit(None))\n",
					"        .otherwise(col(\"address3\"))\n",
					"    ).withColumn(\"address4\",\n",
					"        when(col(\"address4\") == regexp_replace(col(\"zip\"), \" \", \"\"), lit(None))\n",
					"        .otherwise(col(\"address4\"))\n",
					"    ).withColumn(\"address5\",\n",
					"        when(col(\"address5\") == regexp_replace(col(\"zip\"), \" \", \"\"), lit(None))\n",
					"        .otherwise(col(\"address5\"))\n",
					"    )\n",
					"    \n",
					"    logger.info(f\"Cleaned zip code duplicates from {zip_duplicates_count} records\")\n",
					"    \n",
					"    # Step 4: Remove \"NAN\" values from address4\n",
					"    logger.info(\"Step 4: Removing 'NAN' values from address4\")\n",
					"    df = df.withColumn(\"address4\", \n",
					"        when(upper(trim(col(\"address4\"))) == \"NAN\", lit(None))\n",
					"        .otherwise(col(\"address4\"))\n",
					"    )\n",
					"    \n",
					"    # Step 5: Address field reorganization - shift addresses that start with numbers\n",
					"    logger.info(\"Step 5: Reorganizing address fields\")\n",
					"    \n",
					"    # Step 6: Address4 cannot be empty - comprehensive address shifting\n",
					"    logger.info(\"Step 6: Ensuring address4 is not empty through comprehensive shifting\")\n",
					"    \n",
					"    # Shift address3 to address4 if address4 is null and address3 is not null\n",
					"    df = df.withColumn(\"address4\",\n",
					"        when(\n",
					"            (col(\"address4\").isNull()) & \n",
					"            (col(\"address3\").isNotNull()) &\n",
					"            (trim(col(\"address3\")) != \"\"),\n",
					"            col(\"address3\")\n",
					"        ).otherwise(col(\"address4\"))\n",
					"    ).withColumn(\"address3\",\n",
					"        when(\n",
					"            (col(\"address4\") == col(\"address3\")) & \n",
					"            (col(\"address4\").isNotNull()),\n",
					"            lit(None)\n",
					"        ).otherwise(col(\"address3\"))\n",
					"    )\n",
					"    \n",
					"    # Shift address2 to address4 if address4 is still null and address2 doesn't start with number\n",
					"    df = df.withColumn(\"address4\",\n",
					"        when(\n",
					"            (col(\"address4\").isNull()) & \n",
					"            (col(\"address2\").isNotNull()) &\n",
					"            (trim(col(\"address2\")) != \"\") &\n",
					"            (regexp_extract(trim(col(\"address2\")), \"^([0-9])\", 1) == \"\"),  # doesn't start with number\n",
					"            col(\"address2\")\n",
					"        ).otherwise(col(\"address4\"))\n",
					"    ).withColumn(\"address2\",\n",
					"        when(\n",
					"            (col(\"address4\") == col(\"address2\")) & \n",
					"            (col(\"address4\").isNotNull()),\n",
					"            lit(None)\n",
					"        ).otherwise(col(\"address2\"))\n",
					"    )\n",
					"    \n",
					"    # If address4 is still null, try address5 or address6\n",
					"    df = df.withColumn(\"address4\",\n",
					"        when(\n",
					"            (col(\"address4\").isNull()) & \n",
					"            (col(\"address5\").isNotNull()) &\n",
					"            (trim(col(\"address5\")) != \"\"),\n",
					"            col(\"address5\")\n",
					"        ).when(\n",
					"            (col(\"address4\").isNull()) & \n",
					"            (col(\"address6\").isNotNull()) &\n",
					"            (trim(col(\"address6\")) != \"\"),\n",
					"            col(\"address6\")\n",
					"        ).otherwise(col(\"address4\"))\n",
					"    )\n",
					"    \n",
					"    # Step 7: Final cleanup - remove records that still have null address4\n",
					"    logger.info(\"Step 7: Final cleanup - removing records with null address4\")\n",
					"    before_final_filter = df.count()\n",
					"    df = df.filter(col(\"address4\").isNotNull() & (trim(col(\"address4\")) != \"\"))\n",
					"    after_final_filter = df.count()\n",
					"    logger.info(f\"Records after final address4 filtering: {after_final_filter} (removed: {before_final_filter - after_final_filter})\")\n",
					"    \n",
					"    # Step 8: Final cleanup - remove other NAN/NULL values\n",
					"    logger.info(\"Step 8: Cleaning other NAN/NULL values\")\n",
					"    \n",
					"    # Clean address fields\n",
					"    for addr_col in [\"address\", \"address2\", \"address3\", \"address4\", \"address5\", \"address6\"]:\n",
					"        df = df.withColumn(addr_col,\n",
					"            when(upper(trim(col(addr_col))).isin([\"NAN\", \"NULL\", \"NA\", \"\"]), lit(None))\n",
					"            .otherwise(col(addr_col))\n",
					"        )\n",
					"    \n",
					"    # Clean other string fields\n",
					"    for str_col in [\"title\", \"new_marker\", \"flags\", \"perm_disqual\", \"source_id\"]:\n",
					"        if str_col in df.columns:\n",
					"            df = df.withColumn(str_col,\n",
					"                when(upper(trim(col(str_col))).isin([\"NAN\", \"NULL\", \"NA\", \"\"]), lit(None))\n",
					"                .otherwise(col(str_col))\n",
					"            )\n",
					"    \n",
					"    final_count = df.count()\n",
					"    logger.info(f\"Final record count after comprehensive cleaning: {final_count}\")\n",
					"    logger.info(f\"Total records removed: {initial_count - final_count}\")\n",
					"    \n",
					"    return df"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"def transform_to_postgresql_schema(df):\n",
					"    \"\"\"\n",
					"    Transform DataFrame to match exact PostgreSQL juror_mod.voters schema\n",
					"    Note: part_no will be NULL here as it's generated by PostgreSQL sequence\n",
					"    \"\"\"\n",
					"    logger.info(\"Starting transformation to PostgreSQL schema\")\n",
					"    \n",
					"    # Apply comprehensive data cleaning first\n",
					"    df = clean_data_before_transformation(df)\n",
					"    \n",
					"    # Define the PostgreSQL staging schema structure (part_no will be NULL)\n",
					"    postgresql_schema = StructType([\n",
					"        StructField(\"part_no\", StringType(), True),            # NULL - generated in PostgreSQL\n",
					"        StructField(\"register_lett\", StringType(), True),      # VARCHAR(5)\n",
					"        StructField(\"poll_number\", StringType(), True),        # VARCHAR(5)\n",
					"        StructField(\"new_marker\", StringType(), True),         # VARCHAR(1)\n",
					"        StructField(\"title\", StringType(), True),              # VARCHAR(10)\n",
					"        StructField(\"lname\", StringType(), False),             # NOT NULL, VARCHAR(20)\n",
					"        StructField(\"fname\", StringType(), False),             # NOT NULL, VARCHAR(20)\n",
					"        StructField(\"dob\", DateType(), True),                  # DATE\n",
					"        StructField(\"flags\", StringType(), True),              # VARCHAR(2)\n",
					"        StructField(\"address\", StringType(), False),           # NOT NULL, VARCHAR(35)\n",
					"        StructField(\"address2\", StringType(), True),           # VARCHAR(35)\n",
					"        StructField(\"address3\", StringType(), True),           # VARCHAR(35)\n",
					"        StructField(\"address4\", StringType(), True),           # VARCHAR(35)\n",
					"        StructField(\"address5\", StringType(), True),           # VARCHAR(35)\n",
					"        StructField(\"address6\", StringType(), True),           # VARCHAR(35)\n",
					"        StructField(\"zip\", StringType(), True),                # VARCHAR(10)\n",
					"        StructField(\"date_selected1\", DateType(), True),       # DATE\n",
					"        StructField(\"date_selected2\", DateType(), True),       # DATE\n",
					"        StructField(\"date_selected3\", DateType(), True),       # DATE\n",
					"        StructField(\"rec_num\", IntegerType(), True),           # INTEGER\n",
					"        StructField(\"perm_disqual\", StringType(), True),       # VARCHAR(1)\n",
					"        StructField(\"source_id\", StringType(), True),          # VARCHAR(1)\n",
					"        StructField(\"postcode_start\", StringType(), True),     # VARCHAR(10) - generated in PostgreSQL\n",
					"        StructField(\"hash_id\", LongType(), True)             # Keep hash_id for deduplication\n",
					"    ])\n",
					"    \n",
					"    # Apply field length constraints and transformations\n",
					"    logger.info(\"Applying field length constraints\")\n",
					"    \n",
					"    df_constrained = df \\\n",
					"        .withColumn(\"part_no\", lit(None).cast(StringType())) \\\n",
					"        .withColumn(\"register_lett\", \n",
					"                   when(col(\"register_lett\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"register_lett\")), 1, 5))) \\\n",
					"        .withColumn(\"poll_number\", \n",
					"                   when(col(\"poll_number\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"poll_number\")), 1, 5))) \\\n",
					"        .withColumn(\"new_marker\", \n",
					"                   when(col(\"new_marker\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"new_marker\")), 1, 1))) \\\n",
					"        .withColumn(\"title\", \n",
					"                   when(col(\"title\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"title\")), 1, 10))) \\\n",
					"        .withColumn(\"lname\", \n",
					"                   substring(trim(upper(col(\"lname\"))), 1, 20)) \\\n",
					"        .withColumn(\"fname\", \n",
					"                   substring(trim(upper(col(\"fname\"))), 1, 20)) \\\n",
					"        .withColumn(\"flags\", \n",
					"                   when(col(\"flags\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"flags\")), 1, 2))) \\\n",
					"        .withColumn(\"address\", \n",
					"                   substring(trim(upper(col(\"address\"))), 1, 35)) \\\n",
					"        .withColumn(\"address2\", \n",
					"                   when(col(\"address2\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(upper(col(\"address2\"))), 1, 35))) \\\n",
					"        .withColumn(\"address3\", \n",
					"                   when(col(\"address3\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(upper(col(\"address3\"))), 1, 35))) \\\n",
					"        .withColumn(\"address4\", \n",
					"                   when(col(\"address4\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(upper(col(\"address4\"))), 1, 35))) \\\n",
					"        .withColumn(\"address5\", \n",
					"                   when(col(\"address5\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(upper(col(\"address5\"))), 1, 35))) \\\n",
					"        .withColumn(\"address6\", \n",
					"                   when(col(\"address6\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(upper(col(\"address6\"))), 1, 35))) \\\n",
					"        .withColumn(\"zip\", \n",
					"                   when(col(\"zip\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(upper(col(\"zip\"))), 1, 10))) \\\n",
					"        .withColumn(\"perm_disqual\", \n",
					"                   when(col(\"perm_disqual\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"perm_disqual\")), 1, 1))) \\\n",
					"        .withColumn(\"source_id\", \n",
					"                   when(col(\"source_id\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"source_id\")), 1, 1)))\n",
					"    \n",
					"    # Handle date fields - convert string dates to proper date type\n",
					"    logger.info(\"Converting date fields\")\n",
					"    \n",
					"    df_with_dates = df_constrained \\\n",
					"        .withColumn(\"dob\", \n",
					"                   when(col(\"dob\").isNull(), lit(None).cast(DateType()))\n",
					"                   .when(col(\"dob\") == \"\", lit(None).cast(DateType()))\n",
					"                   .otherwise(to_date(col(\"dob\"), \"yyyy-MM-dd\"))) \\\n",
					"        .withColumn(\"date_selected1\", \n",
					"                   when(col(\"date_selected1\").isNull(), lit(None).cast(DateType()))\n",
					"                   .when(col(\"date_selected1\") == \"\", lit(None).cast(DateType()))\n",
					"                   .otherwise(to_date(col(\"date_selected1\"), \"yyyy-MM-dd\"))) \\\n",
					"        .withColumn(\"date_selected2\", \n",
					"                   when(col(\"date_selected2\").isNull(), lit(None).cast(DateType()))\n",
					"                   .when(col(\"date_selected2\") == \"\", lit(None).cast(DateType()))\n",
					"                   .otherwise(to_date(col(\"date_selected2\"), \"yyyy-MM-dd\"))) \\\n",
					"        .withColumn(\"date_selected3\", \n",
					"                   when(col(\"date_selected3\").isNull(), lit(None).cast(DateType()))\n",
					"                   .when(col(\"date_selected3\") == \"\", lit(None).cast(DateType()))\n",
					"                   .otherwise(to_date(col(\"date_selected3\"), \"yyyy-MM-dd\")))\n",
					"    \n",
					"    # Set postcode_start to NULL (generated by PostgreSQL)\n",
					"    logger.info(\"Setting postcode_start to NULL - will be generated by PostgreSQL\")\n",
					"    \n",
					"    df_with_postcode = df_with_dates \\\n",
					"        .withColumn(\"postcode_start\", lit(None).cast(StringType()))\n",
					"    \n",
					"    # Ensure rec_num is integer type\n",
					"    df_with_rec_num = df_with_postcode \\\n",
					"        .withColumn(\"rec_num\", \n",
					"                   when(col(\"rec_num\").isNull(), lit(None).cast(IntegerType()))\n",
					"                   .otherwise(col(\"rec_num\").cast(IntegerType())))\n",
					"    # Ensure hash_id is long type (bigint)\n",
					"    df_with_hash = df_with_rec_num \\\n",
					"          .withColumn(\"hash_id\", \n",
					"                      when(col(\"hash_id\").isNull(), lit(None).cast(LongType()))\n",
					"                      .otherwise(col(\"hash_id\").cast(LongType())))\n",
					"    \n",
					"    # Select only the required columns in the correct order\n",
					"    final_columns = [\n",
					"        \"part_no\", \"register_lett\", \"poll_number\", \"new_marker\", \"title\",\n",
					"        \"lname\", \"fname\", \"dob\", \"flags\", \"address\", \"address2\", \"address3\",\n",
					"        \"address4\", \"address5\", \"address6\", \"zip\", \"date_selected1\",\n",
					"        \"date_selected2\", \"date_selected3\", \"rec_num\", \"perm_disqual\",\n",
					"        \"source_id\", \"postcode_start\", \"hash_id\"\n",
					"    ]\n",
					"    \n",
					"    df_final = df_with_rec_num.select(*final_columns)\n",
					"    \n",
					"    # Final validation - check for any remaining NULL violations\n",
					"    logger.info(\"Performing final validation\")\n",
					"    \n",
					"    null_lname = df_final.filter(col(\"lname\").isNull()).count()\n",
					"    null_fname = df_final.filter(col(\"fname\").isNull()).count()\n",
					"\n",
					"    \n",
					"    if null_lname > 0 or null_fname > 0:\n",
					"        logger.warning(f\"NULL constraint violations found: lname={null_lname}, fname={null_fname}\")\n",
					"        \n",
					"        # Remove records with NULL violations only for empty lname or fname\n",
					"        df_final = df_final.filter(\n",
					"            col(\"lname\").isNotNull() &\n",
					"            col(\"fname\").isNotNull() \n",
					"        )\n",
					"    \n",
					"    # Check hash_id uniqueness for deduplication verification\n",
					"    unique_hash_ids = df_final.select(\"hash_id\").distinct().count()\n",
					"    total_records = df_final.count()\n",
					"    \n",
					"    if unique_hash_ids != total_records:\n",
					"        logger.warning(f\"Duplicate hash_ids found: {total_records - unique_hash_ids} duplicates\")\n",
					"        # Remove duplicates keeping the first occurrence\n",
					"        from pyspark.sql.window import Window\n",
					"        window_spec = Window.partitionBy(\"hash_id\").orderBy(col(\"hash_id\"))\n",
					"        df_final = df_final.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                          .filter(col(\"row_num\") == 1) \\\n",
					"                          .drop(\"row_num\")\n",
					"    \n",
					"    logger.info(f\"Final record count after PostgreSQL schema transformation: {df_final.count()}\")\n",
					"    \n",
					"    return df_final\n",
					"\n",
					"def filter_by_date_range(df, date_list=None):\n",
					"    \"\"\"\n",
					"    Filter DataFrame by specific creation dates\n",
					"    \n",
					"    Args:\n",
					"        df: Input DataFrame\n",
					"        date_list: List of dates in YYYYMMDD format (as integers)\n",
					"    \n",
					"    Returns:\n",
					"        Filtered DataFrame\n",
					"    \"\"\"\n",
					"    if date_list is None or len(date_list) == 0:\n",
					"        logger.info(\"No date filtering applied - returning all records\")\n",
					"        return df\n",
					"    \n",
					"    logger.info(f\"Filtering records by creation dates: {date_list}\")\n",
					"    \n",
					"    # Create filter condition for multiple dates\n",
					"    date_conditions = None\n",
					"    for date_val in date_list:\n",
					"        condition = col(\"creation_date\") == lit(date_val)\n",
					"        if date_conditions is None:\n",
					"            date_conditions = condition\n",
					"        else:\n",
					"            date_conditions = date_conditions | condition\n",
					"    \n",
					"    filtered_df = df.filter(date_conditions)\n",
					"    \n",
					"    filtered_count = filtered_df.count()\n",
					"    logger.info(f\"Records after date filtering: {filtered_count}\")\n",
					"    \n",
					"    return filtered_df\n",
					"\n",
					"def trim_last_columns(df, columns_to_remove=6):\n",
					"    \"\"\"\n",
					"    Remove the last N columns from DataFrame\n",
					"    \n",
					"    Args:\n",
					"        df: Input DataFrame\n",
					"        columns_to_remove: Number of columns to remove from the end (default: 6)\n",
					"    \n",
					"    Returns:\n",
					"        DataFrame with trimmed columns\n",
					"    \"\"\"\n",
					"    logger.info(f\"Trimming last {columns_to_remove} columns from DataFrame\")\n",
					"    \n",
					"    # Get all column names\n",
					"    all_columns = df.columns\n",
					"    logger.info(f\"Original columns ({len(all_columns)}): {all_columns}\")\n",
					"    \n",
					"    # Keep all except the last N columns\n",
					"    if len(all_columns) > columns_to_remove:\n",
					"        columns_to_keep = all_columns[:-columns_to_remove]\n",
					"    else:\n",
					"        logger.warning(f\"DataFrame has only {len(all_columns)} columns, cannot remove {columns_to_remove}\")\n",
					"        columns_to_keep = all_columns\n",
					"    \n",
					"    logger.info(f\"Keeping columns ({len(columns_to_keep)}): {columns_to_keep}\")\n",
					"    \n",
					"    # Select only the columns to keep\n",
					"    trimmed_df = df.select(*columns_to_keep)\n",
					"    \n",
					"    return trimmed_df\n",
					"\n",
					"def export_to_csv(df, output_path, filename_prefix=\"voters_export\"):\n",
					"    \"\"\"\n",
					"    Export DataFrame to CSV with proper formatting\n",
					"    \n",
					"    Args:\n",
					"        df: DataFrame to export\n",
					"        output_path: Base output path\n",
					"        filename_prefix: Prefix for the CSV filename\n",
					"    \n",
					"    Returns:\n",
					"        Path to the exported CSV file\n",
					"    \"\"\"\n",
					"    logger.info(f\"Starting CSV export to {output_path}\")\n",
					"    \n",
					"    # Ensure output directory exists\n",
					"    if not mssparkutils.fs.exists(output_path):\n",
					"        logger.info(f\"Creating output directory: {output_path}\")\n",
					"        mssparkutils.fs.mkdirs(output_path)\n",
					"    \n",
					"    # Create timestamp for unique filename\n",
					"    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
					"    \n",
					"    # Write to temporary CSV folder\n",
					"    temp_output_dir = f\"{output_path}/temp_{timestamp}\"\n",
					"    \n",
					"    logger.info(f\"Writing to temporary directory: {temp_output_dir}\")\n",
					"    \n",
					"    try:\n",
					"        df.coalesce(1).write \\\n",
					"            .option(\"header\", \"true\") \\\n",
					"            .option(\"quoteAll\", \"true\") \\\n",
					"            .option(\"escape\", \"\\\\\") \\\n",
					"            .mode(\"overwrite\") \\\n",
					"            .csv(temp_output_dir)\n",
					"        \n",
					"        logger.info(\"CSV write completed, locating output file\")\n",
					"        \n",
					"        # Locate and rename the .csv file\n",
					"        files_written = mssparkutils.fs.ls(temp_output_dir)\n",
					"        csv_found = False\n",
					"        final_csv_path = None\n",
					"        \n",
					"        for f in files_written:\n",
					"            if f.path.endswith(\".csv\") and \"part-\" in f.path:\n",
					"                final_csv_path = f\"{output_path}/{filename_prefix}_{timestamp}.csv\"\n",
					"                mssparkutils.fs.cp(f.path, final_csv_path)\n",
					"                logger.info(f\"📄 Exported to: {final_csv_path}\")\n",
					"                csv_found = True\n",
					"                break\n",
					"        \n",
					"        # Check if a CSV file was found and copied\n",
					"        if not csv_found:\n",
					"            logger.error(\"⚠️ No CSV file was found in the output directory\")\n",
					"            final_csv_path = None\n",
					"        \n",
					"        # Clean up temp directory\n",
					"        logger.info(\"Cleaning up temporary directory\")\n",
					"        mssparkutils.fs.rm(temp_output_dir, recurse=True)\n",
					"        \n",
					"        return final_csv_path\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error during CSV export: {str(e)}\")\n",
					"        # Clean up temp directory even on error\n",
					"        try:\n",
					"            mssparkutils.fs.rm(temp_output_dir, recurse=True)\n",
					"        except:\n",
					"            pass\n",
					"        raise\n",
					"\n",
					"def export_voters_to_postgresql_schema():\n",
					"    \"\"\"\n",
					"    Main function to export voters data with PostgreSQL schema compliance and date filtering\n",
					"    \"\"\"\n",
					"    try:\n",
					"        # Initialize storage paths\n",
					"        storage_account = \"baubaisadfsaprod\"\n",
					"        source_path = f\"abfss://dl-juror-eric-voters-temp@{storage_account}.dfs.core.windows.net/voters_deduplicated_delta\"\n",
					"        output_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/exports\"\n",
					"        \n",
					"        # MANUAL DATE CONFIGURATION - MODIFY THESE VALUES AS NEEDED\n",
					"        # Define your date range - update these dates manually as needed\n",
					"        target_dates = [\n",
					"            20241213,  # 2024-12-19\n",
					"            # Add more dates as needed in YYYYMMDD format\n",
					"        ]\n",
					"        \n",
					"        logger.info(f\"Target dates for filtering: {target_dates}\")\n",
					"        \n",
					"        # Read the existing Delta table\n",
					"        logger.info(f\"Reading voter data from {source_path}\")\n",
					"        source_df = spark.read.format(\"delta\").load(source_path)\n",
					"        \n",
					"        initial_count = source_df.count()\n",
					"        logger.info(f\"Initial record count from Delta table: {initial_count}\")\n",
					"        \n",
					"        # Filter by date range\n",
					"        filtered_df = filter_by_date_range(source_df, target_dates)\n",
					"        \n",
					"        # Check if we have any records after filtering\n",
					"        filtered_count = filtered_df.count()\n",
					"        if filtered_count == 0:\n",
					"            logger.warning(\"No records found for the specified date range\")\n",
					"            return None, None\n",
					"        \n",
					"        # Trim the last 6 columns\n",
					"        filtered_trimmed_df = trim_last_columns(filtered_df, columns_to_remove=6)\n",
					"        \n",
					"        logger.info(f\"Records ready for export: {filtered_trimmed_df.count()}\")\n",
					"        \n",
					"        # Transform to PostgreSQL schema\n",
					"        logger.info(\"Applying PostgreSQL schema transformation\")\n",
					"        postgresql_df = transform_to_postgresql_schema(filtered_trimmed_df)\n",
					"        \n",
					"        # Export to CSV\n",
					"        logger.info(\"Exporting to CSV\")\n",
					"        csv_file_path = export_to_csv(\n",
					"            postgresql_df, \n",
					"            output_path, \n",
					"            filename_prefix=\"voters_postgresql_export\"\n",
					"        )\n",
					"        \n",
					"        if csv_file_path:\n",
					"            logger.info(f\"✅ Export completed successfully: {csv_file_path}\")\n",
					"            logger.info(f\"📊 Final export statistics:\")\n",
					"            logger.info(f\"   - Initial records: {initial_count}\")\n",
					"            logger.info(f\"   - After date filtering: {filtered_count}\")\n",
					"            logger.info(f\"   - Final exported records: {postgresql_df.count()}\")\n",
					"        else:\n",
					"            logger.error(\"❌ Export failed - no CSV file created\")\n",
					"            \n",
					"        return postgresql_df, csv_file_path\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error in PostgreSQL export process: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return None, None\n",
					"\n",
					"# Execute the export with enhanced functionality\n",
					"if __name__ == \"__main__\":\n",
					"    logger.info(\"Starting Enhanced PostgreSQL Export Pipeline\")\n",
					"    \n",
					"    result_df, csv_path = export_voters_to_postgresql_schema()\n",
					"    \n",
					"    if result_df is not None and csv_path is not None:\n",
					"        logger.info(\"🎉 Enhanced PostgreSQL export pipeline completed successfully\")\n",
					"        logger.info(f\"📁 CSV export location: {csv_path}\")\n",
					"        logger.info(f\"🔢 Total records exported: {result_df.count()}\")\n",
					"        \n",
					"        # Show sample of the exported data\n",
					"        logger.info(\"📋 Sample of exported data:\")\n",
					"        result_df.show(5, truncate=False)\n",
					"        \n",
					"    else:\n",
					"        logger.error(\"💥 Enhanced PostgreSQL export pipeline failed\")"
				],
				"execution_count": 8
			}
		]
	}
}