{
	"name": "Notebook 1",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkstg",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "2f5bba2a-9f2f-4301-8080-4780eece60da"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/74dacd4f-a248-45bb-a2f0-af700dc4cf68/resourceGroups/baubais-data-factory-rg-stg/providers/Microsoft.Synapse/workspaces/baubais-synapse-stg/bigDataPools/sparkstg",
				"name": "sparkstg",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-stg.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkstg",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession, Window\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"from notebookutils import mssparkutils\n",
					"import logging\n",
					"import json\n",
					"import datetime\n",
					"import requests\n",
					"import sys\n",
					"\n",
					"# Configure logging\n",
					"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"logger = logging.getLogger()\n",
					"\n",
					"# Initialize Spark session\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"Update Voters Temp Table\") \\\n",
					"    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
					"    .getOrCreate()\n",
					"\n",
					"def find_latest_data():\n",
					"    \"\"\"Find the latest data in the eric-juror-voters-temp data lake\"\"\"\n",
					"    storage_account = \"baubaisadfsastg\"\n",
					"    base_path = f\"abfss://eric-juror-voters-temp@{storage_account}.dfs.core.windows.net\"\n",
					"    \n",
					"    # Check if latest.txt exists to get the latest date\n",
					"    latest_path = f\"{base_path}/latest.txt\"\n",
					"    \n",
					"    try:\n",
					"        if mssparkutils.fs.exists(latest_path):\n",
					"            latest_date = mssparkutils.fs.read(latest_path).strip()\n",
					"            logger.info(f\"Latest date from latest.txt: {latest_date}\")\n",
					"        else:\n",
					"            # If latest.txt doesn't exist, list directories and find the latest date\n",
					"            dirs = mssparkutils.fs.ls(base_path)\n",
					"            date_dirs = [d.name for d in dirs if d.name.startswith(\"20\") and d.isDir]\n",
					"            \n",
					"            if not date_dirs:\n",
					"                raise Exception(\"No date directories found\")\n",
					"                \n",
					"            # Sort directories to find the latest date\n",
					"            latest_date = sorted(date_dirs)[-1]\n",
					"            logger.info(f\"Latest date from directory listing: {latest_date}\")\n",
					"        \n",
					"        # Check if the directory exists and has data\n",
					"        latest_dir = f\"{base_path}/{latest_date}\"\n",
					"        if not mssparkutils.fs.exists(latest_dir):\n",
					"            raise Exception(f\"Latest directory does not exist: {latest_dir}\")\n",
					"            \n",
					"        # Return the path to the latest data\n",
					"        return latest_dir\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error finding latest data: {str(e)}\")\n",
					"        raise\n",
					"\n",
					"def load_voters_data(latest_path):\n",
					"    \"\"\"Load the voters data from the latest path\"\"\"\n",
					"    try:\n",
					"        logger.info(f\"Loading voters data from: {latest_path}\")\n",
					"        voters_df = spark.read.parquet(latest_path)\n",
					"        logger.info(f\"Loaded {voters_df.count()} voter records\")\n",
					"        \n",
					"        # Preview the schema and data\n",
					"        logger.info(\"Voters data schema:\")\n",
					"        voters_df.printSchema()\n",
					"        \n",
					"        # Sample a few records for debugging\n",
					"        logger.info(\"Sample voter records:\")\n",
					"        voters_df.show(5, truncate=False)\n",
					"        \n",
					"        return voters_df\n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error loading voters data: {str(e)}\")\n",
					"        raise\n",
					"        \n",
					"def get_catchment_area_data():\n",
					"    \"\"\"\n",
					"    Create a simulated dataset for catchment area as we cannot directly query PostgreSQL\n",
					"    In a real scenario, this would be replaced with an actual database connection\n",
					"    or use the Azure Function approach to fetch data from PostgreSQL\n",
					"    \"\"\"\n",
					"    # Sample schema that simulates the catchment_area table\n",
					"    schema = StructType([\n",
					"        StructField(\"loc_code\", StringType(), True),\n",
					"        StructField(\"postcode_start\", StringType(), True)\n",
					"    ])\n",
					"    \n",
					"    # This is where you would replace with either:\n",
					"    # 1. Call to Azure Function to get catchment area data from PostgreSQL\n",
					"    # 2. Direct JDBC connection to PostgreSQL if network allows\n",
					"    \n",
					"    # For this example, we'll simulate some catchment area data\n",
					"    # In reality, you would make an HTTP request to your Azure Function here\n",
					"    \n",
					"    # Example Azure Function call\n",
					"    # response = requests.get(\n",
					"    #     \"https://your-function-app.azurewebsites.net/api/GetCatchmentAreas\",\n",
					"    #     headers={\"x-functions-key\": \"your-function-key\"}\n",
					"    # )\n",
					"    # catchment_data = json.loads(response.text)\n",
					"    # catchment_df = spark.createDataFrame(catchment_data, schema)\n",
					"    \n",
					"    # Simulated data for demonstration\n",
					"    # In production, replace this with actual data from PostgreSQL\n",
					"    catchment_data = [\n",
					"        (\"404\", \"B11\"),\n",
					"        (\"127\", \"B11\"),\n",
					"        (\"405\", \"B12\"),\n",
					"        (\"128\", \"B13\"),\n",
					"        (\"406\", \"B14\"),\n",
					"        (\"129\", \"B15\"),\n",
					"        # Add more sample data as needed\n",
					"    ]\n",
					"    \n",
					"    catchment_df = spark.createDataFrame(catchment_data, schema)\n",
					"    logger.info(f\"Loaded {catchment_df.count()} catchment area records\")\n",
					"    catchment_df.show()\n",
					"    \n",
					"    return catchment_df\n",
					"\n",
					"def match_loc_codes(voters_df, catchment_df):\n",
					"    \"\"\"Match postcode_start with catchment area to fill in loc_code\"\"\"\n",
					"    logger.info(\"Matching postcode_start with catchment areas\")\n",
					"    \n",
					"    # Extract postcode start from full postcode (first part before space)\n",
					"    # Make sure to handle null values and trimming\n",
					"    voters_with_postcode_start = voters_df.withColumn(\n",
					"        \"extracted_postcode_start\",\n",
					"        when(col(\"postcode_start\").isNotNull() & (length(trim(col(\"postcode_start\"))) > 0),\n",
					"             col(\"postcode_start\")\n",
					"        ).when(col(\"zip\").isNotNull() & (length(trim(col(\"zip\"))) > 0),\n",
					"             split(trim(col(\"zip\")), \" \")[0]\n",
					"        ).otherwise(None)\n",
					"    )\n",
					"    \n",
					"    # Join with catchment areas to get loc_code\n",
					"    # Use broadcast join if catchment_df is small enough\n",
					"    result = voters_with_postcode_start.join(\n",
					"        broadcast(catchment_df),\n",
					"        col(\"extracted_postcode_start\") == col(\"postcode_start\"),\n",
					"        \"left\"\n",
					"    )\n",
					"    \n",
					"    # Update loc_code if it's null or empty\n",
					"    result = result.withColumn(\n",
					"        \"loc_code\",\n",
					"        when(\n",
					"            (col(\"loc_code\").isNull()) | \n",
					"            (length(trim(col(\"loc_code\"))) == 0),\n",
					"            col(\"catchment_df.loc_code\")\n",
					"        ).otherwise(col(\"loc_code\"))\n",
					"    )\n",
					"    \n",
					"    # Drop the temporary and duplicate columns\n",
					"    result = result.drop(\"catchment_df.loc_code\", \"extracted_postcode_start\")\n",
					"    \n",
					"    logger.info(f\"After matching, {result.filter(col('loc_code').isNotNull()).count()} records have loc_code\")\n",
					"    \n",
					"    return result\n",
					"\n",
					"def generate_part_no(voters_df):\n",
					"    \"\"\"Generate part_no based on YNNNNNNNN format\"\"\"\n",
					"    logger.info(\"Generating part_no numbers\")\n",
					"    \n",
					"    # Calculate year from creation_date - extract last digit of year\n",
					"    voters_with_year = voters_df.withColumn(\n",
					"        \"year_digit\",\n",
					"        when(col(\"creation_date\").isNotNull(),\n",
					"             substring(year(to_date(col(\"creation_date\"), \"yyyyMMdd\")), -1, 1)  # Take last digit of year\n",
					"        ).otherwise(\n",
					"             substring(year(current_date()), -1, 1)  # Default to current year last digit\n",
					"        )\n",
					"    )\n",
					"    \n",
					"    # Create a window spec to generate sequential numbers by hash_id\n",
					"    # This ensures the same voter gets the same sequence even with different loc_codes\n",
					"    window_spec = Window.partitionBy(\"hash_id\").orderBy(\"creation_date\")\n",
					"    \n",
					"    # Generate sequence numbers within each hash_id group\n",
					"    voters_with_seq = voters_with_year.withColumn(\n",
					"        \"sequence_number\", \n",
					"        row_number().over(window_spec)\n",
					"    )\n",
					"    \n",
					"    # Format part_no as YNNNNNNNN\n",
					"    result = voters_with_seq.withColumn(\n",
					"        \"part_no\",\n",
					"        concat(\n",
					"            col(\"year_digit\"),\n",
					"            lpad(col(\"sequence_number\"), 8, \"0\")\n",
					"        )\n",
					"    )\n",
					"    \n",
					"    # Drop temporary columns\n",
					"    result = result.drop(\"year_digit\", \"sequence_number\")\n",
					"    \n",
					"    logger.info(\"Part numbers generated successfully\")\n",
					"    result.select(\"part_no\", \"hash_id\").show(5)\n",
					"    \n",
					"    return result\n",
					"\n",
					"def check_existing_records(voters_df):\n",
					"    \"\"\"\n",
					"    Check for existing records in voters_temp table based on name, DOB instead of hash_id\n",
					"    In a real scenario, this would query the PostgreSQL database\n",
					"    \"\"\"\n",
					"    logger.info(\"Checking for existing records in voters_temp table\")\n",
					"    \n",
					"    # In an actual implementation, we would collect key identifying information from the dataframe\n",
					"    # and send it to our Azure Function to check for existing records\n",
					"    \n",
					"    # Example of how to implement this in production:\n",
					"    \"\"\"\n",
					"    # Sample a limited number of records to check (to avoid memory issues)\n",
					"    sample_size = min(1000, voters_df.count())\n",
					"    voter_sample = voters_df.select(\"fname\", \"lname\", \"dob\").limit(sample_size).collect()\n",
					"    \n",
					"    # Convert to list of dictionaries\n",
					"    voter_records = [{\"fname\": row.fname, \"lname\": row.lname, \"dob\": row.dob} for row in voter_sample]\n",
					"    \n",
					"    # Call Azure Function to check for existing records\n",
					"    response = requests.post(\n",
					"        \"https://your-function-app.azurewebsites.net/api/CheckExistingRecords\",\n",
					"        json={\"voter_records\": voter_records},\n",
					"        headers={\"x-functions-key\": \"your-function-key\"}\n",
					"    )\n",
					"    \n",
					"    # Parse response\n",
					"    if response.status_code == 200:\n",
					"        existing_records = json.loads(response.text)\n",
					"        logger.info(f\"Found {len(existing_records)} existing records\")\n",
					"    else:\n",
					"        logger.warning(f\"Failed to check existing records: {response.text}\")\n",
					"        existing_records = {}\n",
					"    \"\"\"\n",
					"    \n",
					"    # For this simulation, we'll assume there are no existing records\n",
					"    logger.info(\"This is a simulation - in production, query the actual database\")\n",
					"    \n",
					"    # Return the dataframe without modifications\n",
					"    return voters_df\n",
					"\n",
					"def prepare_final_data(voters_df):\n",
					"    \"\"\"\n",
					"    Prepare final data for insertion by dropping unnecessary columns\n",
					"    and ensuring schema matches the target table\n",
					"    \"\"\"\n",
					"    logger.info(\"Preparing final data for database insertion\")\n",
					"    \n",
					"    # First, explicitly drop the hash_id and creation_date columns as requested\n",
					"    if \"hash_id\" in voters_df.columns:\n",
					"        voters_df = voters_df.drop(\"hash_id\")\n",
					"        logger.info(\"Dropped hash_id column as requested\")\n",
					"        \n",
					"    if \"creation_date\" in voters_df.columns:\n",
					"        voters_df = voters_df.drop(\"creation_date\")\n",
					"        logger.info(\"Dropped creation_date column as requested\")\n",
					"    \n",
					"    # List of columns to keep (all required columns in voters_temp table)\n",
					"    required_columns = [\n",
					"        \"loc_code\", \"part_no\", \"register_lett\", \"poll_number\", \n",
					"        \"new_marker\", \"title\", \"lname\", \"fname\", \"dob\", \n",
					"        \"flags\", \"address\", \"address2\", \"address3\", \"address4\", \n",
					"        \"address5\", \"address6\", \"zip\", \"date_selected1\", \n",
					"        \"date_selected2\", \"date_selected3\", \"rec_num\", \n",
					"        \"perm_disqual\", \"source_id\", \"postcode_start\"\n",
					"    ]\n",
					"    \n",
					"    # Check which columns actually exist in our dataframe\n",
					"    existing_columns = [col for col in required_columns if col in voters_df.columns]\n",
					"    \n",
					"    # Select only required columns for final output\n",
					"    final_df = voters_df.select(existing_columns)\n",
					"    \n",
					"    # For any missing columns, add them with null values\n",
					"    for col_name in required_columns:\n",
					"        if col_name not in existing_columns:\n",
					"            final_df = final_df.withColumn(col_name, lit(None))\n",
					"    \n",
					"    logger.info(f\"Final data prepared with {final_df.count()} records\")\n",
					"    final_df.printSchema()\n",
					"    \n",
					"    return final_df\n",
					"\n",
					"def insert_into_postgresql(final_df):\n",
					"    \"\"\"\n",
					"    Insert data into PostgreSQL using Azure Function\n",
					"    In a real implementation, this would call your Azure Function to do the insert\n",
					"    \"\"\"\n",
					"    # Count records for logging\n",
					"    record_count = final_df.count()\n",
					"    logger.info(f\"Inserting {record_count} records into PostgreSQL\")\n",
					"\n",
					"    # 3. Call your Azure Function to do the insert\n",
					"    \n",
					"    # Example of how you would batch and send (pseudocode):\n",
					"    '''\n",
					"    # Define batch size\n",
					"    batch_size = 1000\n",
					"    \n",
					"    # Get total record count\n",
					"    total_records = final_df.count()\n",
					"    \n",
					"    # Process in batches\n",
					"    for i in range(0, total_records, batch_size):\n",
					"        # Convert batch to list of dictionaries\n",
					"        batch_df = final_df.limit(batch_size).offset(i)\n",
					"        batch_data = [row.asDict() for row in batch_df.collect()]\n",
					"        \n",
					"        # Send to Azure Function\n",
					"        response = requests.post(\n",
					"            \"https://your-function-app.azurewebsites.net/api/InsertVotersData\",\n",
					"            json={\"records\": batch_data, \"table\": \"juror_eric.voters_temp\"},\n",
					"            headers={\"x-functions-key\": \"your-function-key\"}\n",
					"        )\n",
					"        \n",
					"        # Check response\n",
					"        if response.status_code != 200:\n",
					"            logger.error(f\"Error inserting batch: {response.text}\")\n",
					"            raise Exception(f\"Failed to insert batch: {response.text}\")\n",
					"        \n",
					"        logger.info(f\"Inserted batch {i//batch_size + 1}, records {i+1} to {min(i+batch_size, total_records)}\")\n",
					"    '''\n",
					"    \n",
					"    # For this example, we'll just log that we would insert the data\n",
					"    logger.info(\"In a real implementation, data would be sent to PostgreSQL\")\n",
					"    logger.info(f\"Sample of data that would be inserted:\")\n",
					"    final_df.show(5, truncate=False)\n",
					"    \n",
					"    return True\n",
					"\n",
					"def save_to_datalake(final_df):\n",
					"    \"\"\"Save processed data to data lake for verification or backup\"\"\"\n",
					"    storage_account = \"baubaisadfsastg\"\n",
					"    output_path = f\"abfss://eric-juror-voters-temp@{storage_account}.dfs.core.windows.net/processed_outputs/{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
					"    \n",
					"    logger.info(f\"Saving processed data to {output_path}\")\n",
					"    \n",
					"    try:\n",
					"        final_df.write.mode(\"overwrite\").parquet(output_path)\n",
					"        logger.info(f\"Successfully saved processed data to {output_path}\")\n",
					"        return output_path\n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error saving processed data: {str(e)}\")\n",
					"        return None\n",
					"\n",
					"def main():\n",
					"    try:\n",
					"        # Step 1: Find the latest data\n",
					"        latest_path = find_latest_data()\n",
					"        logger.info(f\"Latest data path: {latest_path}\")\n",
					"        \n",
					"        # Step 2: Load the voters data\n",
					"        voters_df = load_voters_data(latest_path)\n",
					"        \n",
					"        # Step 3: Get catchment area data\n",
					"        catchment_df = get_catchment_area_data()\n",
					"        \n",
					"        # Step 4: Match loc_codes\n",
					"        voters_with_loc = match_loc_codes(voters_df, catchment_df)\n",
					"        \n",
					"        # Step 5: Generate part_no\n",
					"        voters_with_part_no = generate_part_no(voters_with_loc)\n",
					"        \n",
					"        # Step 6: Check for existing records\n",
					"        voters_final = check_existing_records(voters_with_part_no)\n",
					"        \n",
					"        # Step 7: Prepare final data (drop creation_date and hash_id)\n",
					"        final_df = prepare_final_data(voters_final)\n",
					"        \n",
					"        # Step 8: Save processed data to data lake for verification\n",
					"        output_path = save_to_datalake(final_df)\n",
					"        \n",
					"        # Step 9: Insert into PostgreSQL via Azure Function\n",
					"        success = insert_into_postgresql(final_df)\n",
					"        \n",
					"        if success:\n",
					"            logger.info(\"Process completed successfully\")\n",
					"            logger.info(f\"Processed data saved to: {output_path}\")\n",
					"            return \"Success\"\n",
					"        else:\n",
					"            logger.error(\"Failed to insert data into PostgreSQL\")\n",
					"            return \"Failed\"\n",
					"            \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error in main process: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return f\"Error: {str(e)}\"\n",
					"\n",
					"# Run the main function\n",
					"if __name__ == \"__main__\":\n",
					"    result = main()\n",
					"    print(f\"Process result: {result}\")"
				],
				"execution_count": null
			}
		]
	}
}