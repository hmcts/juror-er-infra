{
	"name": "Parquet_To_Csv_Whole",
	"properties": {
		"folder": {
			"name": "crime/testing"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkprod",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "a3014ef2-8750-465e-bd02-c5cafe0c1f20"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5ca62022-6aa2-4cee-aaa7-e7536c8d566c/resourceGroups/baubais-data-factory-rg-prod/providers/Microsoft.Synapse/workspaces/baubais-synapse-prod/bigDataPools/sparkprod",
				"name": "sparkprod",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkprod",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"#!/usr/bin/env python\n",
					"# coding: utf-8\n",
					"\n",
					"import datetime\n",
					"import time\n",
					"import os\n",
					"import re\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col, when, lit, regexp_replace, trim, upper, substring, to_date, date_format, expr\n",
					"from pyspark.sql.types import StringType, IntegerType, TimestampType, LongType, DateType\n",
					"import pyspark.sql.functions as F\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Initialize Spark\n",
					"spark = SparkSession.builder.getOrCreate()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"# Configuration\n",
					"storage_account = \"baubaisadfsaprod\"\n",
					"input_dir = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_postgresql\"\n",
					"output_dir = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/exports\"\n",
					"\n",
					"# Max records per batch (for large datasets)\n",
					"MAX_RECORDS_PER_BATCH = 2000000\n",
					"\n",
					"# Ensure output directory exists\n",
					"if not mssparkutils.fs.exists(output_dir):\n",
					"    mssparkutils.fs.mkdirs(output_dir)\n",
					"\n",
					"# Define a function for aggressive address field sanitization\n",
					"def sanitize_address_field(column):\n",
					"    # Replace apostrophes with nothing\n",
					"    cleaned = regexp_replace(column, \"'\", \"\")\n",
					"    # Replace quotes with nothing\n",
					"    cleaned = regexp_replace(cleaned, '\"', \"\")\n",
					"    # Replace backslashes with forward slashes\n",
					"    cleaned = regexp_replace(cleaned, \"\\\\\\\\\", \"/\")\n",
					"    # Replace multiple spaces with single space\n",
					"    cleaned = regexp_replace(cleaned, \"\\\\s+\", \" \")\n",
					"    # Remove any non-ASCII characters (using a pattern that matches non-ASCII)\n",
					"    cleaned = regexp_replace(cleaned, \"[^\\\\x00-\\\\x7F]+\", \"\")\n",
					"    # Remove any other problematic characters (keep alphanumeric, spaces, periods, commas, slashes, hyphens, and #)\n",
					"    cleaned = regexp_replace(cleaned, \"[^\\\\w\\\\s.,/\\\\-#]\", \"\")\n",
					"    return cleaned\n",
					"\n",
					"# Define a function for basic non-address field sanitization\n",
					"def sanitize_non_address_field(column):\n",
					"    # Less aggressive cleaning for non-address fields\n",
					"    # Just remove apostrophes and quotes\n",
					"    cleaned = regexp_replace(column, \"'\", \"\")\n",
					"    cleaned = regexp_replace(cleaned, '\"', \"\")\n",
					"    return cleaned\n",
					"\n",
					"try:\n",
					"    print(f\"🔄 Processing all partitioned parquet data in {input_dir}\")\n",
					"    \n",
					"    # Read all parquet data from the directory (including partitions)\n",
					"    df = spark.read.parquet(input_dir)\n",
					"    record_count = df.count()\n",
					"    print(f\"   ✅ Read {record_count} records from all partitions\")\n",
					"    \n",
					"    # Drop creation_date_partition column if it exists (has 26 columns)\n",
					"    columns = df.columns\n",
					"    if len(columns) == 26 and \"creation_date_partition\" in columns:\n",
					"        df = df.drop(\"creation_date_partition\")\n",
					"        print(\"   ✅ Dropped creation_date_partition column\")\n",
					"    \n",
					"    # Apply data cleaning and transformation (from DataType_transformed_batch_Pyspark_01.py)\n",
					"    print(\"   🧹 Cleaning and transforming data...\")\n",
					"    \n",
					"    # 1. Clean dob column format\n",
					"    if \"dob\" in df.columns:\n",
					"        df = df.withColumn(\n",
					"            \"dob\",\n",
					"            when(\n",
					"                col(\"dob\").isNotNull(),\n",
					"                to_date(col(\"dob\"), \"dd/MM/yyyy\")\n",
					"            ).otherwise(None)\n",
					"        )\n",
					"        df = df.withColumn(\"dob\", date_format(col(\"dob\"), \"yyyy-MM-dd\"))\n",
					"        print(\"   ✅ Normalized dob column format\")\n",
					"    \n",
					"    # 2. Force proper string type for flags and new_marker column\n",
					"    invalid_values = [\"\", \"nan\", \"NaN\", \"None\", \"NONE\", \"null\", \"NULL\", \"nan.0\"]\n",
					"    \n",
					"    if \"new_marker\" in df.columns:\n",
					"        df = df.withColumn(\"new_marker\", trim(col(\"new_marker\").cast(StringType())))\n",
					"        df = df.withColumn(\n",
					"            \"new_marker\",\n",
					"            when(\n",
					"                col(\"new_marker\").isin(invalid_values),\n",
					"                None\n",
					"            ).otherwise(col(\"new_marker\"))\n",
					"        )\n",
					"        # Apply basic sanitization\n",
					"        df = df.withColumn(\"new_marker\", sanitize_non_address_field(col(\"new_marker\")))\n",
					"        print(\"   ✅ Cleaned new_marker column\")\n",
					"    \n",
					"    if \"flags\" in df.columns:\n",
					"        df = df.withColumn(\"flags\", trim(col(\"flags\").cast(StringType())))\n",
					"        df = df.withColumn(\n",
					"            \"flags\",\n",
					"            when(\n",
					"                col(\"flags\").isin(invalid_values),\n",
					"                None\n",
					"            ).otherwise(col(\"flags\"))\n",
					"        )\n",
					"        # Apply basic sanitization\n",
					"        df = df.withColumn(\"flags\", sanitize_non_address_field(col(\"flags\")))\n",
					"        print(\"   ✅ Cleaned flags column\")\n",
					"    \n",
					"    # 3. Force the data type for title column\n",
					"    if \"title\" in df.columns:\n",
					"        df = df.withColumn(\"title\", trim(col(\"title\").cast(StringType())))\n",
					"        df = df.withColumn(\n",
					"            \"title\",\n",
					"            when(\n",
					"                col(\"title\").isin(invalid_values),\n",
					"                None\n",
					"            ).otherwise(col(\"title\"))\n",
					"        )\n",
					"        \n",
					"        # Truncate title to 10 characters max\n",
					"        df = df.withColumn(\n",
					"            \"title\",\n",
					"            when(\n",
					"                F.length(col(\"title\")) > 10,\n",
					"                substring(col(\"title\"), 1, 10)\n",
					"            ).otherwise(col(\"title\"))\n",
					"        )\n",
					"        \n",
					"        # Apply basic sanitization\n",
					"        df = df.withColumn(\"title\", sanitize_non_address_field(col(\"title\")))\n",
					"        print(\"   ✅ Cleaned title column\")\n",
					"    \n",
					"    # 4. Fix register_lett\n",
					"    if \"register_lett\" in df.columns:\n",
					"        df = df.withColumn(\n",
					"            \"register_lett\",\n",
					"            substring(trim(col(\"register_lett\").cast(StringType())), 1, 5)\n",
					"        )\n",
					"        # Apply basic sanitization\n",
					"        df = df.withColumn(\"register_lett\", sanitize_non_address_field(col(\"register_lett\")))\n",
					"        print(\"   ✅ Fixed register_lett column\")\n",
					"    \n",
					"    # 5. Make sure poll_number is properly formatted (5 characters max)\n",
					"    if \"poll_number\" in df.columns:\n",
					"        # First convert to numeric to clean up potential float values (.0)\n",
					"        df = df.withColumn(\n",
					"            \"poll_number\",\n",
					"            when(\n",
					"                col(\"poll_number\").cast(\"double\").isNotNull(),\n",
					"                col(\"poll_number\").cast(\"int\").cast(StringType())\n",
					"            ).otherwise(col(\"poll_number\").cast(StringType()))\n",
					"        )\n",
					"        # Then truncate to 5 characters\n",
					"        df = df.withColumn(\n",
					"            \"poll_number\",\n",
					"            substring(trim(col(\"poll_number\")), 1, 5)\n",
					"        )\n",
					"        # Apply basic sanitization\n",
					"        df = df.withColumn(\"poll_number\", sanitize_non_address_field(col(\"poll_number\")))\n",
					"        print(\"   ✅ Fixed poll_number column\")\n",
					"    \n",
					"    # 6. Ensure hash_id is properly formatted\n",
					"    if \"hash_id\" in df.columns:\n",
					"        df = df.withColumn(\n",
					"            \"hash_id\",\n",
					"            when(\n",
					"                col(\"hash_id\").isNull(),\n",
					"                # Generate a new hash_id if missing using a combination of fields\n",
					"                F.xxhash64(\n",
					"                    F.concat_ws(\n",
					"                        \"||\",\n",
					"                        F.trim(F.coalesce(col(\"register_lett\"), lit(\"\"))),\n",
					"                        F.trim(F.coalesce(col(\"poll_number\"), lit(\"\"))),\n",
					"                        F.upper(F.trim(F.coalesce(col(\"fname\"), lit(\"\")))),\n",
					"                        F.upper(F.trim(F.coalesce(col(\"lname\"), lit(\"\")))),\n",
					"                        F.trim(F.coalesce(col(\"rec_num\"), lit(\"\"))),\n",
					"                        F.trim(F.coalesce(col(\"zip\"), lit(\"\"))),\n",
					"                        F.trim(F.coalesce(col(\"address\"), lit(\"\")))\n",
					"                    )\n",
					"                )\n",
					"            ).otherwise(col(\"hash_id\"))\n",
					"        )\n",
					"        print(\"   ✅ Validated hash_id column\")\n",
					"    \n",
					"    # 7. Clean creation_date\n",
					"    if \"creation_date\" in df.columns:\n",
					"        df = df.withColumn(\n",
					"            \"creation_date\",\n",
					"            when(\n",
					"                col(\"creation_date\").isNotNull(),\n",
					"                to_date(col(\"creation_date\").cast(StringType()), \"yyyyMMdd\")\n",
					"            ).otherwise(None)\n",
					"        )\n",
					"        # Format as YYYY-MM-DD for PostgreSQL\n",
					"        df = df.withColumn(\"creation_date\", date_format(col(\"creation_date\"), \"yyyy-MM-dd\"))\n",
					"        \n",
					"        # Filter out rows with invalid creation_date\n",
					"        df = df.filter(col(\"creation_date\").isNotNull())\n",
					"        print(\"   ✅ Cleaned creation_date column\")\n",
					"    \n",
					"    # 8. Apply address-specific sanitization to all address fields\n",
					"    address_columns = [c for c in df.columns if \"address\" in c.lower() or c.lower() == \"zip\"]\n",
					"    print(f\"   🧹 Applying aggressive sanitization to {len(address_columns)} address columns: {address_columns}\")\n",
					"    \n",
					"    for address_col in address_columns:\n",
					"        if address_col in df.columns:\n",
					"            # Apply aggressive sanitization to address fields\n",
					"            df = df.withColumn(address_col, sanitize_address_field(col(address_col)))\n",
					"    \n",
					"    # 8b. Replace empty address4 with \"UNKNOWN\"\n",
					"    if \"address4\" in df.columns:\n",
					"        df = df.withColumn(\n",
					"            \"address4\",\n",
					"            when(\n",
					"                col(\"address4\").isNull() | (trim(col(\"address4\")) == \"\"),\n",
					"                lit(\"UNKNOWN\")\n",
					"            ).otherwise(col(\"address4\"))\n",
					"        )\n",
					"        print(\"   ✅ Ensured address4 column has no NULL values\")\n",
					"    \n",
					"    # 9. Apply basic sanitization to name fields\n",
					"    name_columns = [\"fname\", \"lname\"]\n",
					"    for name_col in name_columns:\n",
					"        if name_col in df.columns:\n",
					"            df = df.withColumn(name_col, sanitize_non_address_field(col(name_col)))\n",
					"    \n",
					"    # 10. Remove records with empty name fields (critical data quality check)\n",
					"    if \"fname\" in df.columns and \"lname\" in df.columns:\n",
					"        df = df.filter(\n",
					"            col(\"fname\").isNotNull() & (trim(col(\"fname\")) != \"\") &\n",
					"            col(\"lname\").isNotNull() & (trim(col(\"lname\")) != \"\")\n",
					"        )\n",
					"        print(\"   ✅ Removed records with empty name fields\")\n",
					"    \n",
					"    # Check final record count after cleaning\n",
					"    final_count = df.count()\n",
					"    print(f\"   ✅ Final dataset contains {final_count} cleaned records (removed {record_count - final_count} problematic records)\")\n",
					"    \n",
					"    # Determine if we need to split into batches\n",
					"    need_batches = final_count > MAX_RECORDS_PER_BATCH\n",
					"    \n",
					"    # Generate CSV output(s)\n",
					"    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
					"    \n",
					"    if need_batches:\n",
					"        print(f\"   ⚠️ Dataset exceeds {MAX_RECORDS_PER_BATCH} records, splitting into batches\")\n",
					"        \n",
					"        # Calculate number of batches needed\n",
					"        num_batches = (final_count + MAX_RECORDS_PER_BATCH - 1) // MAX_RECORDS_PER_BATCH\n",
					"        \n",
					"        # Create a column for batching\n",
					"        df = df.withColumn(\"batch_id\", (F.monotonically_increasing_id() / MAX_RECORDS_PER_BATCH).cast(\"int\"))\n",
					"        \n",
					"        # Process each batch\n",
					"        for batch_num in range(num_batches):\n",
					"            batch_df = df.filter(col(\"batch_id\") == batch_num).drop(\"batch_id\")\n",
					"            batch_count = batch_df.count()\n",
					"            \n",
					"            print(f\"   📦 Processing batch {batch_num + 1}/{num_batches} with {batch_count} records\")\n",
					"            \n",
					"            # Write to temporary CSV folder\n",
					"            temp_output_dir = f\"{output_dir}/temp_batch{batch_num}_{timestamp}\"\n",
					"            \n",
					"            batch_df.coalesce(1).write \\\n",
					"                .option(\"header\", \"true\") \\\n",
					"                .option(\"quoteAll\", \"true\") \\\n",
					"                .option(\"escape\", \"\\\\\") \\\n",
					"                .option(\"quoteMode\", \"quoteMode.ALL\") \\\n",
					"                .option(\"lineSep\", \"\\n\") \\\n",
					"                .mode(\"overwrite\") \\\n",
					"                .csv(temp_output_dir)\n",
					"            \n",
					"            # Locate and rename the .csv file\n",
					"            files_written = mssparkutils.fs.ls(temp_output_dir)\n",
					"            for f in files_written:\n",
					"                if f.path.endswith(\".csv\") and \"part-\" in f.path:\n",
					"                    final_csv_path = f\"{output_dir}/voters_export_{timestamp}_batch{batch_num+1}.csv\"\n",
					"                    mssparkutils.fs.cp(f.path, final_csv_path)\n",
					"                    print(f\"   📄 Exported batch {batch_num + 1} to: {final_csv_path}\")\n",
					"                    break\n",
					"            \n",
					"            # Clean up temp folder\n",
					"            mssparkutils.fs.rm(temp_output_dir, recurse=True)\n",
					"    else:\n",
					"        print(f\"   📦 Processing entire dataset as a single file\")\n",
					"        \n",
					"        # Write to temporary CSV folder\n",
					"        temp_output_dir = f\"{output_dir}/temp_{timestamp}\"\n",
					"        \n",
					"        df.coalesce(1).write \\\n",
					"            .option(\"header\", \"true\") \\\n",
					"            .option(\"quoteAll\", \"true\") \\\n",
					"            .option(\"escape\", \"\\\\\") \\\n",
					"            .option(\"quoteMode\", \"quoteMode.ALL\") \\\n",
					"            .option(\"lineSep\", \"\\n\") \\\n",
					"            .mode(\"overwrite\") \\\n",
					"            .csv(temp_output_dir)\n",
					"        \n",
					"        # Locate and rename the .csv file\n",
					"        files_written = mssparkutils.fs.ls(temp_output_dir)\n",
					"        for f in files_written:\n",
					"            if f.path.endswith(\".csv\") and \"part-\" in f.path:\n",
					"                final_csv_path = f\"{output_dir}/voters_export_{timestamp}.csv\"\n",
					"                mssparkutils.fs.cp(f.path, final_csv_path)\n",
					"                print(f\"   📄 Exported to: {final_csv_path}\")\n",
					"                break\n",
					"        \n",
					"        # Clean up temp folder\n",
					"        mssparkutils.fs.rm(temp_output_dir, recurse=True)\n",
					"    \n",
					"    print(f\"✅ Export process completed successfully!\")\n",
					"\n",
					"except Exception as e:\n",
					"    print(f\"❌ Failed to process: {str(e)}\")\n",
					"    import traceback\n",
					"    print(traceback.format_exc())"
				],
				"execution_count": null
			}
		]
	}
}