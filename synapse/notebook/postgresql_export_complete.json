{
	"name": "postgresql_export_complete",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkprod",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "919cda79-73ca-4788-a01f-3879337875ff"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5ca62022-6aa2-4cee-aaa7-e7536c8d566c/resourceGroups/baubais-data-factory-rg-prod/providers/Microsoft.Synapse/workspaces/baubais-synapse-prod/bigDataPools/sparkprod",
				"name": "sparkprod",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-prod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkprod",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"#!/usr/bin/env python\n",
					"# coding: utf-8\n",
					"\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql.window import Window\n",
					"from notebookutils import mssparkutils\n",
					"import logging\n",
					"from datetime import datetime\n",
					"import time\n",
					"\n",
					""
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"source": [
					"# Configure logging\n",
					"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"logger = logging.getLogger()\n",
					"\n",
					"# Initialize Spark session with Delta Lake support\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"Voters Data Lake Export - PostgreSQL Schema\") \\\n",
					"    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
					"    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
					"    .getOrCreate()"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"source": [
					"def clean_data_before_transformation(df):\n",
					"    \"\"\"\n",
					"    Comprehensive data cleaning before PostgreSQL transformation\n",
					"    \"\"\"\n",
					"    logger.info(\"Starting comprehensive data cleaning\")\n",
					"    \n",
					"    initial_count = df.count()\n",
					"    logger.info(f\"Initial record count: {initial_count}\")\n",
					"    \n",
					"    # Step 0: Clean register_lett and poll_number fields (critical for data quality)\n",
					"    logger.info(\"Step 0: Cleaning register_lett and poll_number fields\")\n",
					"    \n",
					"    # Clean register_lett: strip whitespace and truncate to 5 chars\n",
					"    if \"register_lett\" in df.columns:\n",
					"        df = df.withColumn(\"register_lett\",\n",
					"            when(col(\"register_lett\").isNull(), lit(None))\n",
					"            .otherwise(substring(trim(col(\"register_lett\").cast(\"string\")), 1, 5))\n",
					"        )\n",
					"        logger.info(\"✅ Cleaned register_lett: stripped and truncated to 5 characters\")\n",
					"    \n",
					"    # Clean poll_number: remove .0 from floats, ensure numeric, truncate to 5 chars\n",
					"    if \"poll_number\" in df.columns:\n",
					"        df = df.withColumn(\"poll_number_temp\",\n",
					"            # First convert to string and remove .0 suffix if present\n",
					"            regexp_replace(col(\"poll_number\").cast(\"string\"), \"\\\\.0$\", \"\")\n",
					"        ).withColumn(\"poll_number_numeric\",\n",
					"            # Convert to numeric, coercing errors to null\n",
					"            col(\"poll_number_temp\").cast(\"double\")\n",
					"        ).withColumn(\"poll_number\",\n",
					"            # Convert back to string and truncate to 5 chars\n",
					"            when(col(\"poll_number_numeric\").isNull(), lit(None))\n",
					"            .otherwise(substring(col(\"poll_number_numeric\").cast(\"int\").cast(\"string\"), 1, 5))\n",
					"        ).drop(\"poll_number_temp\", \"poll_number_numeric\")\n",
					"        logger.info(\"✅ Cleaned poll_number: removed .0 suffix, ensured numeric, truncated to 5 characters\")\n",
					"    \n",
					"    # Step 0.5: Fix DOB format from DD/MM/YYYY to YYYY-MM-DD\n",
					"    logger.info(\"Step 0.5: Converting DOB from DD/MM/YYYY to YYYY-MM-DD format\")\n",
					"    if \"dob\" in df.columns:\n",
					"        df = df.withColumn(\"dob\",\n",
					"            when(col(\"dob\").isNull(), lit(None))\n",
					"            .when(trim(col(\"dob\")) == \"\", lit(None))\n",
					"            .when(upper(trim(col(\"dob\"))).isin([\"NULL\", \"NAN\", \"NA\"]), lit(None))\n",
					"            .otherwise(\n",
					"                # Try DD/MM/YYYY format first (most common in your data)\n",
					"                coalesce(\n",
					"                    to_date(col(\"dob\"), \"dd/MM/yyyy\"),\n",
					"                    to_date(col(\"dob\"), \"yyyy-MM-dd\"),\n",
					"                    to_date(col(\"dob\"), \"MM/dd/yyyy\"),\n",
					"                    to_date(col(\"dob\"), \"yyyy/MM/dd\"),\n",
					"                    to_date(col(\"dob\"), \"dd-MM-yyyy\"),\n",
					"                    to_date(col(\"dob\"), \"MM-dd-yyyy\")\n",
					"                )\n",
					"            )\n",
					"        )\n",
					"        logger.info(\"✅ Converted DOB to proper date format (will be YYYY-MM-DD in output)\")\n",
					"    \n",
					"    # Step 1: Remove records with NULL/empty lname or fname\n",
					"    logger.info(\"Step 1: Removing records with NULL/empty lname or fname\")\n",
					"    df = df.filter(\n",
					"        (col(\"lname\").isNotNull()) & \n",
					"        (trim(col(\"lname\")) != \"\") & \n",
					"        (upper(trim(col(\"lname\"))) != \"NULL\") &\n",
					"        (upper(trim(col(\"lname\"))) != \"NA\") &\n",
					"        (upper(trim(col(\"lname\"))) != \"NAN\") &\n",
					"        (col(\"fname\").isNotNull()) & \n",
					"        (trim(col(\"fname\")) != \"\") & \n",
					"        (upper(trim(col(\"fname\"))) != \"NULL\") &\n",
					"        (upper(trim(col(\"fname\"))) != \"NA\") &\n",
					"        (upper(trim(col(\"fname\"))) != \"NAN\")\n",
					"    )\n",
					"    \n",
					"    after_name_filter = df.count()\n",
					"    logger.info(f\"Records after name filtering: {after_name_filter}\")\n",
					"    \n",
					"    # Step 2: Ensure address is not null and not empty\n",
					"    logger.info(\"Step 2: Ensuring address is not null/empty\")\n",
					"    df = df.filter(\n",
					"        (col(\"address\").isNotNull()) & \n",
					"        (trim(col(\"address\")) != \"\") &\n",
					"        (upper(trim(col(\"address\"))) != \"NULL\") &\n",
					"        (upper(trim(col(\"address\"))) != \"NA\") &\n",
					"        (upper(trim(col(\"address\"))) != \"NAN\")\n",
					"    )\n",
					"    \n",
					"    after_address_filter = df.count()\n",
					"    logger.info(f\"Records after address filtering: {after_address_filter}\")\n",
					"    \n",
					"    # Step 3: Clean address4 - remove if it matches zip without spaces\n",
					"    logger.info(\"Step 3: Cleaning address4 - removing zip code duplicates\")\n",
					"    df = df.withColumn(\"address4\", \n",
					"        when(col(\"address4\") == regexp_replace(col(\"zip\"), \" \", \"\"), lit(None))\n",
					"        .otherwise(col(\"address4\"))\n",
					"    )\n",
					"    \n",
					"    # Step 4: Remove \"NAN\" values from address4\n",
					"    logger.info(\"Step 4: Removing 'NAN' values from address4\")\n",
					"    df = df.withColumn(\"address4\", \n",
					"        when(upper(trim(col(\"address4\"))) == \"NAN\", lit(None))\n",
					"        .otherwise(col(\"address4\"))\n",
					"    )\n",
					"    \n",
					"    # Step 5: Address4 cannot be empty - comprehensive address shifting\n",
					"    logger.info(\"Step 5: Ensuring address4 is not empty through comprehensive shifting\")\n",
					"    \n",
					"    # Shift address3 to address4 if address4 is null and address3 is not null\n",
					"    df = df.withColumn(\"address4\",\n",
					"        when(\n",
					"            (col(\"address4\").isNull()) & \n",
					"            (col(\"address3\").isNotNull()) &\n",
					"            (trim(col(\"address3\")) != \"\"),\n",
					"            col(\"address3\")\n",
					"        ).otherwise(col(\"address4\"))\n",
					"    ).withColumn(\"address3\",\n",
					"        when(\n",
					"            (col(\"address4\") == col(\"address3\")) & \n",
					"            (col(\"address4\").isNotNull()),\n",
					"            lit(None)\n",
					"        ).otherwise(col(\"address3\"))\n",
					"    )\n",
					"    \n",
					"    # Shift address2 to address4 if address4 is still null and address2 doesn't start with number\n",
					"    df = df.withColumn(\"address4\",\n",
					"        when(\n",
					"            (col(\"address4\").isNull()) & \n",
					"            (col(\"address2\").isNotNull()) &\n",
					"            (trim(col(\"address2\")) != \"\") &\n",
					"            (regexp_extract(trim(col(\"address2\")), \"^([0-9])\", 1) == \"\"),  # doesn't start with number\n",
					"            col(\"address2\")\n",
					"        ).otherwise(col(\"address4\"))\n",
					"    ).withColumn(\"address2\",\n",
					"        when(\n",
					"            (col(\"address4\") == col(\"address2\")) & \n",
					"            (col(\"address4\").isNotNull()),\n",
					"            lit(None)\n",
					"        ).otherwise(col(\"address2\"))\n",
					"    )\n",
					"    \n",
					"    # If address4 is still null, try address5 or address6\n",
					"    df = df.withColumn(\"address4\",\n",
					"        when(\n",
					"            (col(\"address4\").isNull()) & \n",
					"            (col(\"address5\").isNotNull()) &\n",
					"            (trim(col(\"address5\")) != \"\"),\n",
					"            col(\"address5\")\n",
					"        ).when(\n",
					"            (col(\"address4\").isNull()) & \n",
					"            (col(\"address6\").isNotNull()) &\n",
					"            (trim(col(\"address6\")) != \"\"),\n",
					"            col(\"address6\")\n",
					"        ).otherwise(col(\"address4\"))\n",
					"    )\n",
					"    \n",
					"    # Final cleanup - remove records that still have null address4\n",
					"    logger.info(\"Step 6: Final cleanup - removing records with null address4\")\n",
					"    before_final_filter = df.count()\n",
					"    after_final_filter = df.count()\n",
					"    logger.info(f\"Records after final address4 filtering: {after_final_filter} (removed: {before_final_filter - after_final_filter})\")\n",
					"    \n",
					"    # Clean other NAN/NULL values\n",
					"    logger.info(\"Step 7: Cleaning other NAN/NULL values\")\n",
					"    \n",
					"    # Clean address fields\n",
					"    for addr_col in [\"address\", \"address2\", \"address3\", \"address4\", \"address5\", \"address6\"]:\n",
					"        df = df.withColumn(addr_col,\n",
					"            when(upper(trim(col(addr_col))).isin([\"NAN\", \"NULL\", \"NA\", \"\"]), lit(None))\n",
					"            .otherwise(col(addr_col))\n",
					"        )\n",
					"    \n",
					"    # Clean other string fields\n",
					"    for str_col in [\"title\", \"new_marker\", \"flags\", \"perm_disqual\", \"source_id\"]:\n",
					"        if str_col in df.columns:\n",
					"            df = df.withColumn(str_col,\n",
					"                when(upper(trim(col(str_col))).isin([\"NAN\", \"NULL\", \"NA\", \"\"]), lit(None))\n",
					"                .otherwise(col(str_col))\n",
					"            )\n",
					"    \n",
					"    final_count = df.count()\n",
					"    logger.info(f\"Final record count after comprehensive cleaning: {final_count}\")\n",
					"    logger.info(f\"Total records removed: {initial_count - final_count}\")\n",
					"    \n",
					"    \n",
					"    # Clean address fields\n",
					"    for addr_col in [\"address\", \"address2\", \"address3\", \"address4\", \"address5\", \"address6\"]:\n",
					"        df = df.withColumn(addr_col,\n",
					"            when(upper(trim(col(addr_col))).isin([\"NAN\", \"NULL\", \"NA\", \"\"]), lit(None))\n",
					"            .otherwise(col(addr_col))\n",
					"        )\n",
					"    \n",
					"    # Clean other string fields\n",
					"    for str_col in [\"title\", \"new_marker\", \"flags\", \"perm_disqual\", \"source_id\"]:\n",
					"        if str_col in df.columns:\n",
					"            df = df.withColumn(str_col,\n",
					"                when(upper(trim(col(str_col))).isin([\"NAN\", \"NULL\", \"NA\", \"\"]), lit(None))\n",
					"                .otherwise(col(str_col))\n",
					"            )\n",
					"    \n",
					"    final_count = df.count()\n",
					"    logger.info(f\"Final record count after comprehensive cleaning: {final_count}\")\n",
					"    logger.info(f\"Total records removed: {initial_count - final_count}\")\n",
					"    \n",
					"    return df"
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"source": [
					"def transform_to_postgresql_schema(df):\n",
					"    \"\"\"\n",
					"    Transform DataFrame to match exact PostgreSQL juror_mod.voters schema\n",
					"    Note: part_no will be NULL here as it's generated by PostgreSQL sequence\n",
					"    \"\"\"\n",
					"    logger.info(\"Starting transformation to PostgreSQL schema\")\n",
					"    \n",
					"    # Apply comprehensive data cleaning first\n",
					"    df = clean_data_before_transformation(df)\n",
					"    \n",
					"    # Apply field length constraints and transformations\n",
					"    logger.info(\"Applying field length constraints\")\n",
					"    \n",
					"    df_constrained = df \\\n",
					"        .withColumn(\"part_no\", lit(None).cast(StringType())) \\\n",
					"        .withColumn(\"register_lett\", \n",
					"                   when(col(\"register_lett\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"register_lett\")), 1, 5))) \\\n",
					"        .withColumn(\"poll_number\", \n",
					"                   when(col(\"poll_number\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"poll_number\")), 1, 5))) \\\n",
					"        .withColumn(\"new_marker\", \n",
					"                   when(col(\"new_marker\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"new_marker\")), 1, 1))) \\\n",
					"        .withColumn(\"title\", \n",
					"                   when(col(\"title\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"title\")), 1, 10))) \\\n",
					"        .withColumn(\"lname\", \n",
					"                   substring(trim(upper(col(\"lname\"))), 1, 20)) \\\n",
					"        .withColumn(\"fname\", \n",
					"                   substring(trim(upper(col(\"fname\"))), 1, 20)) \\\n",
					"        .withColumn(\"flags\", \n",
					"                   when(col(\"flags\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(col(\"flags\")), 1, 2))) \\\n",
					"        .withColumn(\"address\", \n",
					"                   substring(trim(upper(col(\"address\"))), 1, 35)) \\\n",
					"        .withColumn(\"address2\", \n",
					"                   when(col(\"address2\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(upper(col(\"address2\"))), 1, 35))) \\\n",
					"        .withColumn(\"address3\",\n",
					"                   when(col(\"address3\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(upper(col(\"address3\"))), 1, 35))) \\\n",
					"        .withColumn(\"address4\",\n",
					"                   when(col(\"address4\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(upper(col(\"address4\"))), 1, 35))) \\\n",
					"        .withColumn(\"address5\",\n",
					"                   when(col(\"address5\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(upper(col(\"address5\"))), 1, 35))) \\\n",
					"        .withColumn(\"address6\", \n",
					"                   when(col(\"address6\").isNull(), lit(None))\n",
					"                   .when(trim(col(\"address6\")) == \"\", lit(None))\n",
					"                   .otherwise(lit(None))) \\\n",
					"        .withColumn(\"zip\", \n",
					"                   when(col(\"zip\").isNull(), lit(None))\n",
					"                   .otherwise(substring(trim(upper(col(\"zip\"))), 1, 10))) \\\n",
					"        .withColumn(\"perm_disqual\", \n",
					"                   when(col(\"perm_disqual\").isNull(), lit(None))\n",
					"                   .when(trim(col(\"perm_disqual\")) == \"\", lit(None))\n",
					"                   .otherwise(lit(None))) \\\n",
					"        .withColumn(\"source_id\", \n",
					"                   when(col(\"source_id\").isNull(), lit(None))\n",
					"                   .when(trim(col(\"source_id\")) == \"\", lit(None))\n",
					"                   .otherwise(lit(None)))\n",
					"    \n",
					"    logger.info(\"Field constraints applied successfully\")\n",
					"    \n",
					"    # Handle date fields - convert string dates to proper date type or NULL for PostgreSQL\n",
					"    logger.info(\"Converting date fields - ensuring empty strings become NULL\")\n",
					"    \n",
					"    # Function to properly handle date conversion for PostgreSQL (YYYY-MM-DD format)\n",
					"    def safe_date_conversion(date_col):\n",
					"        return when(\n",
					"            (col(date_col).isNull()) | \n",
					"            (trim(col(date_col)) == \"\") |\n",
					"            (upper(trim(col(date_col))).isin([\"NULL\", \"NAN\", \"NA\"])),\n",
					"            lit(None)\n",
					"        ).otherwise(\n",
					"            # Try multiple input formats but always convert to YYYY-MM-DD\n",
					"            coalesce(\n",
					"                to_date(col(date_col), \"yyyy-MM-dd\"),    # Already in correct format\n",
					"                to_date(col(date_col), \"dd/MM/yyyy\"),    # Convert from DD/MM/YYYY\n",
					"                to_date(col(date_col), \"MM/dd/yyyy\"),    # Convert from MM/DD/YYYY\n",
					"                to_date(col(date_col), \"yyyy/MM/dd\"),    # Convert from YYYY/MM/DD\n",
					"                to_date(col(date_col), \"dd-MM-yyyy\"),    # Convert from DD-MM-YYYY\n",
					"                to_date(col(date_col), \"MM-dd-yyyy\")     # Convert from MM-DD-YYYY\n",
					"            )\n",
					"        )\n",
					"    \n",
					"    df_with_dates = df_constrained \\\n",
					"        .withColumn(\"dob\", safe_date_conversion(\"dob\")) \\\n",
					"        .withColumn(\"date_selected1\", safe_date_conversion(\"date_selected1\")) \\\n",
					"        .withColumn(\"date_selected2\", safe_date_conversion(\"date_selected2\")) \\\n",
					"        .withColumn(\"date_selected3\", safe_date_conversion(\"date_selected3\"))\n",
					"    \n",
					"    # Set postcode_start to NULL (generated by PostgreSQL)\n",
					"    logger.info(\"Setting postcode_start to NULL - will be generated by PostgreSQL\")\n",
					"    \n",
					"    df_with_postcode = df_with_dates \\\n",
					"        .withColumn(\"postcode_start\", lit(None).cast(StringType()))\n",
					"    \n",
					"    # Ensure rec_num is integer type\n",
					"    df_with_rec_num = df_with_postcode \\\n",
					"        .withColumn(\"rec_num\", \n",
					"                   when(col(\"rec_num\").isNull(), lit(None).cast(IntegerType()))\n",
					"                   .otherwise(col(\"rec_num\").cast(IntegerType())))\n",
					"    \n",
					"    # Ensure creation_date is properly formatted\n",
					"    df_with_creation_date = df_with_rec_num \\\n",
					"        .withColumn(\"creation_date\", \n",
					"                   when(col(\"creation_date\").isNull(), lit(None))\n",
					"                   .otherwise(col(\"creation_date\").cast(StringType())))\n",
					"    \n",
					"    # Select only the required columns in the correct order\n",
					"    final_columns = [\n",
					"        \"part_no\", \"register_lett\", \"poll_number\", \"new_marker\", \"title\",\n",
					"        \"lname\", \"fname\", \"dob\", \"flags\", \"address\", \"address2\", \"address3\",\n",
					"        \"address4\", \"address5\", \"address6\", \"zip\", \"date_selected1\",\n",
					"        \"date_selected2\", \"date_selected3\", \"rec_num\", \"perm_disqual\",\n",
					"        \"source_id\", \"postcode_start\", \"creation_date\", \"hash_id\"\n",
					"    ]\n",
					"    \n",
					"    df_final = df_with_creation_date.select(*final_columns)\n",
					"    \n",
					"    # Final validation - check for any remaining NULL violations\n",
					"    logger.info(\"Performing final validation\")\n",
					"    \n",
					"    null_lname = df_final.filter(col(\"lname\").isNull()).count()\n",
					"    null_fname = df_final.filter(col(\"fname\").isNull()).count()\n",
					"    null_address = df_final.filter(col(\"address\").isNull()).count()\n",
					"    \n",
					"    if null_lname > 0 or null_fname > 0 or null_address > 0:\n",
					"        logger.warning(f\"NULL constraint violations found: lname={null_lname}, fname={null_fname}, address={null_address}\")\n",
					"        \n",
					"        # Remove records with NULL violations\n",
					"        df_final = df_final.filter(\n",
					"            col(\"lname\").isNotNull() &\n",
					"            col(\"fname\").isNotNull() &\n",
					"            col(\"address\").isNotNull()\n",
					"        )\n",
					"    \n",
					"    # Check hash_id uniqueness for deduplication verification\n",
					"    unique_hash_ids = df_final.select(\"hash_id\").distinct().count()\n",
					"    total_records = df_final.count()\n",
					"    \n",
					"    if unique_hash_ids != total_records:\n",
					"        logger.warning(f\"Duplicate hash_ids found: {total_records - unique_hash_ids} duplicates\")\n",
					"        # Remove duplicates keeping the first occurrence\n",
					"        window_spec = Window.partitionBy(\"hash_id\").orderBy(col(\"hash_id\"))\n",
					"        df_final = df_final.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
					"                          .filter(col(\"row_num\") == 1) \\\n",
					"                          .drop(\"row_num\")\n",
					"    \n",
					"    logger.info(f\"Final record count after PostgreSQL schema transformation: {df_final.count()}\")\n",
					"    \n",
					"    return df_final"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"source": [
					"def create_postgresql_compatible_csv(df, output_path):\n",
					"    \"\"\"\n",
					"    Create a PostgreSQL-compatible CSV export with proper NULL handling\n",
					"    All dates will be in YYYY-MM-DD format as required by PostgreSQL\n",
					"    \"\"\"\n",
					"    logger.info(\"Creating PostgreSQL-compatible CSV export with YYYY-MM-DD date format\")\n",
					"    \n",
					"    # Convert all date columns to YYYY-MM-DD string format, ensuring NULLs are properly handled\n",
					"    csv_ready_df = df \\\n",
					"        .withColumn(\"dob\", \n",
					"                   when(col(\"dob\").isNull(), lit(None))\n",
					"                   .otherwise(date_format(col(\"dob\"), \"yyyy-MM-dd\"))) \\\n",
					"        .withColumn(\"date_selected1\", \n",
					"                   when(col(\"date_selected1\").isNull(), lit(None))\n",
					"                   .otherwise(date_format(col(\"date_selected1\"), \"yyyy-MM-dd\"))) \\\n",
					"        .withColumn(\"date_selected2\", \n",
					"                   when(col(\"date_selected2\").isNull(), lit(None))\n",
					"                   .otherwise(date_format(col(\"date_selected2\"), \"yyyy-MM-dd\"))) \\\n",
					"        .withColumn(\"date_selected3\", \n",
					"                   when(col(\"date_selected3\").isNull(), lit(None))\n",
					"                   .otherwise(date_format(col(\"date_selected3\"), \"yyyy-MM-dd\")))\n",
					"    \n",
					"    # Ensure all string fields have proper NULL handling\n",
					"    string_columns = [\"part_no\", \"register_lett\", \"poll_number\", \"new_marker\", \"title\",\n",
					"                     \"lname\", \"fname\", \"flags\", \"address\", \"address2\", \"address3\",\n",
					"                     \"address4\", \"address5\", \"address6\", \"zip\", \"perm_disqual\",\n",
					"                     \"source_id\", \"postcode_start\", \"creation_date\", \"hash_id\"]\n",
					"    \n",
					"    for col_name in string_columns:\n",
					"        if col_name in csv_ready_df.columns:\n",
					"            csv_ready_df = csv_ready_df.withColumn(col_name,\n",
					"                when(col(col_name).isNull(), lit(None))\n",
					"                .when(trim(col(col_name)) == \"\", lit(None))\n",
					"                .otherwise(col(col_name))\n",
					"            )\n",
					"    \n",
					"    # Log a sample of date formatting for verification\n",
					"    logger.info(\"Sample date formatting verification:\")\n",
					"    sample_dates = csv_ready_df.select(\"dob\", \"date_selected1\").limit(5).collect()\n",
					"    for i, row in enumerate(sample_dates):\n",
					"        logger.info(f\"  Row {i+1}: dob={row.dob}, date_selected1={row.date_selected1}\")\n",
					"    \n",
					"    # Write CSV with PostgreSQL-compatible options\n",
					"    csv_ready_df.coalesce(1).write \\\n",
					"        .option(\"header\", \"true\") \\\n",
					"        .option(\"nullValue\", \"\") \\\n",
					"        .option(\"emptyValue\", \"\") \\\n",
					"        .option(\"quote\", '\"') \\\n",
					"        .option(\"escape\", '\"') \\\n",
					"        .option(\"quoteAll\", \"false\") \\\n",
					"        .mode(\"overwrite\") \\\n",
					"        .csv(output_path)\n",
					"    \n",
					"    logger.info(\"✅ All dates exported in YYYY-MM-DD format for PostgreSQL compatibility\")\n",
					"    return output_path\n",
					""
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"source": [
					"def export_voters_to_postgresql_schema():\n",
					"    \"\"\"\n",
					"    Main function to export voters data with PostgreSQL schema compliance\n",
					"    \"\"\"\n",
					"    try:\n",
					"        # Initialize storage paths\n",
					"        storage_account = \"baubaisadfsaprod\"\n",
					"        source_path = f\"abfss://dl-juror-eric-voters-temp@{storage_account}.dfs.core.windows.net/voters_deduplicated_delta\"\n",
					"        output_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/voters_postgresql\"\n",
					"        \n",
					"        # Read the existing Delta table\n",
					"        logger.info(f\"Reading voter data from {source_path}\")\n",
					"        source_df = spark.read.format(\"delta\").load(source_path)\n",
					"        \n",
					"        initial_count = source_df.count()\n",
					"        logger.info(f\"Initial Delta table record count: {initial_count}\")\n",
					"        \n",
					"        # Check if source has data\n",
					"        if initial_count == 0:\n",
					"            logger.error(\"❌ No data found in source Delta table\")\n",
					"            return None, None\n",
					"        \n",
					"        # Filter for specific batch date (hardcoded for batch processing)\n",
					"        target_dates = [\"20241213\", \"20241216\"]  # Update this for your batch date\n",
					"        logger.info(f\"Filtering for creation_date = {target_dates}\")\n",
					"        \n",
					"        # Check what creation_dates are available first\n",
					"        logger.info(\"Checking available creation_dates...\")\n",
					"        available_dates = source_df.select(\"creation_date\").distinct().orderBy(\"creation_date\").collect()\n",
					"        logger.info(\"Available creation_dates:\")\n",
					"        for date_row in available_dates[:10]:  # Show first 10\n",
					"            count_for_date = source_df.filter(col(\"creation_date\") == lit(date_row.creation_date)).count()\n",
					"            logger.info(f\"   - {date_row.creation_date}: {count_for_date} records\")\n",
					"        \n",
					"        # Apply the filter\n",
					"        filtered_df = source_df.filter(col(\"creation_date\").isin(target_dates))\n",
					"        filtered_count = filtered_df.count()\n",
					"        \n",
					"        logger.info(f\"Records with creation_date = {target_dates}: {filtered_count}\")\n",
					"        \n",
					"        if filtered_count == 0:\n",
					"            logger.error(f\"❌ No records found for creation_date = {target_dates}\")\n",
					"            logger.error(\"Please check the available creation_dates above and update the target_dates\")\n",
					"            return None, None\n",
					"        \n",
					"        # Apply PostgreSQL schema transformation\n",
					"        logger.info(\"Applying PostgreSQL schema transformation\")\n",
					"        postgresql_df = transform_to_postgresql_schema(filtered_df)\n",
					"        \n",
					"        if postgresql_df is None:\n",
					"            logger.error(\"❌ PostgreSQL schema transformation failed\")\n",
					"            return None, None\n",
					"        \n",
					"        final_count = postgresql_df.count()\n",
					"        logger.info(f\"Final PostgreSQL schema record count: {final_count}\")\n",
					"        logger.info(f\"Records removed during transformation: {filtered_count - final_count}\")\n",
					"        \n",
					"        if final_count == 0:\n",
					"            logger.error(\"❌ No records remaining after transformation\")\n",
					"            return None, None\n",
					"        \n",
					"        # Write to output path (Parquet format)\n",
					"        logger.info(f\"Writing PostgreSQL-ready data to {output_path}\")\n",
					"        try:\n",
					"            postgresql_df.write \\\n",
					"                .mode(\"overwrite\") \\\n",
					"                .option(\"overwriteSchema\", \"true\") \\\n",
					"                .parquet(output_path)\n",
					"            logger.info(\"✅ Parquet export completed successfully\")\n",
					"        except Exception as e:\n",
					"            logger.error(f\"❌ Failed to write Parquet: {str(e)}\")\n",
					"            return postgresql_df, None\n",
					"        \n",
					"        # Create a PostgreSQL-compatible CSV for import\n",
					"        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
					"        csv_output_path = f\"abfss://juror-etl@{storage_account}.dfs.core.windows.net/exports/postgresql_import_{timestamp}\"\n",
					"        \n",
					"        # Create PostgreSQL-compatible CSV\n",
					"        logger.info(f\"Creating PostgreSQL-compatible CSV export at {csv_output_path}\")\n",
					"        try:\n",
					"            csv_path = create_postgresql_compatible_csv(postgresql_df, csv_output_path)\n",
					"            logger.info(\"✅ CSV export initiated\")\n",
					"        except Exception as e:\n",
					"            logger.error(f\"❌ CSV export failed: {str(e)}\")\n",
					"            return postgresql_df, None\n",
					"        \n",
					"        # Verify the CSV export\n",
					"        try:\n",
					"            if mssparkutils.fs.exists(csv_output_path):\n",
					"                logger.info(f\"✅ CSV directory created successfully: {csv_output_path}\")\n",
					"                \n",
					"                # Find the actual CSV file within the directory\n",
					"                files = mssparkutils.fs.ls(csv_output_path)\n",
					"                csv_file_path = None\n",
					"                \n",
					"                for file in files:\n",
					"                    if file.name.endswith('.csv') and 'part-' in file.name:\n",
					"                        csv_file_path = file.path\n",
					"                        break\n",
					"                \n",
					"                if csv_file_path:\n",
					"                    logger.info(f\"✅ CSV file found: {csv_file_path}\")\n",
					"                    logger.info(f\"📊 Final export statistics:\")\n",
					"                    logger.info(f\"   - Initial records: {initial_count}\")\n",
					"                    logger.info(f\"   - After creation_date filtering: {filtered_count}\")\n",
					"                    logger.info(f\"   - Final exported records: {final_count}\")\n",
					"                    logger.info(f\"   - Records cleaned/removed: {filtered_count - final_count}\")\n",
					"                    logger.info(f\"   - Data loss percentage: {((filtered_count - final_count) / filtered_count * 100):.2f}%\")\n",
					"                else:\n",
					"                    logger.warning(\"⚠️ CSV directory created but no CSV file found inside\")\n",
					"                    # List all files for debugging\n",
					"                    logger.info(\"Files in CSV directory:\")\n",
					"                    for file in files:\n",
					"                        logger.info(f\"   - {file.name}\")\n",
					"                    csv_file_path = csv_output_path  # Return directory path as fallback\n",
					"            else:\n",
					"                logger.error(\"❌ CSV export failed - output directory not created\")\n",
					"                csv_file_path = None\n",
					"                \n",
					"        except Exception as e:\n",
					"            logger.error(f\"❌ Error verifying CSV export: {str(e)}\")\n",
					"            csv_file_path = None\n",
					"        \n",
					"        logger.info(\"🎉 PostgreSQL export pipeline completed successfully\")\n",
					"        return postgresql_df, csv_file_path\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"❌ Error in PostgreSQL export process: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(\"Full traceback:\")\n",
					"        logger.error(traceback.format_exc())\n",
					"        return None, None\n",
					"\n",
					"def check_data_availability():\n",
					"    \"\"\"\n",
					"    Quick function to check what data is available before running the full export\n",
					"    \"\"\"\n",
					"    try:\n",
					"        storage_account = \"baubaisadfsaprod\"\n",
					"        source_path = f\"abfss://dl-juror-eric-voters-temp@{storage_account}.dfs.core.windows.net/voters_deduplicated_delta\"\n",
					"        \n",
					"        logger.info(f\"Checking data availability in {source_path}\")\n",
					"        \n",
					"        # Check if path exists\n",
					"        if not mssparkutils.fs.exists(source_path):\n",
					"            logger.error(f\"❌ Source path does not exist: {source_path}\")\n",
					"            return False\n",
					"        \n",
					"        source_df = spark.read.format(\"delta\").load(source_path)\n",
					"        \n",
					"        total_count = source_df.count()\n",
					"        logger.info(f\"✅ Total records in Delta table: {total_count}\")\n",
					"        \n",
					"        if total_count == 0:\n",
					"            logger.warning(\"⚠️ Delta table exists but contains no records\")\n",
					"            return False\n",
					"        \n",
					"        # Check creation_dates\n",
					"        logger.info(\"📅 Available creation_dates:\")\n",
					"        creation_dates = source_df.select(\"creation_date\").distinct().orderBy(\"creation_date\").collect()\n",
					"        \n",
					"        for date_row in creation_dates:\n",
					"            count_for_date = source_df.filter(col(\"creation_date\") == lit(date_row.creation_date)).count()\n",
					"            logger.info(f\"   - {date_row.creation_date}: {count_for_date:,} records\")\n",
					"        \n",
					"        # Check sample data structure\n",
					"        logger.info(\"📋 Sample data columns:\")\n",
					"        for col_name in source_df.columns[:10]:  # Show first 10 columns\n",
					"            logger.info(f\"   - {col_name}\")\n",
					"        \n",
					"        if len(source_df.columns) > 10:\n",
					"            logger.info(f\"   ... and {len(source_df.columns) - 10} more columns\")\n",
					"            \n",
					"        return True\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"❌ Error checking data: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        return False\n",
					"\n",
					"# Execute the export with enhanced functionality\n",
					"if __name__ == \"__main__\":\n",
					"    logger.info(\"🚀 Starting Enhanced PostgreSQL Export Pipeline\")\n",
					"    \n",
					"    # First check data availability\n",
					"    logger.info(\"Step 1: Checking data availability...\")\n",
					"    if check_data_availability():\n",
					"        logger.info(\"✅ Data availability check passed\")\n",
					"        \n",
					"        # Run the export\n",
					"        logger.info(\"Step 2: Running PostgreSQL export...\")\n",
					"        result_df, csv_path = export_voters_to_postgresql_schema()\n",
					"        \n",
					"        if result_df is not None and csv_path is not None:\n",
					"            logger.info(\"🎉 Enhanced PostgreSQL export pipeline completed successfully\")\n",
					"            logger.info(f\"📁 CSV export path: {csv_path}\")\n",
					"            logger.info(f\"📊 Total records exported: {result_df.count():,}\")\n",
					"            \n",
					"            # Show sample of final data\n",
					"            logger.info(\"📋 Sample of exported data:\")\n",
					"            sample_data = result_df.limit(3).collect()\n",
					"            for i, row in enumerate(sample_data):\n",
					"                logger.info(f\"   Row {i+1}: lname={row.lname}, fname={row.fname}, address={row.address}, address4={row.address4}\")\n",
					"                \n",
					"        else:\n",
					"            logger.error(\"❌ Export pipeline failed\")\n",
					"            if result_df is None:\n",
					"                logger.error(\"   - DataFrame transformation failed\")\n",
					"            if csv_path is None:\n",
					"                logger.error(\"   - CSV export failed\")\n",
					"    else:\n",
					"        logger.error(\"❌ Data availability check failed - cannot proceed with export\")"
				],
				"execution_count": 30
			}
		]
	}
}