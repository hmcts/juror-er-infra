{
	"name": "potgresql_batch",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkstg",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b10c6076-e22d-4ef6-83d6-95581c3636ab"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/74dacd4f-a248-45bb-a2f0-af700dc4cf68/resourceGroups/baubais-data-factory-rg-stg/providers/Microsoft.Synapse/workspaces/baubais-synapse-stg/bigDataPools/sparkstg",
				"name": "sparkstg",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-stg.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkstg",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"import logging\n",
					"import time\n",
					"import os\n",
					"from notebookutils import mssparkutils\n",
					"import tempfile"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Configure logging\n",
					"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
					"logger = logging.getLogger()\n",
					"\n",
					"# Initialize Spark session (should already be available in Synapse notebooks)\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"CSV Export for PostgreSQL\") \\\n",
					"    .getOrCreate()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"def export_to_csv_and_upload(source_path, filter_date=None, destination_container=\"postgresql-batch\"):\n",
					"    \"\"\"\n",
					"    Exports data from Parquet to CSV and uploads to the destination container\n",
					"    \n",
					"    Args:\n",
					"        source_path: Path to the source Parquet data\n",
					"        filter_date: Optional date to filter the data (format: YYYYMMDD)\n",
					"        destination_container: Container to upload the CSV file\n",
					"        destination_folder: Folder inside the container to store the CSV- no needed\n",
					"    \n",
					"    Returns:\n",
					"        Path to the uploaded CSV file\n",
					"    \"\"\"\n",
					"    start_time = time.time()\n",
					"    \n",
					"    try:\n",
					"        logger.info(f\"Reading data from {source_path}\")\n",
					"        # Read the source data\n",
					"        df = spark.read.parquet(source_path)\n",
					"        \n",
					"        # Apply filter if specified\n",
					"        if filter_date:\n",
					"            logger.info(f\"Filtering data for creation_date = {filter_date}\")\n",
					"            df = df.filter(col(\"creation_date\") == filter_date)\n",
					"        \n",
					"        # Get record count for logging\n",
					"        record_count = df.count()\n",
					"        logger.info(f\"Processing {record_count} records\")\n",
					"        \n",
					"        # Optimize performance: select only needed columns and coalesce to reduce partitions\n",
					"        # This is optional - you can include only the columns you need for PostgreSQL\n",
					"        # df = df.select(\"col1\", \"col2\", \"col3\")  # Uncomment and specify columns if needed\n",
					"        \n",
					"        # Coalesce to fewer partitions to create fewer (or a single) CSV file\n",
					"        # For smaller datasets (<1GB), a single partition works well\n",
					"        if record_count < 10000000:  # Adjust threshold based on your data size\n",
					"            df = df.coalesce(1)\n",
					"        else:\n",
					"            # For larger datasets, use multiple partitions\n",
					"            df = df.coalesce(4)  # Adjust number based on your cluster capacity\n",
					"        \n",
					"        # Create a temporary directory for CSV output\n",
					"        with tempfile.TemporaryDirectory() as temp_dir:\n",
					"            # Create a local path for the CSV file\n",
					"            local_csv_path = os.path.join(temp_dir, \"export_data.csv\")\n",
					"            \n",
					"            logger.info(f\"Writing data to CSV at {local_csv_path}\")\n",
					"            \n",
					"            # Convert to Pandas and save as CSV (for small to medium datasets)\n",
					"            # This approach is simpler but works best for smaller datasets\n",
					"            try:\n",
					"                pandas_df = df.toPandas()\n",
					"                pandas_df.to_csv(local_csv_path, index=False)\n",
					"                logger.info(f\"Successfully wrote data to CSV using pandas\")\n",
					"            except Exception as e:\n",
					"                logger.error(f\"Error using pandas method: {str(e)}\")\n",
					"                logger.info(\"Falling back to Spark CSV writer\")\n",
					"                \n",
					"                # Alternative approach using Spark's CSV writer\n",
					"                # Better for larger datasets but creates a directory with part files\n",
					"                spark_csv_dir = os.path.join(temp_dir, \"spark_csv\")\n",
					"                df.write.option(\"header\", \"true\").csv(spark_csv_dir)\n",
					"                \n",
					"                # Find the part file (there should be only one if coalesce(1) was used)\n",
					"                part_files = [f for f in os.listdir(spark_csv_dir) if f.startswith(\"part-\")]\n",
					"                if not part_files:\n",
					"                    raise Exception(\"No CSV part files were generated\")\n",
					"                \n",
					"                # Use the first part file\n",
					"                local_csv_path = os.path.join(spark_csv_dir, part_files[0])\n",
					"                logger.info(f\"Using Spark CSV at {local_csv_path}\")\n",
					"            \n",
					"            # Prepare destination path\n",
					"            timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
					"            file_name = f\"export_{timestamp}.csv\"\n",
					"            if filter_date:\n",
					"                file_name = f\"export_{filter_date}_{timestamp}.csv\"\n",
					"                \n",
					"            storage_account = \"baubaisadfsastg\"  # This should match your environment\n",
					"            destination_path = f\"abfss://{destination_container}@{storage_account}.dfs.core.windows.net/{file_name}\"\n",
					"            \n",
					"            logger.info(f\"Uploading CSV to {destination_path}\")\n",
					"            \n",
					"            # Ensure destination directory exists\n",
					"            dest_dir = f\"abfss://{destination_container}@{storage_account}.dfs.core.windows.net\"\n",
					"            if not mssparkutils.fs.exists(dest_dir):\n",
					"                logger.info(f\"Creating destination directory: {dest_dir}\")\n",
					"                mssparkutils.fs.mkdirs(dest_dir)\n",
					"            \n",
					"            # Copy the file to the destination\n",
					"            mssparkutils.fs.cp(f\"file://{local_csv_path}\", destination_path, True)\n",
					"            \n",
					"            # Verify the file was uploaded\n",
					"            if mssparkutils.fs.exists(destination_path):\n",
					"                logger.info(f\"Successfully uploaded CSV to {destination_path}\")\n",
					"                logger.info(f\"Total processing time: {time.time() - start_time:.2f} seconds\")\n",
					"                return destination_path\n",
					"            else:\n",
					"                raise Exception(f\"Failed to upload CSV to {destination_path}\")\n",
					"                \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error exporting to CSV: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        raise\n",
					"\n",
					"# Execute the export function\n",
					"try:\n",
					"    # Example usage with filter\n",
					"    source_path = \"abfss://dl-juror-eric-voters-temp@baubaisadfsastg.dfs.core.windows.net/2025-03-24\"\n",
					"    filter_date = \"20231205\"  # Optional filter\n",
					"    \n",
					"    # Call the function\n",
					"    result_path = export_to_csv_and_upload(source_path, filter_date)\n",
					"    print(f\"CSV export completed successfully!\")\n",
					"    print(f\"CSV file available at: {result_path}\")\n",
					"except Exception as e:\n",
					"    print(f\"Export failed: {str(e)}\")"
				],
				"execution_count": null
			}
		]
	}
}