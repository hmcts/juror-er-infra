{
	"name": "validation_handlers",
	"properties": {
		"folder": {
			"name": "Configuration"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkstg",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "ef0314c6-30bb-49f9-b897-612e5b0cf3c2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/74dacd4f-a248-45bb-a2f0-af700dc4cf68/resourceGroups/baubais-data-factory-rg-stg/providers/Microsoft.Synapse/workspaces/baubais-synapse-stg/bigDataPools/sparkstg",
				"name": "sparkstg",
				"type": "Spark",
				"endpoint": "https://baubais-synapse-stg.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkstg",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# validation_handlers.py\n",
					"\"\"\"\n",
					"Implementation of validation rules defined in validation_rules.py\n",
					"\"\"\"\n",
					"import logging\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql.functions import col, lit, regexp_extract, when, upper, length, substring, current_timestamp, date_format\n",
					"from pyspark.sql.types import StringType, IntegerType, TimestampType, StructType, StructField\n",
					"\n",
					"# Import validation rules\n",
					"from validation_rules import POSTCODE_SETTINGS, NULL_COLUMNS, COLUMN_MAPPINGS, TARGET_SCHEMA\n",
					"\n",
					"def process_postcodes(df: DataFrame) -> DataFrame:\n",
					"    \"\"\"\n",
					"    Standardize and validate postcodes based on configuration rules.\n",
					"    \n",
					"    Args:\n",
					"        df: Input DataFrame\n",
					"        \n",
					"    Returns:\n",
					"        DataFrame with processed postcodes and filtered invalid rows if configured\n",
					"    \"\"\"\n",
					"    logger = logging.getLogger()\n",
					"    \n",
					"    # Determine which postcode column exists\n",
					"    postcode_col = None\n",
					"    for col_name in POSTCODE_SETTINGS[\"possible_columns\"]:\n",
					"        if col_name in df.columns:\n",
					"            postcode_col = col_name\n",
					"            break\n",
					"    \n",
					"    if not postcode_col:\n",
					"        logger.warning(\"No postcode column found in DataFrame\")\n",
					"        return df\n",
					"    \n",
					"    # Store original row count\n",
					"    original_count = df.count()\n",
					"    logger.info(f\"Processing {original_count} records for postcode validation\")\n",
					"    \n",
					"    # Standardize postcodes\n",
					"    df = df.withColumn(\n",
					"        postcode_col, \n",
					"        when(\n",
					"            col(postcode_col).isNotNull() & (length(col(postcode_col)) > 0),\n",
					"            regexp_extract(\n",
					"                upper(col(postcode_col)),\n",
					"                POSTCODE_SETTINGS[\"extraction_pattern\"],\n",
					"                POSTCODE_SETTINGS[\"format_template\"]\n",
					"            )\n",
					"        ).otherwise(None)\n",
					"    )\n",
					"    \n",
					"    # Extract postcode_start for routing\n",
					"    df = df.withColumn(\n",
					"        \"postcode_start\",\n",
					"        when(\n",
					"            col(postcode_col).isNotNull() & (length(col(postcode_col)) > 0),\n",
					"            regexp_extract(\n",
					"                col(postcode_col), \n",
					"                POSTCODE_SETTINGS[\"postcode_start_pattern\"],\n",
					"                1\n",
					"            )\n",
					"        ).otherwise(None)\n",
					"    )\n",
					"    \n",
					"    # Filter out invalid postcodes if configured\n",
					"    if POSTCODE_SETTINGS[\"remove_invalid\"]:\n",
					"        pattern = POSTCODE_SETTINGS[\"validation_pattern\"]\n",
					"        df = df.filter(\n",
					"            col(postcode_col).rlike(pattern) | \n",
					"            col(postcode_col).isNull() | \n",
					"            (length(col(postcode_col)) == 0)\n",
					"        )\n",
					"        \n",
					"        # Log results\n",
					"        valid_count = df.count()\n",
					"        removed = original_count - valid_count\n",
					"        \n",
					"        logger.info(f\"Postcode validation removed {removed} records with invalid postcodes\")\n",
					"        logger.info(f\"Remaining valid records: {valid_count}\")\n",
					"    \n",
					"    return df\n",
					"\n",
					"def enforce_null_columns(df: DataFrame) -> DataFrame:\n",
					"    \"\"\"\n",
					"    Ensure specified columns have NULL values, regardless of their current values.\n",
					"    \n",
					"    Args:\n",
					"        df: Input DataFrame\n",
					"        \n",
					"    Returns:\n",
					"        DataFrame with NULL values in specified columns\n",
					"    \"\"\"\n",
					"    for col_name in NULL_COLUMNS:\n",
					"        if col_name in df.columns:\n",
					"            df = df.withColumn(col_name, lit(None))\n",
					"    \n",
					"    return df\n",
					"\n",
					"def enforce_column_lengths(df: DataFrame) -> DataFrame:\n",
					"    \"\"\"\n",
					"    Ensure columns don't exceed their maximum configured length.\n",
					"    \n",
					"    Args:\n",
					"        df: Input DataFrame\n",
					"        \n",
					"    Returns:\n",
					"        DataFrame with truncated string values where needed\n",
					"    \"\"\"\n",
					"    for field in TARGET_SCHEMA:\n",
					"        col_name = field[\"name\"]\n",
					"        if col_name in df.columns and \"max_length\" in field and field[\"type\"] == \"string\":\n",
					"            max_length = field[\"max_length\"]\n",
					"            df = df.withColumn(\n",
					"                col_name,\n",
					"                when(\n",
					"                    col(col_name).isNotNull() & (length(col(col_name)) > max_length),\n",
					"                    substring(col(col_name), 1, max_length)\n",
					"                ).otherwise(col(col_name))\n",
					"            )\n",
					"    \n",
					"    return df\n",
					"\n",
					"def create_target_schema_df(df: DataFrame) -> DataFrame:\n",
					"    \"\"\"\n",
					"    Transform DataFrame to match target schema from configuration.\n",
					"    \n",
					"    Args:\n",
					"        df: Input DataFrame\n",
					"        \n",
					"    Returns:\n",
					"        DataFrame with standardized columns according to TARGET_SCHEMA\n",
					"    \"\"\"\n",
					"    logger = logging.getLogger()\n",
					"    \n",
					"    try:\n",
					"        # Helper function to safely access columns\n",
					"        def safe_col(column_name):\n",
					"            return col(column_name) if column_name in df.columns else lit(None)\n",
					"        \n",
					"        # Map source columns to target columns\n",
					"        select_expressions = []\n",
					"        \n",
					"        for field in TARGET_SCHEMA:\n",
					"            target_name = field[\"name\"]\n",
					"            \n",
					"            # Handle special cases\n",
					"            if target_name == \"part_no\":\n",
					"                # Always null\n",
					"                select_expressions.append(lit(None).cast(\"string\").alias(target_name))\n",
					"                \n",
					"            elif target_name in [\"date_selected1\", \"date_selected2\", \"date_selected3\"]:\n",
					"                # Always null\n",
					"                select_expressions.append(lit(None).cast(\"timestamp\").alias(target_name))\n",
					"                \n",
					"            elif target_name in [\"register_lett\", \"poll_number\"]:\n",
					"                # Both come from Elector Number\n",
					"                select_expressions.append(\n",
					"                    when(\n",
					"                        safe_col(\"Elector Number\").isNotNull(),\n",
					"                        safe_col(\"Elector Number\").cast(\"int\")\n",
					"                    ).otherwise(lit(None)).alias(target_name)\n",
					"                )\n",
					"                \n",
					"            elif target_name == \"rec_num\":\n",
					"                # No longer using LA_Code\n",
					"                select_expressions.append(lit(None).cast(\"int\").alias(target_name))\n",
					"                \n",
					"            elif target_name == \"creation_date\":\n",
					"                # Use CreationDate if available, otherwise current date\n",
					"                select_expressions.append(\n",
					"                    when(\n",
					"                        safe_col(\"CreationDate\").isNotNull(),\n",
					"                        safe_col(\"CreationDate\")\n",
					"                    ).otherwise(date_format(current_timestamp(), \"yyyyMMdd\")).alias(target_name)\n",
					"                )\n",
					"                \n",
					"            elif target_name == \"postcode_start\":\n",
					"                # Already calculated in process_postcodes\n",
					"                select_expressions.append(safe_col(\"postcode_start\"))\n",
					"                \n",
					"            else:\n",
					"                # Standard column mapping\n",
					"                found = False\n",
					"                for source_col, mapped_col in COLUMN_MAPPINGS.items():\n",
					"                    if mapped_col == target_name and source_col in df.columns:\n",
					"                        select_expressions.append(safe_col(source_col).alias(target_name))\n",
					"                        found = True\n",
					"                        break\n",
					"                \n",
					"                # If no mapping found, add a NULL column\n",
					"                if not found:\n",
					"                    if field[\"type\"] == \"string\":\n",
					"                        select_expressions.append(lit(None).cast(\"string\").alias(target_name))\n",
					"                    elif field[\"type\"] == \"integer\":\n",
					"                        select_expressions.append(lit(None).cast(\"int\").alias(target_name))\n",
					"                    elif field[\"type\"] == \"timestamp\":\n",
					"                        select_expressions.append(lit(None).cast(\"timestamp\").alias(target_name))\n",
					"                    else:\n",
					"                        select_expressions.append(lit(None).alias(target_name))\n",
					"        \n",
					"        # Create DataFrame with all expressions\n",
					"        result_df = df.select(*select_expressions)\n",
					"        return result_df\n",
					"        \n",
					"    except Exception as e:\n",
					"        logger.error(f\"Error transforming to target schema: {str(e)}\")\n",
					"        import traceback\n",
					"        logger.error(traceback.format_exc())\n",
					"        \n",
					"        # Create a minimal valid DataFrame with correct schema\n",
					"        schema = StructType([\n",
					"            StructField(field[\"name\"], \n",
					"                      StringType() if field[\"type\"] == \"string\" else\n",
					"                      IntegerType() if field[\"type\"] == \"integer\" else\n",
					"                      TimestampType() if field[\"type\"] == \"timestamp\" else\n",
					"                      StringType(), \n",
					"                      field.get(\"nullable\", True))\n",
					"            for field in TARGET_SCHEMA\n",
					"        ])\n",
					"        \n",
					"        # Return empty DataFrame with correct schema\n",
					"        from pyspark.sql import SparkSession\n",
					"        spark = SparkSession.builder.getOrCreate()\n",
					"        return spark.createDataFrame([], schema)\n",
					"\n",
					"def process_dataframe(df: DataFrame) -> DataFrame:\n",
					"    \"\"\"\n",
					"    Apply all validation and transformation rules to a DataFrame.\n",
					"    \n",
					"    Args:\n",
					"        df: Input DataFrame\n",
					"        \n",
					"    Returns:\n",
					"        Processed DataFrame conforming to all defined rules\n",
					"    \"\"\"\n",
					"    logger = logging.getLogger()\n",
					"    logger.info(f\"Starting data processing with {df.count()} rows\")\n",
					"    \n",
					"    # Step 1: Process postcodes (standardize and filter invalid)\n",
					"    df = process_postcodes(df)\n",
					"    \n",
					"    # Step 2: Enforce NULL columns\n",
					"    df = enforce_null_columns(df)\n",
					"    \n",
					"    # Step 3: Transform to target schema\n",
					"    df = create_target_schema_df(df)\n",
					"    \n",
					"    # Step 4: Enforce column length limits\n",
					"    df = enforce_column_lengths(df)\n",
					"    \n",
					"    logger.info(f\"Completed data processing with {df.count()} rows\")\n",
					"    return df"
				],
				"execution_count": null
			}
		]
	}
}